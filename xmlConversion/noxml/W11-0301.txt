
	
		The connection between part-of-speech (POS) categories and morphological properties is well-documented in linguistics but underutilized in text processing systems.
		This paper proposes a novel model for morphological segmentation that is driven by this connection.
		Our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words.
		Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmentation of Arabic.
		1
	
	
			A tight connection between morphology and syntax is well-documented in linguistic literature.
			In many languages, morphology plays a central role in marking syntactic structure, while syntactic relations help to reduce morphological ambiguity (Harley and Phillips, 1994).
			Therefore, in an unsupervised linguistic setting which is rife with ambiguity, modeling this connection can be particularly beneficial.
			However, existing unsupervised morphological analyzers take little advantage of this linguistic property.
			In fact, most of them operate at the vocabulary level, completely ignoring sentence context.
			This design is not surprising: a typical morphological analyzer does not have access to syntac 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/morphsyn/.
			tic information, because morphological segmentation precedes other forms of sentence analysis.
			In this paper, we demonstrate that morphological analysis can utilize this connection without assuming access to full-fledged syntactic information.
			In particular, we focus on two aspects of the morpho- syntactic connection: • Morphological consistency within POS categories.
			Words within the same syntactic category tend to select similar affixes.
			This linguistic property significantly reduces the space of possible morphological analyses, ruling out assignments that are incompatible with a syntactic category.
			• Morphological realization of grammatical agreement.
			In many morphologically rich languages, agreement between syntactic dependents is expressed via correlated morphological markers.
			For instance, in Semitic languages, gender and number agreement between nouns and adjectives is expressed using matching suffixes.
			Enforcing mutually consistent segmentations can greatly reduce ambiguity of word- level analysis.
			In both cases, we do not assume that the relevant syntactic information is provided, but instead jointly induce it as part of morphological analysis.
			We capture morpho-syntactic relations in a Bayesian model that grounds intra-word decisions in sentence-level context.
			Like traditional unsupervised models, we generate morphological structure from a latent lexicon of prefixes, stems, and suffixes.
			1 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 1–9, Portland, Oregon, USA, 23–24 June 2011.
			Qc 2011 Association for Computational Linguistics In addition, morphological analysis is guided by a latent variable that clusters together words with similar affixes, acting as a proxy for POS tags.
			Moreover, a sequence-level component further refines the analysis by correlating segmentation decisions between adjacent words that exhibit morphological agreement.
			We encourage this behavior by encoding a transition distribution over adjacent words, using string match cues as a proxy for grammatical agreement.
			We evaluate our model on the standard Arabic treebank.
			Our full model yields 86.2% accuracy, outperforming the best published results (Poon et al., 2009) by 8.5%.
			We also found that modeling morphological agreement between adjacent words yields greater improvement than modeling syntactic categories.
			Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis.
	
	
			Research in unsupervised morphological segmentation has gained momentum in recent years bringing about significant developments to the area.
			These advances include novel Bayesian formulations (Goldwater et al., 2006; Creutz and Lagus, 2007; Johnson, 2008), methods for incorporating rich features in unsupervised log-linear models (Poon et al., 2009) and the development of multilingual morphological segmenters (Snyder and Barzi- lay, 2008a).
			Our work most closely relates to approaches that aim to incorporate syntactic information into morphological analysis.
			Surprisingly, the research in this area is relatively sparse, despite multiple results that demonstrate the connection between morphology and syntax in the context of part-of-speech tagging (Toutanova and Johnson, 2008; Habash and Rambow, 2005; Dasgupta and Ng, 2007; Adler and Elhadad, 2006).
			Toutanova and Cherry (2009) were the first to systematically study how to incorporate part-of-speech information into lemmatization and empirically demonstrate the benefits of this combination.
			While our high-level goal is similar, our respective problem formulations are distinct.
			Toutanova and Cherry (2009) have considered a semi-supervised setting where an initial morpholog ical dictionary and tagging lexicon are provided but the model also has access to unlabeled data.
			Since a lemmatizer and tagger trained in isolation may produce mutually inconsistent assignments, and their method employs a log-linear reranker to reconcile these decisions.
			This reranking method is not suitable for the unsupervised scenario considered in our paper.
			Our work is most closely related to the approach of Can and Manandhar (2009).
			Their method also incorporates POS-based clustering into morphological analysis.
			These clusters, however, are learned as a separate preprocessing step using distributional similarity.
			For each of the clusters, the model selects a set of affixes, driven by the frequency of their occurrences in the cluster.
			In contrast, we model morpho-syntactic decisions jointly, thereby enabling tighter integration between the two.
			This design also enables us to capture additional linguistic phenomena such as agreement.
			While this technique yields performance improvement in the context of their system, the final results does not exceed state- of-the-art systems that do not exploit this information (for e.g., (Creutz and Lagus, 2007)).
	
	
			Given a corpus of unannotated and unsegmented sentences, our goal is to infer the segmentation boundaries of all words.
			We represent segmentations and syntactic categories as latent variables with a directed graphical model, and we perform Bayesian inference to recover the latent variables of interest.
			Apart from learning a compact morpheme lexicon that explains the corpus well, we also model morpho-syntactic relations both within each word and between adjacent words to improve segmentation performance.
			In the remaining section, we first provide the key linguistic intuitions on which our model is based before describing the complete generative process.
			3.1 Linguistic Intuition.
			While morpho-syntactic interface spans a range of linguistic phenomena, we focus on two facets of this connection.
			Both of them provide powerful constraints on morphological analysis and can be modeled without explicit access to syntactic annotations.
			Morphological consistency within syntactic category.
			Words that belong to the same syntactic category tend to select similar affixes.
			In fact, the power of affix-related features has been empirically shown in the task of POS tag prediction (Habash and Ram- bow, 2005).
			We hypothesize that this regularity can also benefit morphological analyzers by eliminating assignments with incompatible prefixes and suffixes.
			For instance, a state-of-the-art segmenter er roneously divides the word “Al{ntxAbAt” into fourmorphemes “Al-{ntxAb-A-t” instead of three “Al{ntxAb-At” (translated as “the-election-s”.)
			The affix assignment here is clearly incompatible — de terminer “Al” is commonly associated with nouns, while suffix “A” mostly occurs with verbs.
			Since POS information is not available to the model, we introduce a latent variable that encodes affix-based clustering.
			In addition, we consider a variant of the model that captures dependencies between latent variables of adjacent words (analogous to POS transitions).
			Morphological realization of grammatical agreement.
			In morphologically rich languages, agreement is commonly realized using matching suffices.
			In many cases, members of a dependent pair such as adjective and noun have the exact same suffix.
			A common example in the Arabic Treebank is the bigram “AlDf-p Algrby-p” (which is translated word-for-word as “the-bank the-west”) where the last morpheme “p” is a feminine singular noun suffix.
			Fully incorporating agreement constraints in the model is difficult, since we do not have access to syntactic dependencies.
			Therefore, we limit our attention to adjacent words which end with similar strings – for e.g., “p” in the example above.
			The model encourages consistent segmentation of such pairs.
			While our string-based cue is a simple proxy for agreement relation, it turns to be highly effective in practice.
			On the Penn Arabic treebank corpus, our cue has a precision of around 94% at the token-level.
			3.2 Generative Process.
			The high-level generative process proceeds in four phases: (a) Lexicon Model: We begin by generating morpheme lexicons L using parameters γ.
			This set of lexicons consists of separate lexicons for prefixes, stems, and suffixes generated in a hierarchical fashion.
			(b) Segmentation Model: Conditioned on L, we draw word types, their segmentations, and also their syntactic categories (W , S, T ).
			(c) Token-POS Model: Next, we generate the un- segmented tokens in the corpus and their syntactic classes (w, t) from a standard first-order HMM which has dependencies between adjacent syntactic categories.
			(d) Token-Seg Model: Lastly, we generate token segmentations s from a first-order Markov chain that has dependencies between adjacent segmentations.
			The complete generative story can be summarized by the following equation: P (w,s, t, W , S, T , L, Θ, θ|γ, α, β) = P (L|γ) (a) P (W , S, T , Θ|L, γ, α) (b) Ppos(w, t, θ|W , S, T , L, α) (c) Pseg(s|W , S, T , L, β, α) (d) where γ, α, Θ, θ, β are hyperparameters and parameters whose roles we shall detail shortly.
			Our lexicon model captures the desirability of compact lexicon representation proposed by prior work by using parameters γ that favors small lexicons.
			Furthermore, if we set the number of syntactic categories in the segmentation model to one and exclude the token-based models, we recover a segmenter that is very similar to the unigram Dirichlet Process model (Goldwater et al., 2006; Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b).
			We shall elaborate on this point in Section 4.
			The segmentation model captures morphological consistency within syntactic categories (POS tag), whereas the Token-POS model captures POS tag dependencies between adjacent tokens.
			Lastly, the Token-Seg model encourages consistent segmentations between adjacent tokens that exhibit morphological agreement.
			Lexicon Model The design goal is to encourage morpheme types to be short and the set of affixes (i.e. prefixes and suffixes) to be much smaller than the set of stems.
			To achieve this, we first draw each morpheme σ in the master lexicon L∗ according to a geometric distribution which assigns monotonically smaller probability to longer morpheme lengths: |σ| ∼ Geometric(γl ) The parameter γl for the geometric distribution is fixed and specified beforehand.
			We then draw the prefix, the stem, and suffix lexicons (denoted by By generating parameters in this manner, we allow the multinomial distributions to generate only morphemes that are present in the lexicon.
			Also, at inference time, only morphemes in the lexicons receive pseudo-counts.
			Note that the affixes are generated conditioned on the tag; But the stem are not.2 Now, we are ready to generate each word type W , its segmentation S, and its syntactic category T . First, we draw the number of morpheme segments|S| from a geometric distribution truncated to gener ate at most five morphemes: S| ∼ Truncated-Geometric(γ|S|) L−, L0, L+ respectively) from morphemes in L∗.Generating the lexicons in such a hierarchical fash ion allows morphemes to be shared among the lower-level lexicons.
			For instance, once determiner “Al” is generated in the master lexicon, it can be used to generate prefixes or stems later on.
			To favor compact lexicons, we again make use of a geometric distribution that assigns smaller probability to lexicons that contain more morphemes: prefix: |L−| ∼ Geometric(γ−) stem: |L0| ∼ Geometric(γ0) suffix: |L+| ∼ Geometric(γ+) By separating morphemes into affixes and stems, we can control the relative sizes of their lexicons with different parameters.
			Segmentation Model The model independently generates each word type using only morphemes in the affix and stem lexicons, such that each word has exactly one stem and is encouraged to have few morphemes.
			We fix the number of syntactic categories (tags) to K and begin the process by generating multinomial distribution parameters for the POS | Next, we pick one of the morphemes to be the stem uniformly at random, and thus determine the number of prefixes and suffixes.
			Then, we draw the syntactic category T for the word.
			(Note that T is a latent variable which we recover during inference.)
			T ∼ Multinomial(ΘT ) After that, we generate each stem σ0, prefix σ−, and suffix σ+ independently: σ0 ∼ Multinomial(Θ0) σ−|T ∼ Multinomial(Θ−|T ) σ+|T ∼ Multinomial(Θ+|T ) Token-POS Model This model captures the dependencies between the syntactic categories of adjacent tokens with a first-order HMM.
			Conditioned on the type-level assignments, we generate (unsegmented) tokens w and their POS tags t: Ppos(w, t|W , T , θ) = n P (ti−1|ti, θt t)P (w |t , θ ) tag prior from a Dirichlet prior: wi ,ti | i i w|t ΘT ∼ Dirichlet(αT , {1, . . .
			, K }) Next, for each possible value of the tag T ∈ where the parameters of the multinomial distributions are generated by Dirichlet priors:{1, . . .
			, K }, we generate parameters for a multino mial distribution (again from a Dirichlet prior) for θt|t ∼ Dirichlet(α t|t , {1, . . .
			, K }) each of the prefix and the suffix lexicons: θw|t ∼ Dirichlet(αw|t, W t) 2 We design the model as such since the.
			dependencies be Θ−|T ∼ Dirichlet(α− , L−) tween affixes and the POS tag are much stronger than those be Θ0 ∼ Dirichlet(α0, L0) Θ+|T ∼ Dirichlet(α+, L+) tween the stems and tags.
			In our preliminary experiments, when stems are also generated conditioned on the tag, spurious stems are easily created and associated with garbage-collecting tags.
			Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).
			Token-Seg Model The model captures the morphological agreement between adjacent segmentations using a first-order Markov chain.
			The probability of drawing a sequence of segmentations s is given by all latent variables, including the segmentations s. P (s, t, S, T , L|w, W , γ, α, β) r ∝ P (w, s, t, W , S, T , L, Θ, θ|γ, α, β)dΘdθ We want to sample from the above distribution using collapsed Gibbs sampling (Θ and θ integrated out.)
			In each iteration, we loop over each word type Wi and sample the following latent variables: its tag Ti, its segmentation Si, the segmentations and tags for all of its token occurrences (si, ti), and also the Pseg(s|W , S, T , L, β, α) = n (si−1 ,si ) p(si|si−1) morpheme lexicons L:For each pair of segmentations si−1 and si, we de P (L, Ti, Si, si, ti| termine: (1) if they should exhibit morphosyntacticagreement, and (2) if their morphological segmenta s−i , t−i, S −i, T −i, w−i, W −i, γ, α, β) (1) tions are consistent.
			To answer the first question, we first obtain the final suffix for each of them.
			Next, we obtain n, the length of the longer suffix.
			For each segmentation, we define the ending to be the last n characters of the word.
			We then use matching endings as a proxy for morpho-syntactic agreement between the two words.
			To answer the second question, we use matching final suffixes as a cue for consistent morphological segmentations.
			To encode the linguistic intuition that words that exhibit morpho- syntactic agreement are likely to be morphological consistent, we define the above probability distribution to be: p(si|si−1)  β1 if same endings and same final suffix such that the type and token-level assignments are consistent, i.e. for all t ∈ ti we have t = Ti, and for all s ∈ si we have s = Si.
			4.1 Approximate Inference.
			Naively sampling the lexicons L is computationally infeasible since their sizes are unbounded.
			Therefore, we employ an approximation which turns is similar to performing inference with a Dirichlet Process segmentation model.
			In our approximation scheme, for each possible segmentation and tag hypothesis (Ti, Si, si, ti), we only consider one possible value for L, which we denote the minimal lexicons.
			Hence, the total number of hypothesis that we have to consider is only as large as the number of possibilities for (Ti, Si, si, ti).
			=  β if same endings but different final suffixes Specificall y, we recover the minimal lexicons as  β3 otherwise (e.g. no suffix) where β1 + β2 + β3 = 1, with β1 > β3 > β2.
			By setting β1 to a high value, we encourage adjacent tokens that are likely to exhibit morpho-syntactic agreement to have the same final suffix.
			And by setting β3 > β2, we also discourage adjacent tokens with the same endings to be segmented differently.
			3
	
	
			Given a corpus of unsegmented and unannotated word tokens w, the objective is to recover values of follows: for each segmentation and tag hypothesis, we determine the set of distinct affix and stem types in the whole corpus, including the morphemes introduced by segmentation hypothesis under consideration.
			This set of lexicons, which we call the minimal lexicons, is the most compact ones that are needed to generate all morphemes proposed by the current hypothesis.
			Furthermore, we set the number of possible POS tags K = 5.
			4 For each possible value of the tag, we consider all possible segmentations with at most five segments.
			We further restrict the stem to have no 3 Although p sums to one, it makes the model deficient since,.
			conditioned everything already generated, it places some probability mass on invalid segmentation sequences.
			4 We find that increasing K to 10 does not yield improve-.
			ment.
			more than two prefixes or suffixes and also that the stem cannot be shorter than the affixes.
			This further restricts the space of segmentation and tag hypotheses, and hence makes the inference tractable.
			4.2 Sampling equations.
			Suppose we are considering the hypothesis with segmentation S and POS tag T for word type Wi.
			Let L = (L∗, L−, L0, L+) be the minimal lexicons for The first factor is the truncated geometric distribution of the number of segmentations |S|, and the second factor is the probability of generate the tag T . The rest are the probabilities of generating the stem σ0, the prefix σ−, and the suffix σ+ (where the parameters of the multinomial distribution collapsed out).
			n−1 is the number of word types with tag T and N −i is the total number of word types.
			n−i σ−|T refers to the number of times prefix σ− is seen in all this hypothesis (S, T ).
			We sample the hypothesis word types that are tagged with T , and N −i is the (S, T , s = S, t = T , L) proportional to the product total number of prefixes in all word types −|T that has tag of the following four equations.
			Lexicon Model T . All counts exclude the word type Wi whose segmentation we are sampling.
			If there is another pre fix, N −i is incremented (and also n−i σ−|Tif the sec n γl (1 − γl )|σ| × ond −| prefix is the same as the first one.)
			Integrating σ∈L∗ γ−(1 − γ−)| L−| × out the parameters introduces dependencies between prefixes.
			The rest of the notations read analogously.
			γ0(1 − γ0)|L0 | × γ+(1 − γ+)|L+ | (2) Token-POS Model (mi ) This is a product of geometric distributions involving the length of each morpheme σ and the size (M −i αw|t i × t + αw|t|W t|)(m ) iof each of the prefix, the stem, and the suffix lexi tl|t + αt|t) (mtl|t )cons (denoted as |L−|, |L0|, |L+| respectively.)
			Sup pose, a new morpheme type σ0 is introduced as a n n t=1 tl=1 (M −i (mi ) t t tl|t (4) stem.
			Relative to a hypothesis that introduces none, this one incurs an additional cost of (1 − γ0) and γl (1 − γl )|σ0 |.
			In other words, the hypothesis is pe nalized for increasing the stem lexicon size and generating a new morpheme of length |σ0|.
			In this way, the first and second terms play a role similar to the concentration parameter and base distribution in a DP-based model.
			t + α | ) The two terms are the token-level emission and transition probabilities with parameters integrated out.
			The integration induces dependences between all token occurrences of word type W which results in ascending factorials defined as α(m) = α(α + 1) · · · (α + m − 1) (Liang et al., 2010).
			M −i is the number of tokens that have POS tag t, mi is the Segmentation Model number of tokens w , a n d m − i t l | tis the number of to γ|S|(1 − γ|S|)| 5 × kens t-to-tl transitions.
			(Both exclude counts con tributed by tokens belong to word type Wi.)
			|W t| is j=0 γ|S|(1 − γ|S|)j T + α N −i + αK × n−i the number of word types with tag t. Token-Seg Model mi mi mi β1 β2 β3 σ0 + α0 N0 + α0|L0| × β1 β2 β3 (5) n−i + α Here, mi 1 refers to the number of transitions involv σ−|T − N −i × ing token occurrences of word type Wi that exhibit −|T + α−|L−| σ+ |T + α+ +|T + α+|L+| (3) morphological agreement.
			This does not result in ascending factorials since the parameters of transition probabilities are fixed and not generated from Dirichlet priors, and so are not integrated out.
			4.3 Staged Training.
			Although the Gibbs sampler mixes regardless of the initial state in theory, good initialization heuristics often speed up convergence in practice.
			We therefore train a series of models of increasing complexity (see section 6 for more details), each with 50 iterations of Gibbs sampling, and use the output of the preceding model to initialize the subsequent model.
			The initial model is initialized such that all words are not segmented.
			When POS tags are first introduced, they are initialized uniformly at random.
	
	
			Performance metrics To enable comparison with previous approaches, we adopt the evaluation setup of Poon et al.
			(2009).
			They evaluate segmentation accuracy on a per token basis, using recall, precision and F1-score computed on segmentation points.
			We also follow a transductive testing scenario where the same (unlabeled) data is used for both training and testing the model.
			Data set We evaluate segmentation performance on the Penn Arabic Treebank (ATB).5 It consists of about 4,500 sentences of modern Arabic obtained from newswire articles.
			Following the preprocessing procedures of Poon et al.
			(2009) that exclude certain word types (such as abbreviations and digits), we obtain a corpus of 120,000 tokens and 20,000 word types.
			Since our full model operates over sentences, we train the model on the entire ATB, but evaluate on the exact portion used by Poon et al.
			(2009).
			Predefined tunable parameters and testing regime In all our experiments, we set γl = 1 (for 1 M od el R P F 1t te st P C T 09 M orf es so r 69 .2 72 .6 88 .5 77 .4 7 7 . 7 7 4 . 9 B AS IC + P O S + T O K EN P O S + T O K EN S E G 71 .4 75 .4 75 .7 82 .1 86 .7 87 .4 88 .5 90 .8 78 .3 (2.
			9) 81 .0 (1.
			5) 81 .6 (0.
			7) 86 .2 (0.
			4) + ∼ + + Table 1: Results on the Arabic Treebank (ATB) data set: We compare our models against Poon et al.
			(2009) (PCT09) and the Morfessor system (Morfessor-CAT).
			For our full model (+TOKEN-SEG) and its simplifications (BASIC, +POS, +TOKEN POS), we perform five random restarts and show the mean scores.
			The sample standard deviations are shown in brackets.
			The last column shows results of a paired t-test against the preceding model: ++ (significant at 1%), + (significant at 5%), ∼ (not significant), - (test not applicable).
			(for unsegmented tokens) and αt|t = 1.0 (for POS tags transition.)
			To encourage adjacent words that exhibit morphological agreement to have the same final suffix, we set β1 = 0.6, β2 = 0.1, β1 = 0.3.
			In all the experiments, we perform five runs using different random seeds and report the mean score and the standard deviation.
			Baselines Our primary comparison is against the morphological segmenter of Poon et al.
			(2009) which yields the best published results on the ATB corpus.
			In addition, we compare against the Morfessor Categories-MAP system (Creutz and Lagus, 2007).
			Similar to our model, their system uses latent variables to induce clustering over morphemes.
			The difference is in the nature of the clustering: the Morfessor algorithm associates a latent variable for each morpheme, grouping morphemes into four broad length of morpheme types) and γ|S| =2 (for num ber of morpheme segments of each word.)
			To encourage a small set of affix types relative to stem 1 categories (prefix, stem, suffix, and non morpheme) but not introducing dependencies between affixes directly.
			For both systems, we quote their performance types, we set γ− = γ+ =1.1 (for sizes of the af fix lexicons) and γ0 = 1 (for size of the stem lexicon.)
			We employ a sparse Dirichlet prior for the type-level models (for morphemes and POS tag) by setting α = 0.1.
			For the token-level models, we set hyperparameters for Dirichlet priors αw t = 10−5 5 Our evaluation does not include the Hebrew and Arabic Bible datasets (Snyder and Barzilay, 2008a; Poon et al., 2009) since these corpora consists of short phrases that omit sentence context.
			reported by Poon et al.
			(2009).
	
	
			Comparison with the baselines Table 1 shows that our full model (denoted +TOKEN-SEG) yields a mean F1-score of 86.2, compared to 77.7 and 74.9 obtained by the baselines.
			This performance gap corresponds to an error reduction of 38.1% over the best published results.
			Ablation Analysis To assess relative impact of various components, we consider several simplified variants of the model: • BASIC is the type-based segmentation model that is solely driven by the lexicon.6 • +POS adds latent variables but does not capture transitions and agreement constraints.
			• +TOKEN-POS is equivalent to the full model, without agreement constraints.
			Our results in Table 1 clearly demonstrate that modeling morpho-syntactic constraints greatly improves the accuracy of morphological segmentation.
			We further examine the performance gains arising from improvements due to (1) encouraging morphological consistency within syntactic categories, and (2) morphological realization of grammatical agreement.
			We evaluate our models on a subset of words that exhibit morphological consistency.
			Table 2 shows the accuracies for words that begin with the prefix “Al” (determiner) and end with a suffix “At” (pluralnoun suffix.)
			An example is the word “Al-{ntxAb At” which is translated as “the-election-s”.
			Such words make up about 1% of tokens used for evaluation, and the two affix boundaries constitute about 3% of the all gold segmentation points.
			By introducing a latent variable to capture dependencies between affixes, +POS is able to improve segmentation performance over BASIC.
			When dependencies between latent variables are introduced, +TOKEN- POS yields additional improvements.
			We also examine the performance gains due to morphological realization of grammatical agreement.
			We select the set of tokens that share the same final suffix as the preceding token, such as the bigram “AlDf-p Algrby-p” (which is translated word-for-word as “the-bank the-west”) where the last morpheme “p” is a feminine singular noun suffix.
			This subset makes up about 4% of the evaluation set, and the boundaries of the final suffixes take up about 5% of the total gold segmentation boundaries.
			6 The resulting model is similar in spirit to the unigram DP- based segmenter (Goldwater et al., 2006; Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b).
			Table 2: Segmentation performance on words that begin with prefix “Al” (determiner) and end with suffix “At” (plural noun suffix).
			The mean F1 scores are computed using all boundaries of words in this set.
			For each word, we also determine if both affixes are recovered while ignoring any other boundaries between them.
			The other two columns report this accuracy at both the type-level and the token-level.
			M od el T o k e n T y p e F 1 Acc..
			F 1 Acc..
			B AS IC + P O S + T O K EN P O S + T O K EN SE G 85 .6 70.6 87 .6 76.4 87 .5 75.2 92 .8 91.1 79 .5 58.6 82 .3 66.3 82 .2 65.3 88 .9 84.4 Table 3: Segmentation performance on words that have the same final suffix as their preceding words.
			The F1 scores are computed based on all boundaries within the words, but the accuracies are obtained using only the final suffixes.
			Table 3 reveals this category of errors persisted until the final component (+TOKEN-SEG) was introduced.
	
	
			Although the connection between syntactic (POS) categories and morphological structure is well- known, this relation is rarely exploited to improve morphological segmentation performance.
			The performance gains motivate further investigation into morpho-syntactic models for unsupervised language analysis.
	
	
			This material is based upon work supported by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF10-10533.
			Thanks to the MIT NLP group and the reviewers for their comments.
	
