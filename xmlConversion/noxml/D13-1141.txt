
	
		We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models.
		We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence.
		The new em- beddings significantly outperform baselines in word semantic similarity.
		A single semantic similarity feature induced with bilingual em- beddings adds near half a BLEU point to the results of NIST08 ChineseEnglish machine translation task.
	
	
			It is difficult to recognize and quantify semantic similarities across languages.
			The FrEn phrase-pair {‘un cas de force majeure’, ‘case of absolute necessity’}, ZhEn phrase pair {‘依然故我’,‘persist in a stubborn manner’} are similar in semantics.
			If co- occurrences of exact word combinations are rare in the training parallel text, it can be difficult for classical statistical MT methods to identify this similarity, or produce a reasonable translation given the source phrase.
			We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages.
			As an extension to their monolingual counterpart (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages.
			This prop erty allows them to define semantic similarity metrics across phrase-pairs, making them perfect features for machine translation.
			To learn bilingual embeddings, we use a new objective function which embodies both monolingual semantics and bilingual translation equivalence.
			The latter utilizes word alignments, a natural sub-task in the machine translation pipeline.
			Through large- scale curriculum training (Bengio et al., 2009), we obtain bilingual distributed representations which lie in the same feature space.
			Embeddings of direct translations overlap, and semantic relationships across bilingual embeddings were further improved through unsupervised learning on a large unlabeled corpus.
			Consequently, we produce for the research community a first set of Mandarin Chinese word embed- dings with 100,000 words trained on the Chinese Gigaword corpus.
			We evaluate these embedding on Chinese word semantic similarity from SemEval 2012 (Jin and Wu, 2012).
			The embeddings significantly outperform prior work and pruned tfidf baselines.
			In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity Recognition on the OntoNotes dataset (Hovy et al., 2006) with a neural network model.
			We apply the bilingual embeddings in an end-to- end phrase-based MT system by computing semantic similarities between phrase pairs.
			On NIST08 ChineseEnglish translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system.
			1393 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393–1398, Seattle, Washington, USA, 1821 October 2013.
			Qc 2013 Association for Computational Linguistics
	
	
			Distributed word representations are useful in NLP applications such as information retrieval (Pas¸ca et al., 2006; Manning et al., 2008), search query expansions (Jones et al., 2006), or representing semantics of words (Reisinger et al., 2010).
			A number of methods have been explored to train and apply word embeddings using continuous models for language.
			Collobert et al.
			(2008) learn embed- dings in an unsupervised manner through a con- trastive estimation technique.
			Mnih and Hinton ( 2008), Morin and Bengio ( 2005) proposed efficient hierarchical continuous-space models.
			To systematically compare embeddings, Turian et al.
			(2010) evaluated improvements they bring to state-of-the- art NLP benchmarks.
			Huang et al.
			(2012) introduced global document context and multiple word prototypes.
			Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013).Bilingual word representations have been explored with hand-designed vector space mod Here f is a function defined by a neural network.
			wr is a word chosen in a random subset VR of the vocabulary, and cwr is the context window containing word wr . This unsupervised objective function contrasts the score between when the correct word is placed in context with when a random word is placed in the same context.
			We incorporate the global context information as in Huang et al.
			(2012), shown to improve performance of word embed- dings.
			3.2 Bilingual initialization and training.
			In the joint semantic space of words across two languages, the Chinese word ‘政府’ is expected to be close to its English translation ‘government’.
			At the same time, when two words are not direct transla tions, e.g. ‘lake’ and the Chinese word ‘潭’ (deep pond), their semantic proximity could be correctly quantified.
			We describe in the next subsections the methods to intialize and train bilingual embeddings.
			These methods ensure that bilingual embeddings retain els (Peirsman and Pado´ , 2010; Sumita, 2000), their translational equivalence while their distribu and with unsupervised algorithms such as LDA and LSA (BoydGraber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006).
			Only recently have continuous space models been applied to machine translation (Le et al., 2012).
			Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation.
			In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical tional semantics are improved during online training with a monolingual corpus.
			3.2.1 Initialization by MT alignments First, we use MT Alignment counts as weighting to initialize Chinese word embeddings.
			In our experiments, we use MT word alignments extracted with the Berkeley Aligner (Liang et al., 2006) 1 . Specifically, we use the following equation to compute starting word embeddings: phrase-based MT system.
	
	
			S Wtinit = ) s=1 Cts + 1 Ct + S Ws (2) 3.1 Unsupervised training with global context.
			Our method starts with embedding learning formulations in Collobert et al.
			(2008).
			Given a context window c in a document d, the optimization minimizes the following Context Objective for a word w in the vocabulary: In this equation, S is the number of possible target language words that are aligned with the source word.
			Cts denotes the number of times when word t in the target and word s in the source are aligned in the training parallel text; Ct denotes the total number of counts of word t that appeared in the target language.
			Finally, Laplace smoothing is applied to this weighting function.
			J (c,d) wr ∈VR max(0, 1 − f (cw , d) + f (cw , d)) (1) 1 On NIST08 Zh-.
			En training data and data from GALE MT evaluation in the past 5 years Single-prototype English embeddings by Huang et al.
			(2012) are used to initialize Chinese em- beddings.
			The initialization readily provides a set (Align-Init) of benchmark embeddings in experiments (Section 4), and ensures translation equivalence in the embeddings at start of training.
			3.2.2 Bilingual training Using the alignment counts, we form alignment matrices Aen→zh and Azh→en.
			For Aen→zh, each row corresponds to a Chinese word, and each column an English word.
			An element aij is first assigned the counts of when the ith Chinese word is aligned with the jth English word in parallel text.
			After assignments, each row is normalized such that it sums to one.
			The matrix Azh→en is defined similarly.
			Denote the set of Chinese word embeddings as Vzh, with each row a word embedding, and the set of English word embeddings as Ven.
			With the two alignment matrices, we define the Translation Equivalence Objective: 3.3 Curriculum training.
			We train 100k-vocabulary word embeddings using curriculum training (Turian et al., 2010) with Equation 5.
			For each curriculum, we sort the vocabulary by frequency and segment the vocabulary by a band-size taken from {5k, 10k, 25k, 50k}.
			Separate bands of the vocabulary are trained in parallel using minibatch L-BFGS on the Chinese Gigaword corpus 3 . We train 100,000 iterations for each curriculum, and the entire 100k vocabulary is trained for 500,000 iterations.
			The process takes approximately 19 days on a eight-core machine.
			We show visualization of learned embeddings overlaid with English in Figure 1.
			The two-dimensional vectors for this visualization is obtained with t-SNE (van der Maaten and Hinton, 2008).
			To make the figure comprehensible, subsets of Chinese words are provided with reference translations in boxes with green borders.
			Words across the two languages are positioned by the semantic relationships implied by their embed- dings.
			JT EOen zh = Vzh − Aen zhVen 2 JT EOzh en = Ven − Azh enVzh 2 (3) (4) We optimize for a combined objective during training.
			For the Chinese embeddings we optimize for: JC O-zh + λJT EOen→zh (5) For the English embeddings we optimize for: JC O-en + λJT EOzh→en (6) During bilingual training, we chose the value of λ such that convergence is achieved for both JC O and JT EO . A small validation set of word similarities from (Jin and Wu, 2012) is used to ensure the em- beddings have reasonable semantics.
			2 In the next sections, ‘bilingual trained’ embed- dings refer to those initialized with MT alignments and trained with the objective defined by Equation 5.
			‘Monolingual trained’ embeddings refer to those intialized by alignment but trained without JT EOen→zh.
			2 In our experiments, λ = 50..
			Figure 1: Overlaid bilingual embeddings: English words are plotted in yellow boxes, and Chinese words in green; reference translations to English are provided in boxes with green borders directly below the original word.
	
	
			4.1 Semantic Similarity.
			We evaluate the Mandarin Chinese embeddings with the semantic similarity test-set provided by the or 3 Fifth Edition.
			LDC catelog number LDC2011T13.
			We only exclude cna cmn, the Traditional Chinese segment of the corpus.
			Table 1: Results on Chinese Semantic Similarity Method Sp.
			Corr.
			K. Tau (×100) (×100) Prior work (Jin and Wu, 2012) 5.0 Tfidf Naive tfidf 41.5 28.7 Pruned tfidf 46.7 32.3 Word Embeddings Align-Init 52.9 37.6 Mono-trained 59.3 42.1 Biling-trained 60.8 43.3 Table 2: Results on Named Entity Recognition Embeddings Prec.
			Rec.
			F1 Improve Align-Init 0.34 0.52 0.41 Mono-trained 0.54 0.62 0.58 0.17 Biling-trained 0.48 0.55 0.52 0.11 Table 3: Vector Matching Alignment AER (lower is better) Embeddings Prec.
			Rec.
			AER Mono-trained 0.27 0.32 0.71 Biling-trained 0.37 0.45 0.59 ganizers of SemEval2012 Task 4.
			This test-set contains 297 Chinese word pairs with similarity scores estimated by humans.
			The results for semantic similarity are shown in Table 1.
			We show two evaluation metrics: Spear- man Correlation and Kendall’s Tau.
			For both, bilingual embeddings trained with the combined objective defined by Equation 5 perform best.
			For pruned tfidf, we follow Reisinger et al.
			(2010; Huang et al.
			(2012) and count word co-occurrences in a 10- word window.
			We use the best results from a range of pruning and feature thresholds to compare against our method.
			The bilingual and monolingual trained embeddings4 outperform pruned tfidf by14.1 and 12.6 Spearman Correlation (×100), respec tively.
			Further, they outperform embeddings initialized from alignment by 7.9 and 6.4.
			Both our tfidf implementation and the word embeddings have significantly higher Kendall’s Tau value compared to Prior work (Jin and Wu, 2012).
			We verified Tau calculations with original submissions provided by the authors.
			4.2 Named Entity Recognition.
			We perform NER experiments on OntoNotes (v4.0) (Hovy et al., 2006) to validate the quality of the Chinese word embeddings.
			Our experimental setup is the same as Wang et al.
			(2013).
			With em- beddings, we build a naive feed-forward neural network (Collobert et al., 2008) with 2000 hidden neurons and a sliding window of five words.
			This naive setting, without sequence modeling or sophisticated join optimization, is not competitive with state-of- the-art (Wang et al., 2013).
			Table 2 shows that the bilingual embeddings obtains 0.11 F1 improvement, lagging monolingual, but significantly better than Align-Init (as in Section3.2.1) on the NER task.
			4.3 Vector matching alignment.
			Translation equivalence of the bilingual embeddings is evaluated by naive word alignment to match word embeddings by cosine distance.5 The Alignment Error Rates (AER) reported in Table 3 suggest that bilingual training using Equation 5 produces embed- dings with better translation equivalence compared to those produced by monolingual training.
			4.4 Phrase-based machine translation.
			Our experiments are performed using the Stanford Phrasal phrase-based machine translation system (Cer et al., 2010).
			In addition to NIST08 training data, we perform phrase extraction, filtering and phrase table learning with additional data from GALE MT evaluations in the past 5 years.
			In turn, our baseline is established at 30.01 BLEU and reasonably competitive relative to NIST08 results.
			We use Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoder.
			In the phrase-based MT system, we add one feature to bilingual phrase-pairs.
			For each phrase, the word embeddings are averaged to obtain a feature vector.
			If a word is not found in the vocabulary, we disregard and assume it is not in the phrase; if no word is found in a phrase, a zero vector is assigned 4 Due to variations caused by online minibatch L-BFGS, we.
			take embeddings from five random points out of last 105 mini- batch iterations, and average their semantic similarity results.
	
	
			pairs from the MT training set.
			Table 4: NIST08 ChineseEnglish translation BLEU Method BLEU Our baseline 30.01 Embeddings Random-Init Mono-trained 30.09 Align-Init 30.31 Mono-trained 30.40 Biling-trained 30.49 to it.
			We then compute the cosine distance between the feature vectors of a phrase pair to form a semantic similarity feature for the decoder.
			Results on NIST08 ChineseEnglish translation task are reported in Table 46 . An increase of 0.48 BLEU is obtained with semantic similarity.
			with bilingual embeddings.
			The increase is modest, just surpassing a reference standard deviation 0.29BLEU Cer et al.
			(2010)7 evaluated on a similar sys tem.
			We intend to publish further analysis on statistical significance of this result as an appendix.
			From these suggestive evidence in the MT results, random initialized monolingual trained embeddings add little gains to the baseline.
			Bilingual initialization and training seem to be offering relatively more consistent gains by introducing translational equivalence.
			5 Conclusion.
			In this paper, we introduce bilingual word embed- dings through initialization and optimization constraint using MT alignments The embeddings are learned through curriculum training on the Chinese Gigaword corpus.
			We show good performance on Chinese semantic similarity with bilingual trained embeddings.
			When used to compute semantic similarity of phrase pairs, bilingual embeddings improve NIST08 end-to-end machine translation results by just below half a BLEU point.
			This implies that semantic embeddings are useful features for improving MT systems.
			Further, our results offer suggestive evidence that bilingual word embeddings act as high-quality semantic features and embody bilingual translation equivalence across languages.
	
	
			We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Broad Operational Language Translation (BOLT) program through IBM.
			Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA, or the US government.
			We thank John Bauer and Thang Luong for helpful discussions.
	
