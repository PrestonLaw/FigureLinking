
	
		This paper describes the Chinese Word Segmenter for the fourth International Chinese Language Processing Bakeoff.
		Base on Conditional Random Field (CRF) model, a basic segmenter is designed as a problem of character-based tagging.
		To further improve the performance of our segmenter, we employ a word-based approach to increase the in-vocabulary (IV) word recall and a post-processing to increase the out-of-vocabulary (OOV) word recall.
		We participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora.
	
	
			Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al., 2004), (Tseng et al., 2005), (Low et al., 2005), (Zhao et al., 2006).
			Thus, as a powerful sequence tagging model, CRF became the dominant method in the Bakeoff 2006 (Levow, 2006).In this paper, we improve basic segmenter un der the CRF work frame in two aspects, namely IV and OOV identification respectively.
			We use the result from word-based segmentation to revise the CRF output so that we gain a higher IV word recall.
			For the OOV part a post-processing rule is proposed to find those OOV words which are wrongly segmented into several fractions.
			Our system performs well in the Fourth Bakeoff, achieving four second best and on the fifth in all the five corpora.
			In the following of this paper, we describe our method in more detail.
			The rest of this paper is organized as follows.
			In Section 2, we first give a brief review to the basic CRF tagging approach and then we propose our methods to improve IV and OOV performance respectively.
			In Section 3 we give the experiment results on the fourth Bakeoff corpora to show that our method is effective to improve the performance of the segmenter.
			In Section 4, we conclude our work.
	
	
			In this section, we describe our system in more detail.
			Our system includes three modules: a basic CRF tagger, a word-base segmenter to improve the IV recall and a post-processing rule to improve the OOV recall.
			In the following of this section, we introduce these three modules respectively.
			2.1 Basic CRF tagger.
			Sequence tagging approach treat Word Segmentation task as a labeling problem.
			Every character in input sentences will be given a label which indicates whether this character is a word boundary.
			Our basic CRF1 tagger is almost the same as the system described in (Zhao et al., 2006) except we add a feature to incorporate word information, which is learned from training corpus.
			1 CRF tagger in this paper is implemented by CRF++.
			which is downloaded from http://crfpp.sourceforge.net/ Type Feature Function Unigram C-1, C0, C1 Previous, current and next character Bigram C-1 C0, C0 C1 Two adjacent character Jump C-1 C1 Previous character and next character Word Flag F0 F1 Whether adjacent characters form an IV word Table 1 Feature templates used for CRF in our system Under the CRF tagging scheme, each character in one sentence will be given a label by CRF model to indicate which position this character occupies in a word.
			In our system, CRF tag set is proposed to distinguish different positions in the multi-character words when the word length is less than 6, namely 6-tag set {B, B2, B3, M, E, O}.
			Here, Tag B and E stand for the first and the last position in a multi-character word, respectively.
			S stands up a single-character word.
			B2 and B3 stand for the second and the third position in a multi-character word, whose length is larger than two-character or three-character.
			M stands for the fourth or more rear position in a multi-character word, whose length is larger than four-character.
			We add a new feature, which also used in maximum entropy model for word segmentation task by (Low et al., 2005), to the feature templates for CRF model while keep the other features same as (Zhao et al., 2006).
			The feature templates are defined in table 1.
			In the feature template, only the Word Flag feature needs an explanation.
			The bi nary function F0 = 1 if and only if C-1 C0 form a IV mentations for any input sentence.
			However, the exponential time and space of the length of the input sentence are needed for such a search and it is always intractable in practice.
			Thus, we use the trigram language model to select top B (B is a constant predefined before search and in our experiment 3 is used) best candidates with highest probability at each stage so that the search algorithm can work in practice.
			Finally, when the whole sentence has been read, the best candidate with the highest probability will be selected as the segmentation result.
			After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).
			Since word-based segmentation result also corresponds to a tag sequence according to the 6-tag set, we now have two tags for each character, word-based tag (WT) and CRF tag (CT).
			Which tag will be kept as the final result depends on Marginal Probability (MP) of the CT. Here, we give a short explanation about what is the MP of the CT. Suppose there is a sentence C  c0c1...cM , where ciis the character this sen word, else F0 = 0 and F1 = 1 if and only if C0 C1 tence containing.
			CRF model gives this sentence a form a IV word, else F1 = 0.
			optimal tag sequence T  t0t1...tM , where ti is the 2.2 Word based segmenter and revise rules.
			tag for c . If t  t and t  {B, B , B , M , E, S} , i i 2 3 For the word-based word segmentation, we collect dictionary from training corpus first.
			Instead of the MP of ti is defined as: Maximum Match, trigram language model 2 MP(t  t)  T ,t t P(T | C)trained on training corpus is employed for disam biguation.
			During the disambiguation procedure, a i P(T | C) T beam search decoder is used to seek the most Here,P(T | C) is the conditional probability giv possible segmentation.
			For detail, the decoder reads characters from the input sentence one at a time, and generates candidate segmentations in- crementally.
			At each stage, the next incoming character is combined with an existing candidate in two different ways to generate new candidates: it is either appended to the last word in the candidate, or taken as the start of a new word.
			This method guarantees exhaustive generation of possible seg 2 Language model used in this paper is SLRIM down-.
			loaded from http://www.speech.sri.com/projects/srilm/ en by CRF model.
			For more detail about how to calculate this conditional probability, please refer to (Lafferty et al., 2001).
			Assume that the tag assigned to the current character is CT by CRF and WT by word-based segmenter respectively.
			The rules under which we revise CRF result with word-based result is that if MP(CT) of a character is less than a predefined threshold and WT is not “S”, the WT of this character will be kept as the final result, else the CT of the character will be kept as the final result.
			The restriction that WT should not be “S” is reasonable because word-based segmentation is incapable to recognize the OOV word and always segments OOV word into single characters.
			Besides CRF model is better at dealing with OOV word than our word-based segmentation.
			When WT is “S” it is possible that current word is an OOV word and segmented into single character wrongly by the word-based segmenter, so the CT of the character should be kept under such situation.
			For more detail about this analysis please refer to (Wang et al., 2008).
			2.3 Post-processing rule.
			The rules we described in last subsection is helpful to improve the IV word recall and now we introduce our post-processing rule to improve the OOV recall.
			Our post-processing rule is designed to deal with one typical type of OOV errors, namely an OOV word wrongly segmented into several parts.
			In practice many OOV errors belong to such type.
			The rule is quite simple.
			When we read a sentence from the result we get by the last step, we also kept the last N sentences in memory, in our system we set N equals to 20.
			We do this because adjacent sentences are always relevant and some named entity likely occurs repeatedly in these sentences.
			Then, we scan these sentences to find all n-grams (n from 2 to 7) and count their occurrence.
			If certain n-gram appears more than a threshold and this n-gram never appears in training corpus, the n-gram will be selected as a word candidate.
			Then, we filter these word candidates according to the context entropy (Luo and Song, 2004).
			Assume w is a word candidate appearsn times in the current sentence and last N sen threshold will be bind as a whole word in test corpus no matter what tag sequence the segmenter giving it.
			If a shorter n-gram is contained in a longer n-gram and both of them satisfy the above condition, the shorter n-gram will be overlooked and the longer n-gram is bind as a whole word.
	
	
			On the corpora of the Fourth Bakeoff, we evaluate our system.
			We carry out our evaluation on the closed tracks.
			It means that we do not use any additional knowledge beyond the training corpus.
			The thresholds set for MP and CE on each corpus are tuned on left-out data of training corpus by cross validation.
			To analyze our methods on IV and OOV words, we use a detailed evaluation metric than Bakeoff 2006 (Levow, 2006) which includes Foov and Fiv.
			Our results are shown in Table 2.
			In Table 2, the row “Basic Model” means the results produced by our basic CRF tagger, the row “+IV” means the results produced by the combination of CRF tagger and word-based segmenter and the row “+IV+OOV” means the result we get by executing post-processing rule on the combination results.
			The F measure of the basic CRF tagger alone in the Table 2 is within the top three in the closed tests except Cityu.
			Performance on Cityu corpus is not so good because the inconsistencies existing in Cityu training and test corpora.
			In the training corpus the quotation marks are 「」while in test corpus quotation marks are“”, which never apper in the training corpus.
			As a reult, a lot of errors were caused by quotation marks.
			For example, the following four character “事業”were combined as a one word in our result and fragment“越位”was tagged as two “越 and 位”.
			Because CRF tagger never tences and   {a0 , a1 ,..., al } is the set of left side characters of w . Left Context Entropy (LCE) can be defined as: words met “ and ” in training corpus so the tagger gave the most common tags, namely B and E to the quotation marks, which cause segmentation LCE (w)  1  n a  C (ai , w) log n C (ai , w) errors not only on quotation marks themselv es but also on the character s adjacent to them.
			We remove these inconsiste ncies munually and got Here, C (ai , w) is the count of concur rence of the F measu re 0.5 percen tage higer than the rusultai and w . For the Right Context Entropy, the de finition is the same except change left into right.
			Now, we define Context Entropy (CE) of a word in table 2.
			This result is within the top three in the closed tests.
			On all the five corpora, our “+IV” module can increase the Fiv and our “+OOV” candidate w as min(LCE(w), RCE(w)) . The.
			module can increase Foov respectively.
			However, word candidates with CE larger than a predefined these improvements are not significant.
			Co rp us Me tho d R P F RO OV PO OV FO OV RIV PIV FIV C K IP Ba sic M od el 0.9 46 0.9 23 0.9 40 0.6 51 0.7 19 0.6 83 0.9 69 0.9 48 0.9 58 + IV 0.9 49 0.9 35 0.9 42 0.6 47 0.7 41 0.6 91 0.9 73 0.9 48 0.9 60 + IV + O O V 0.9 50 0.9 36 0.9 43 0.6 56 0.7 48 0.6 99 0.9 73 0.9 49 0.9 61 Ci ty U Ba sic M od el 0.9 44 0.9 34 0.9 39 0.6 54 0.7 21 0.6 86 0.9 70 0.9 51 0.9 60 + IV 0.9 46 0.9 36 0.9 41 0.6 55 0.7 38 0.6 94 0.9 72 0.9 51 0.9 62 + IV + O O V 0.9 49 0.9 37 0.9 43 0.6 78 0.7 59 0.7 16 0.9 73 0.9 51 0.9 62 C T B Ba sic M od el 0.9 53 0.9 51 0.9 52 0.7 03 0.7 27 0.7 15 0.9 67 0.9 64 0.9 65 + IV 0.9 54 0.9 52 0.9 53 0.6 97 0.7 47 0.7 21 0.9 69 0.9 63 0.9 66 + IV + O O V 0.9 54 0.9 53 0.9 53 0.7 03 0.7 49 0.7 25 0.9 69 0.9 64 0.9 66 N C C Ba sic M od el 0.9 40 0.9 28 0.9 34 0.4 38 0.5 80 0.4 99 0.9 65 0.9 40 0.9 52 + IV 0.9 44 0.9 30 0.9 36 0.4 34 0.6 03 0.5 04 0.9 69 0.9 41 0.9 55 + IV + O O V 0.9 45 0.9 32 0.9 39 0.4 50 0.6 20 0.5 22 0.9 70 0.9 43 0.9 56 S X U Ba sic M od el 0.9 60 0.9 53 0.9 56 0.6 36 0.6 74 0.6 54 0.9 77 0.9 67 0.9 72 + IV 0.9 62 0.9 55 0.9 58 0.6 37 0.6 96 0.6 65 0.9 80 0.9 67 0.9 73 + IV + O O V 0.9 62 0.9 55 0.9 59 0.6 45 0.7 02 0.6 73 0.9 79 0.9 68 0.9 74 Table 2 performance each step of our system achieves
	
	
			In this paper, we propose a three-stage strategy in Chinese Word Segmentation.
			Based on the results produced by basic CRF, our word-based segmentation module and post-processing module are designed to improve IV and OOV performance respectively.
			The results above show that our system achieves the state-of-the-art performance.
			Since only the CRF tagger is good enough as we shown in our experiment, in the future work we will pay effort on the semi-supervised learning for CRF model in order to mining more useful information from training and test corpus for CRF tag- ger.
	
