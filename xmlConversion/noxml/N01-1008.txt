
	
		Traditionally coreference is resolved by satisfying a combination of salience, syntactic, semantic and discourse constraints.
		The acquisition of such knowledge is time-consuming, difficult anderror-prone.
		Therefore, we present a knowledge- minimalist methodology of mining coreference rules from annotated text corpora.
		Semantic consistency evidence, which is a form of knowledge required bycoreference, is easily retrieved from WordNet.
		Additional consistency knowledge is discovered by a metabootstrapping algorithm applied to unlabeled texts.
	
	
			Reference resolution is an important task for discourse or dialogue processing systems since iden tity relations between anaphoric textual entities andtheir antecedents is a prerequisite to the understanding of text or conversation.
			Traditionally, coreference resolution has been performed by combin ing linguistic and cognitive knowledge of language.Linguistic information is provided mostly by syn tactic and semantic modeling of language whereascognitive information is incorporated in computational models of discourse.
			Computational meth ods based on linguistic and congitive informationwere presented in (Hobbs 1978), (Lappin and Le ass 1994), (Brennan et al.1987), (Grosz et al.1995)and (Webber 1988).
			The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.
			Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.
			(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).
			For example, COG NIAC (Baldwin 1997), a system based on just sevenordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal refer ence.In our work, we approached the coreference res olution problem by trying to determine how muchmore knowledge is required to supplement the abovementioned knowledge-poor methods and how to de rive that knowledge.
			To this end we (1) analyze the data to find what types of anaphor-antecedent pairs are most popular in real-world texts; (2) deviseknowledge-minimalist rules for handling the major ity of those popular cases; and (3) discover what supplementary knowledge is needed for remaining, more difficult cases.
			To analyze coreference data we use a corpus of annotated texts.
			To devise minimalist coreference resolution rules we consider (1) strong indicators ofcohesion, such as repetitions, name aliases or apposi tions; and (2) gender, number and class agreements.
			WordNet (Miller 1995), the vast semantic knowledge base, provides suplementary knowledge in the form of semantic consistency between coreferring nouns.Additional semantic consistency knowledge is generated by a bootstrapping mechanism when our corefer ence resolution system, COCKTAIL&apos;, processes new texts.
			This bootstrapping mechanism inspired by the technique presented in (Riloff and Jones 1999)targets one of the most problematic forms of knowl edge needed for coreference resolution: the semantic consistency of corefering nominals.
			The rest of the paper is organized as follows.
			Section 2 discusses our text mining methodologyfor analysing the data and devising knowledge minimalist rules for resolving the most popularcoreference cases.
			Section 3 presents the knowledge mining components of COCKTAIL that use WordNet for deriving semantic consistency as well as gender information.
			Section 4 presents an entropy-based method for optimally combining coreference rulesand Section 5 presents the bootstrapping mechanism.
			Section 6 reports and discusses the experimental results while Section 7 summarizes the con clusions.
			&apos;COCKTAIL iS a pun on CoGNIAC, because COCKTAIL usesmultiple coreference resolution rules corresponding to differ ent forms of coreference, blended together in a single system.
			Bill Clinton Bill Clinton Bill Clinton his his his he he he his his Clinton his Clinton Clinton (a) (b) (c) LEGEND: Original Annotation AutoTagCoref Annotation E1 E1 E1 E2 R1 E2 apposition E2 apposition apposition E3 E3 E3 (b) (a) (c) E4 E4 E4 E5 E5 E5 R2 E6 E6 E6 RULE-1-Filter-1-Pronoun (R1F1Pron) If (( Syntactic_Category(Anaphor)== Pronoun) AND oRepetition (Anaphor, Antecedent) ) then Cast_in_Chain(Antecedent, Anaphor) RULE-1-Filter-1-Nominal (R1F1Nom) If (( Syntactic_Category(Anaphor)== Common Noun) AND o(Anaphor == Apposition(Antecedent) ) then Cast_in_Chain(Antecedent, Anaphor) RULE-2-Filter-1-Nominal (R2F1Nom) If (( Syntactic_Category(Anaphor)== oSyntactic_Category(Antecedent)==Proper Noun) AND oSame-Category(Antecedent,Anaphor) ) If ( Category(Anaphor) == PERSON) AND ( []Last_Name(Antecedent)==Last_Name(Anaphor) ) AND AND (Gender(Antecedent) = Gender(Anaphor) AND []Surface_Distance(Anaphor,Antecedent)=min) then Cast_in_Chain(Antecedent, Anaphor) If ( Category(Anaphor) == ORGANIZATION) AND []Acronym(Anaphor,Antecedent)) then Cast_in_Chain(Antecedent, Anaphor) of a nominal, or a disjunct of two or three of them, as illustrated in Table 2.
			The gender attributes may have the values: • m for masculine nouns; • f for feminine nouns; and • n for all the nouns that either are not from the PERSON category or are polysemous&apos; and at least one of the senses does not belong to the PERSON category.
			G Noun examples # m V f V n client, leader, neighbour 807 m V f lawyer, loser, patron, newborn 5217 m V n king, antique, father 42 f V n maiden, mezzo, nanny, harpy 81 m groom, housefather, nobleman 208 f woman, daughter, bride, sheika 417 Table 2: Distribution of gender information.Gender attributes are assigned by the two follow ing heuristics: Heuristic 1 If a collocation fom a WordNet synset contains the word male, the expression G for the whole sysnet is m. If the collocation contains the words female or woman, G= f . Heuristic 2 Consider the first four words from thesynset gloss.
			If any of the gloss words have been assigned gender information, propagate the same in formation to the defined synset as well.
			Each hyponym of the concept {person, individual, human}, categorized as PERSON has expression Ginitialized to f V m, since all lexemes represent per sons, that can be either males or females.
			Whenever one of the two heuristics previously defined can be applied at any node S from this subhierarchy, three operations take place:t&gt; Operation 1: We update G with the new expres sion brough forward by the heuristic.
			t&gt; Operation 2: We propagate all the expression to the hyponyms of S;t&gt; Operation 3: We revisit the whole PERSON sub hierarchy, in search for concepts D that are defined with glosses that use any of the words from synset S or any word from any of its hyponyms.
			Whenever we find such a word, we update its G expressionto G(S).
			We also note that many words are polyse mous, thus a word w may have multiple senses under the PERSON sub-hierarchy and moreover, each sense might have a different G expression.
			In this case, allwords from the synsets containing w receive the dis junct of the gender attributes corresponding to each sense of w. Mining semantic information from WordNetWe used the WordNet knowledge base to mine patterns of WordNet paths that connect pairs of core ferring nouns from the annotated chains.
			The paths are combinations of any of the following WordNet 6A polysemous noun has multiple semantic senses and therefore has multiple entries in the WordNet dictionary.
			relations: • SYNONYM connecting all elements of a synset; • IS-A connecting nouns and verbs from the samehierarchies.
			We also consider the reversed IS-A re lation, denote RIs-A; • GLOSS connecting any element of a synset with the genus of its glossed definition.
			We also consider its reverse relation, named DEFINES; • IN-GLOSS connecting any element of a synset with one of the first four words of its glossed definition.We also consider its reversed relation, named IN DEFINITION • HAS-PART connecting a concept to its meronyms.
			We also consider the reversed IS-PART relation; • MORPHO-DERIVATION connecting a word to its morphological derivations.
			• COLLIDE-SENSE connecting several senses of the same word.To determine the confidence of the path we con sider three factors: *Factor fi has only two values.
			It is set to 1 when another coreference chain contains elements in the same NPs as the anaphor and the anetcedent.
			For example, if NPi is &amp;quot;the professor&apos;s son&amp;quot; and NP2is &amp;quot;his father&amp;quot;, the semantic consistency between fa ther and professor is more likely, given that his and son corefer.
			Otherwise, fi is set to 0.*Factor f2 favors (a) relations that are consid ered &amp;quot;stronger&amp;quot; (e.g. SYNONYMY, GLOSS); and (b) shorter paths.
			For this purpose we assign the following weights to each relation considered: W(SYNONYM) = 1.0; w(IS-A) = 0.9; w(GLoss) = 0.9; w(IN-GLoss) = 0.3; w(HAs-PART) = 0.7;w(MoRPHo-DERIVATION) = 0.6; and W(COLLIDE SENSE) = 0.5.
			When computing the f2 factor, we assume that whenever at least two relations of the same kind repeat, we should consider the sequence of relations equivalent to a single relation, having the weight devided by the length of the sequence.
			If we denote by rir„i the number of different relationtypes encountered in a path, and rirsame(rel) de notes the number of links of type rel in a sequence, then we define f2 with the formula: 1 w(rel) f2 — E nrrel relEPath nrsame(rel)*Factor h is a semantic measure operating on a con ceptual space.
			When searching for a lexico-semantic path, a search space SS is created, which contains all WordNet content words that can be reached from the candidate antecedent or the anaphor in at most five combinations of the seven relations used by the third filter.
			We denote by N the total number of nouns and verbs in the search space.
			C represents the number of nouns and verbs that can be reachedby both nominals.
			In addition rirtotal is the num ber of concepts along all paths established, whereas rel(p+) 1 0.5 00.51 0 0.5 1 R3 B A R2 CN1 CN2 R1 CN1 New semantic consistency path = New coreference rule Semantic consistency Path: Mropho-Derivation : Is-A : Collide-Sense Coreference Rule: If (x is Morpho-Derivation ( Anaphor) ) AND AND (y is one of the hypernyms of x) AND AND (z is SYNONYM of y) AND AND (z is SYNONYM of anaphor) then Cast_in_Chain(Anaphor,antecedent) (Riloff and Jones 1999) note that the performanceof the mutual bootstrapping algorithm can deterio rate rapidly if erroneous rules are entered.
			To makethe algorithm more robust we use the same solu tion by introducing a second level of bootrapping.
			The outer level, called metabootstrapping identifiesthe most reliable k rules based on semantic consis tency and discard all the others before restartingthe mutual bootstrapping loop again.
			In our experi ments we have retained only those rules for which the new performance, given by the F-measure was largerthan the median of the past four loops.
			The for mula for the van Rijsbergen&apos;s F-measure combines 2 xpP±XRR the precision P with the recall R in F = 6 Evaluation.
			To measure the performance of COCKTAIL we have trained the system on 30 MUC6 and MUC7 texts and tested it on the remaining 30 documents.We computed the precision, the recall and the Fmeasure.
			The performance measures have been ob tained automatically using the MUC6 coreference scoring program (Vilain et al. 1995).
			Table 4 lists the results.
			Precision Recall F-measure COCKTAIL 87.1% 61.7% 72.3% rules COCKTAIL 91.3% 58.6% 71.8% rules combined COCKTAIL 92.0% 73.9% 81.9% +bootstrapping Table 4: Bootstrapping effect on COCKTAILTable 4 shows that the seed set of rules had good pre cision but poor recall.
			By combining the rules with theentropy-based measure, we obtained further enhancement in precision, but the recall dropped.
			The appli cation of the bootstrapping methodology determined an enhancement of recall, and thus of the F-measure.
			In the future we intend to compare the overall effect of rulesthat recognize referential expressions on the overall per formance of the system.
			7 Conclusion.
			We have introduced a new data-driven method for coreference resolution, implemented in the COCKTAIL system.
			Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.
			Furthermore, by using an entropy-based method we determine the best partition of corefering expressions in coreference chains.
			New rules are learned by applying a bootstrapping methodology that uncovers additional semantic consistency data.
	
