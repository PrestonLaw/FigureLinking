
	
	
			1000 10000 rank(wi) The values of c. / for the 30 most frequent tokens of BNCfrg are listed in Table 3.2.
			The most frequent words in any corpus are generally function words like prepositions, determiners, pronouns and punctuation signs.
			Notice that, as our corpus was not analysed using the tools described in Sect.
			3.1.1, the words the and The are considered as two distinct tokens.
			Also, because of an encoding problem, there are two different apostrophe characters.
			In Sect.
			3.1.3, our goal is to estimate the probability of an arbitrary token or sequence of tokens.
			Therefore, it is useful to study the empirical distribution of the values of function c. /.
			Unlike the heights of humans or the grades of students in a class, the word counts in a corpus are not normally distributed around the mean.
			Instead, they are distributed according to a power law, also known as Zipfian distribution.
			Many other events in the world are distributed according to power laws (Newman 2005).
			In order to illustrate the Zipfian distribution, we will use the rank plot of Fig.
			3.1.
			A rank plot is a graphic where the word counts are sorted in descending order and assigned to their rank positions r , like those in the first column of Table 3.2.
			Formally, the rank r of a given word w can be defined as the value of a bijective function rank.w/ W V ! Œ1::jV j] which assigns a distinct integer to each word respecting the constraint .8w1 ; w2 2 V /Œrank.w1/ < rank.w2 / ” c.w1 / :> c.w2 /].
			Notice that, in the presence of ties, the rank function is not uniquely defined.
			Any valid function respecting the aforementioned constraint could be used.
			Therefore, we assume that lexicographic order is used to assign the ranks of words with identical numbers of occurrences, uniquely defining the rank function.
			The rank plot of Fig.
			3.1 is in logarithmic scale, otherwise it would be impossible to visualise the counts.
			The main characteristic of power laws is that there is a very large number of rare events.
			In BNCfrg, for example, there are 21,423 word types occurring only once in the corpus,10 that is, almost 57 % of the vocabulary.
			On the opposite end of the range, frequent words correspond to a tiny portion of the vocabulary.
			The graphic shows that the number of words decreases exponentially in the ranked vocabulary.
			In other words, Zipf ’s law states that the number of occurrences of a type in the corpus is inversely proportional to its position in the rank.
			A derived property is that the size of the vocabulary increases logarithmically with the size of the corpus.
			This is exemplified by plotting the number of types in a corpus as its size, that is, the number of tokens, increases, on shown in Figure 3.2.
			While the number of tokens increases linearly, the number of types increases fast at the beginning and much slower afterwards.
			This happens because, as the sample increases, many common words tend to be repeated while new words become harder to find.
			One could assume that, if an infinitely large corpus (the sample) was available, the size of the vocabulary would stop growing at some point, converging to the total number of words used in that language (the population).
			Fig.
			3.2 Tokens in the corpus versus types in the vocabulary 10 A word occurring once in the corpus is called a hapax, from the Greek hapax legomena.
			The statistics of a corpus depend on its size, language, genre, domain and sublanguage.
			Nonetheless, the logarithmic relation between the number of word tokens and the number of different word types holds as well as Zipf ’s law.
			This means that, in general, around half of the words in a corpus occur only once.
			Distributions like these are called large number of rare events (LNRE).
			When the underlying model is a LNRE distribution, specific statistical tools able to deal with sparse data must be employed.
			Besides, one needs to be careful because standard assumptions for a sample drawn from a population normally distributed do not apply to corpora.
			Operations like parameter estimation, hypothesis testing and the like need to be adapted when working with LNRE distributions.
			This has an impact on the types of lexical association measures that can be used for unsupervised MWE acquisition (Sect.
			3.1.4).
			For further details, one may refer to Baayen (2001).
			3.1.3 N -Grams, Language Models and Suffix Arrays When we consider word sequences, each token in the corpus is represented as wi , where the subscript i stands for its position with respect to other tokens.
			For instance, a sequence of n consecutive tokens in the corpus can be represented as 11 w1w2 ::: wn1wn . Such contiguous sequences are called n-grams.
			We use the abbreviated notation wj to represent an n-gram formed by j - i C 1 words wi through wj . By extension, the function c. / can be applied to n-grams and returns the number of times they occur in a corpus.
			For example, cBNCfrg .I will be/ D 5 because this 3-gram occurs 5 times in the corpus BNCfrg.
			An language model (LM) is a probabilistic model that estimates to what extent a group of words belongs to a certain language.
			An n-gram LM is a set of probability density functions that estimate the probabilities of any n-gram in a language.
			For instance, an n-gram like I will be paying is more plausible in English than will paying I be, thus the model will assign it a higher probability.
			LMs are widely employed not only in MWE acquisition but in many other NLP applications like speech recognition, OCR, spell checking, MT. They are often used to choose among several possible outputs because they simulate grammatical and semantic preferences in sentences.
			One way to estimate n-gram probabilities is to learn them from a sample of language: the training corpus.
			We can count all the n-grams in the training corpus and then return as a probability estimates the relative frequencies of the n-grams.
			For instance, according to Table 3.1, the BNCfrg corpus contains 414;602 tokens or unigrams.
			If we use BNCfrg as training corpus, the probability estimate p of the 11 Discontiguous sequences are sometimes referred to as flexigrams, that is, n-grams with gaps..
			unigram Mary is p.Mary/ 18 and the probability estimate of the 3-gram I 414;602 c.I will be/ 5 will be is p.I will be/ � N D 414;602 . Although a good idea in theory, it is not feasible to store all the counts for each distinct n-gram of arbitrary length (1 to N ) in a large corpus, as the number of n-grams grows quickly.
			For instance, BNCfrg contains 37,651 unigrams, 210,183 2-grams, 346,450 3-grams and so on.
			In order to solve this practical problem, we first apply the probability chain rule, that is, for an arbitrary n-gram: n p.wn / D p.w1/ p.w2 jw1/ p.w3 jw2/ ::: p.wn jwn1 / D p.w1 / Y p.wk jwk1 / 1 1 1 kD2 1 (3.1) We further simplify calculations by applying the Markov assumption in order to approximate the conditional probability of a token given a short history instead of using the whole preceding sequence.
			That is, given m > 1 as the fixed maximum size of n-gram that we can store, we ignore all words preceding wkmC1.
			The order m of the model typically ranges from 2 to 5 according to the target application.
			This simplification assumes that the presence of a word only depends on a short number of words to the left of it, completely ignoring the right context.
			p.wk jwk1/ � p.wk jwk1 / (3.2) 1 k-mC1 For instance, let us consider a model of order m D 2, built using the BNCfrg corpus as training data.
			Given this model, we want to estimate the probability of the 4-gram I will be visiting.
			Thus, p.I will be visiting/ D p.I/ p.willjI/ p.bej will/ p.visitingjbe/ D c.I/ c.I will/ c.will be/ c.be visiting/ 2;816 34 312 1 N c.I/ c.will/ c.be/ D 414;602 2;816 1;093 2;535 D 0:000000009.
			This model uses the principle of maximum likelihood estimation (MLE), that is, it assumes that the sample is the population.
			In other words, the chosen model parameters are those that maximise the likelihood of the observed sample.
			The problem with MLE is that it does not take into account n-grams that were not observed in the corpus as a side effect of sampling a very large event space.
			In other words, no matter how large a training corpus is, a large number of perfectly valid n-grams will surely be missing from the model, thus yielding zero probability for the whole product.
			In order to solve this problem, current LM tools implement sophisticated smoothing techniques.
			The idea of smoothing is to assign some probability mass to unseen events, discounting it from the probabilities of seen n-grams (Chen and Goodman 1999; Good 1953; Kneser and Ney 1995).
			Furthermore, it is also possible to use backoff in order to estimate the probabilities of larger unseen n-grams by combining the probabilities of smaller n-grams contained in them.
			Because such techniques are rarely employed in empirical MWE acquisition from corpora, we Fig.
			3.3 (a) Example of suffix tree.
			(b) Example of suffix array will not discuss their details here.
			One of the rare works concerning smoothing for MWE acquisition is that of Lapata (2002).
			When dealing with very large corpora, it is crucial to have efficient access to n-gram counts in order to estimate their probabilities.
			The intuition behind quick access to n-gram counts in a corpus is to organise the n-grams in a data structure that allows fast search (that is, direct access or binary search).
			N -gram models with a fixed order m can be represented using structures based on suffix trees.
			A suffix tree is a representation in which each edge is labelled with a word and each node contains a count.
			Concatenating the words on the edges of a path from the root to a node ni generates an n-gram whose count is stored in ni . For example, in Fig.
			3.3a, the path I will be leads to a node containing the value of c.I will be/ D 5.
			In order to optimise the access to the child nodes, it is possible to build hash tables for constant access or ordered lists for binary search.
			While suffix trees are appropriate for LMs with fixed order, counting arbitrarily long n-grams requires another kind of data structure.
			A suffix array is an efficient structure to represent n-grams of arbitrary size (Manber and Myers 1990; Yamamoto and Church 2001).
			The corpus is viewed as an array of N words w1 to wN . Each word wi is the beginning of a corpus suffix of size Ni C 1, for instance, wN2 wN1 wN is a suffix of size 3.
			The trick is that the list containing all the N suffixes is sorted in lexicographic order.
			Therefore, one can perform binary search in order to locate the first and the last positions starting with the searched n-gram.
			For example, in Fig.
			3.3b we represent part of a suffix array of BNCfrg.
			If we want to know how many times the n-gram I will be occurs in the corpus, we will perform two binary searches in O.log N/ time to find the first index F and last index L in the array containing a suffix which starts with the searched n-gram.
			The number of occurrences of the n-gram is then simply L - F C 1 D 108104 C 1 D 5.
			If now we need to obtain the count for I will, we repeat the procedure and find 133100 C 1 D 34.
			Table 3.3 Top-15 most frequent n-grams in BNCfrg r c.w 1 w2 / w1 w2 c.w 1 / c.w 2 / E.w 1w2 /t sco re 1 3,06 0 of the 11,9 23 20,7 65 597.
			2 44.5 2 1,78 8 in the 6,7 58 20,7 65 338.
			5 34.3 3 1,13 9 to the 9,8 30 20,7 65 492.
			3 19.2 4 77 2 on the 2,5 50 20,7 65 127.
			7 23.2 5 73 8 and the 9,7 71 20,7 65 489.
			4 9.2 6 73 3 to be 9,8 30 2,5 35 60.
			1 24.9 7 68 7 for the 3,2 48 20,7 65 162.
			7 20.0 8 52 6 at the 1,7 82 20,7 65 89.
			3 19.0 9 52 5 by the 1,8 99 20,7 65 95.
			1 18.8 10 50 0 that the 4,3 51 20,7 65 217.
			9 12.6 11 47 3 of a 11,9 23 7,3 46 211.
			3 12.0 12 45 7 fro m the 1,5 32 20,7 65 76.
			7 17.8 13 45 6 with the 2,4 05 20,7 65 120.
			5 15.7 14 36 9 it is 3,0 64 4,0 29 29.
			8 17.7 15 36 2 in a 6,7 58 7,3 46 119.
			7 12.7 In our implementation (see Sect.
			5.1.2) each suffix is represented with an integer index pointing to the position in the corpus where it starts, thus optimising memory use.
			Thus, a suffix array uses a constant amount of memory with respect to N : if every word and every word position in the corpus is encoded as a 4-byte integer, a suffix array uses precisely 4 2 N bytes, plus the size of the vocabulary, which is generally very small if compared to N . 3.1.4 Lexical Association Measures The principle of corpus-based MWE acquisition is that words that form an expression will co-occur more often than if they were randomly combined by a coincidence of syntactic rules and semantic preferences.
			In this context, lexical association measures are applied to n-gram counts in order to estimate how much the occurrences of two or more words depend on each other.12 A simple method to acquire MWEs from corpora is to use ranked n-gram lists.
			For example, Table 3.3 lists the 15 most frequent n-grams of BNCfrg.
			Unfortunately, all of the returned items are uninteresting combinations of function words like determiners the and a, prepositions and auxiliary verbs.
			Moreover, the list only contains 2-grams and no 3-grams and larger n-grams.
			This is a consequence of the fact that the count of a larger n-gram will always be less than or equal to 12 The term association measure is standard in MWE acquisition, but it would be more appropriate.
			to talk about association scores instead, since not all the scores discussed here are proper measures.
			the count of the n-grams that it contains, thus biasing the acquisition towards short n-grams.
			We could solve these problems by separately acquiring n-grams of different lengths, using regular expression patterns to filter out sequences of function words contained in stopword lists or matching unwanted POS tags.
			This is actually performed in many real-world systems, specially for automatic terminology acquisition, with surprisingly good results (Justeson and Katz 1995; Ramisch 2009).
			However, if we are to acquire general MWEs (and not only multiword terms), we need a more sophisticated way to tell whether an n-gram is just a random co-occurrence of frequent words or whether it presents some statistical idiomaticity.
			A common preprocessing step when dealing with n-gram counts is to eliminate all combinations that occur less than a fixed threshold.
			This is important because statistics tend not to be reliable in low frequency ranges.
			As the counts decrease, it is impossible to distinguish statistically significant events from coincidences due to sampling error.
			Unfortunately, there is no rule or algorithm for determining the value of such threshold except common sense and trial and error.
			For example, statistics calculated over hapax are surely unreliable while setting the threshold at 100 occurrences will probably result in too little data (if any).
			3.1.4.1 Measures Based on Hypothesis Testing Now, in order to investigate whether an n-gram is a MWE, let us assume that words are combined randomly.
			That is, the occurrence of words at given positions are independent events.
			This hypothesis does not hold, otherwise languages would have no grammar.
			Nonetheless, it provides a powerful way to test the association strength between words.
			By the definition of statistical independence, if the occurrence of a word w2 does not depend on the occurrence of the preceding word w1 , then we expect that the joint probability of the 2-gram is the product of the probabilities of the individual events, that is: p.w2 / D p.w1/ p.w2 / (3.3) For the sake of simplicity, let us use MLE estimators for the probabilities of the c.wi / individual words through relative frequencies, that is p.wi / D . Then, for an.
			N arbitrary n-gram wn , the expected relative frequency would be the probability: p.wn / D c.w1 / N c.w2 / N ::: c.wn / N D c.w1 / c . w 2 / : : : c . w n / N n (3.4) We can scale this probability estimate by the approximate number of n-grams in the corpus (N - n C 1 � N ) to obtain the expected count E.wn /: E.wn / D N c.w1 / c.w2 / ::: c.wn / N n c.w1 / c.w2 / ::: c.wn / D N n-1 (3.5) Column 6 of Table 3.3 shows the values of E.wn / for the top-30 most frequent 2-grams in BNCfrg.
			Combinations of frequent words are expected to occur frequently while combinations involving rarer words are expected to occur less.
			One way to test whether the difference between the expected count E. / and the observed count c. / is statistically significant is to use a hypothesis test.
			Our null hypothesis H0 is that an n-gram will occur as many times as we expect it to occur, and our alternative hypothesis H1 is that the observed number of occurrences c.wn / is actually greater than the expected number of occurrences: H0 W c.wn / D E.wn / 1 1 H1 W c.wn /> E.wn / 1 1 Our goal is to reject this null hypothesis using a one-tailed test.
			If we can reject H0 at some significance level, this means that the number of occurrences of the n- gram is above what we expected, and we have good chances of finding a MWE.
			In theory, we should perform an exact test, using the binomial or Poisson test statistics,13 that model the discrete distribution of n-gram counts (Evert 2004).
			For instance, a binomomial test statistic uses the binomial distribution to estimate the probability of observing c.wn / or more occurrences of an n-gram given its expected n 1 1 N N N ! E.wn / k E.wn / N -k p.X > c.wn // D X kDc.wn / k 1 N 1 1 N (3.6) In practice, however, this test statistic is computationally costly.
			The number of terms in the sum, N - c.wn / C 1 is prohibitive in most cases because N is much larger than c.wn /.
			Moreover, for large values of N , the binomial distribution can be approximated by a normal distribution.
			As a consequence, it is possible use a z test n n statistic instead of the exact binomial test statistic: p.X > c.wn // D c.w1 /-E.w1 / . 1 E.wn / A very common test statistic employed in MWE acquisition is Student’s t test statistic, a heuristic variation of the z test statistic in which the variance of the sample is estimated through its observed count c.wn / rather than from the expected count E.wn /.
			This approximation holds if we consider the corpus as a sequence of randomly generated n-grams and a Bernoulli trial that assigns 1 to the occurrence of wn and 0 otherwise.
			Then, the probability p of generating 1 is the mean of the sample, xN D p D c .wn / . For small values of p, s p .1 p/ p, thus the variance of the sample s2 is equivalent to the mean xN . Finally, the estimated theoretical mean Jl is the normalised estimated count E.w1 / , thus yielding the following formulation for the t test statistic: 13 The test statistic is a random variable with a known distribution, from which we can obtain the.
			p-value.
			If the p-value is below a certain significance level, we can reject the null hypothesis.
			c.wn / E.wn / x - Jl 1 1 c.wn / - E.wn / t D N N N 1 1 D D (3.7) q s2 N q c.wn / N 2 pc.wn / As we have seen in Sect.
			3.1.2, word counts do not follow a normal distribution, but they can be modelled using a power law distribution, and the same applies to n-grams.
			As a consequence, from a theoretical perspective, the use of Student’s t test statistic here does not make sense as it assumes that the c.wn / follows a normal distribution.
			Nonetheless, most of the time in MWE acquisition, our goal is to rank candidate n-grams according to their association strength.
			Thus, the value of the t test statistic is not used to calculate the p-value, but is used directly as a ranking criterion.
			This ranking criterion is called the t-score, and it is interpreted as follows: a large value means strong word association and thus a potential MWE, a small value means that n-gram is more likely to be a random word combination, thus uninteresting for MWE acquisition.
			Notice that, for the examples in Table 3.3, the statistic is larger when the combination is composed of rarer words.
			3.1.4.2 Measures Based on Pointwise Comparisons The t-score is an example of lexical association measure (AM), that is, a numerical score that estimates the degree of dependence or association strength between the number of occurrences of the n-gram and the number of occurrences of the individual words that compose it.
			Similarly to n-gram counts, when more than one corpus is involved, we will subscribe the name of the association measure with the name of the corpus from which the counts used to calculate it were obtained, like in t-scoreBNCfrg . In addition to the t-score, there are many other proposed measures in the literature, not necessarily based on hypothesis testing.
			Church and Hanks (1990), for instance, suggest to use pointwise mutual information (pmi), a notion coming from information theory which estimates the predictability of a word given the preceding words.
			In other terms, pmi quantifies the discrepancy between the observed count and the expected count: c.wn / pmi D log2 1 1 (3.8) This AM has its maximal value when the two words only occur together, that is c.w1 / D ::: D c.wn / D c.wn /.
			One disadvantage of pmi is that there is no lower bound for its value, since log 0 is undefined.
			Another disadvantage is that this score requires hard frequency thresholds, as it overestimates the importance of rare n-grams.
			For instance, the maximal value can be obtained for any n-gram with c.w1 / D ::: D c.wn / D c.wn / D 1.
			Another commonly employed AM is Dice’s coefficient, a classical score used in information retrieval to calculate the similarity between two sets.
			The original version of Dice’s coefficient divides the size of the intersection between n sets (scaled by n) by the sum of their individual sizes.
			In the case of n-grams, we adapt this measure to compare the n-gram count (intersection between all words) with the sum of the counts of the individual words contained in it: n c.wn / dice D 1 i D1 c.wi / (3.9) These scores are applicable to arbitrary-length n-grams, but they only take into account the observed number of occurrences c.wn /.
			In spite of their simplicity, they are quite popular in MWE acquisition and in other NLP tasks.
			3.1.4.3 Measures Based on Contingency Tables Nonetheless, more robust and theoretically sound AMs exist for the special case of 2-grams.
			These measures are based on contingency tables, that is, a representation like the one showed in Table 3.4, in which we consider the occurrence of two words as two random variables.
			We denote as :wi the occurrence of any word different from wi . Notice that all the cell values are derived from the count of the 2-gram c.w1 w2 /, the individual word counts c.w1 /, c.w2 / and the total number of tokens in the corpus N . The values in the last row represent the sum of the values of the inner cells, and analogously for the last column.
			These are often called marginal counts because they are written in the margins of the contingency table.
			The value of the cell in the last row and column corresponds the number of elements in the sample N , and is equivalent to the sum of the marginal counts in both directions.
			For every cell in the contingency table, it is possible to calculate the equivalent expected value if the occurrences of the two words were independent events, as follows: 8wi 2 fw1; :w1 g; 8wj 2 fw2; :w2 g; E.wi wj / D c.wi / c.wj / (3.10) N Table 3.4 Contingency table for two random variables: the occurrence of the first word w1 and the occurrence of the second word w2 , :wi denotes the occurrence of any word except wi w2 :w2 w1 c.w 1w2 / c.w 1 :w2 / D c.w1 / - c.w1 w2 / c.w1 / :w1 c.: w1w 2 / D c.w2 / - c.w1 w2 / c.: w1 :w2 / D N - c.w1 / - c.w2 / C c.w1 w2 / c.: w1 / D N - c.w1 / c.w 2/ c.: w2 / D N - c.w2 / N We can employ the X 2 test statistic in order to estimate whether the difference between observed and expected contingency tables is statistically significant, as we did for the count c.wn /.
			The X 2 test statistic is a scaled mean squared error measure between observed and expected cell values.
			That is, for all values of wi 2 fw1; :w1 g and wj 2 fw2; :w2g, X 2 D X wi ;wj Œc.wi wj / - E.wi wj /]2 E.wi wj / (3.11) The X 2 test statistic for two random variables has an asymptotic x2 distribution with one degree of freedom.
			Thus, it is possible to obtain the p-value which, if sufficiently small, indicates a significant difference between the tables.
			However, as for the t test, usually the test statistic is considered by itself as a ranking criterion.
			A very popular AM based on contingency tables is the log-likelihood ratio (ll).
			It was proposed for the first time for MWE acquisition by Dunning (1993).
			This measure is preferable over X 2 because, for small samples with LNRE distributions, it provides more accurate association estimators, as demonstrated through numerical simulation by Dunning (1993).
			The simplified version of the ll AM is: ll D 2 X c.wi wj / log wi ;wj c.wi wj / E.wi wj / (3.12) This measure has the advantage that, in addition to being theoretically sound, numerically simple and robust to low frequencies, it has a simple interpretation.
			Its value equals the number of times the 2-gram is more likely under the hypothesis that the words are not independent than the individual counts would suggest.
			While on the one hand ll is robust and theoretically sound, on the other hand it is only applicable to the case where n D 2.
			Extensions to larger n-grams, although possible, are far from being intuitive (see the documentation of the NSP package, described in Sect.
			3.2.3.1, for an example).
			3.1.4.4 Other Measures There are numerous AMs available for MWE acquisition.
			Pecina (2008) presentsa table containing 84 measures among which some are rank-equivalent to each other.
			Hoang et al.
			(2009) propose and evaluate the adaptation of traditional AMs for word pairs in which one word is very frequent and the other is rather rare, like it is the case for English phrasal verbs formed by rare verbs (e.g., nail) with frequent prepositions and adverbs (e.g., down).
			Table 3.5 shows the top-15 n-grams acquired from BNCfrg as ranked by some of the AMs presented here.
			A threshold of at least 3 occurrences was set to reduce noise.
			The first measure, the t-score, seems to retrieve rather long specialised MWEs like proper names (Unix System Laboratories Inc) and terminological Table 3.5 Top-15 n-grams (2–5) extracted from BNCfrg and ranked according to AMst sco re pmi dic e ll Net earn ings per shar e amo unte d of the CH AN CE RY DIV ISI ON of the repo rted first quar ter net prof it in the hom ocli nic orbi ts in the Mic roso ft Cor p ’s Win dow s NT , and Los Ang eles , but (7) mm Hg to be You rs sinc erel y to be or fum e or othe r imp urit y , but Gre enh ouse Effe ct I ’m earn ings per shar e amo unte d to on the Hon g Kon g have been 7 ) mm Hg in for the gon na do n’t dust or fum e or othe r .’ inter alia , and has repo rted first quar ter net to the Kh mer Rou ge will be Œ CH AN CER Y DIV ISI ON ] at the Inla nd Rev enu e the the Inc has repo rted first quar ter by the Sri Lan ka per cent ; Œ 199 1 ] 2 fro m the Cru z Ope ratio n , , N C V O it is per cent has been Uni x Syst em Lab orat orie s Inc will be Mol ecul ar biol ogy on the you ’re gon na get it’s Win ston Chu rchil l the . phraseology (reported first quarter net profit).
			The list illustrates one of the problems with n-gram based methods: the extraction of nested expressions, that is, a shorter expression like first quarter contained in a larger one like first quarter net profit.
			Delimiting the borders of a MWE is a current challenge in acquisition tools and methods.
			The dice coefficient, on the other hand, retrieves shorter n-grams among which we find many MWE types like proper names (Sri Lanka, Winston Churchill), noun compounds (Greenhouse Effect, molecular biology), formulaic sequences (Yours sincerely) and fixed expressions (per cent, inter alia).
			Both t-score and dice tend to retrieve rarer sequences, which only occur three to four times in the corpus.
			The other two measures seem to fail in extracting any interesting MWE, as they give much weight to frequent combinations of function words.
			The ll score retrieves some cases of rare double commas or double the determiners.
			Most of the applications of pmi and ll in the literature are targeted, as these AMs are used to classify possible collocates for a given fixed word, and not to blindly acquire unknown MWEs from a corpus (Dunning 1993; Church and Hanks 1990).
			The unfortunate reality in AMs for MWE acquisition is that sometimes the most theoretically sound measures perform worse than intuitive heuristics.
			This example is an illustration of how AMs work and shows that their results are complementary, suggesting that their combination should be envisaged for broad coverage acquisition.
			Although there is some published work on fair comparisons among AMs (Pearce 2002; Evert and Krenn 2005; Wermter and Hahn 2006; Schone and Jurafsky 2001), this falls out of the scope of our work and is not the goal of our example.
			Moreover, the measures have different weaknesses: some overestimate the importance of rare n-grams while others are not capable of dealing with frequent items.
			Thus, different count thresholds should be applied for each AM, specially for such a small corpus as the BNCfrg.
			In addition, further cleaning of function words and punctuation is an easy step that should be performed in any case.
			Besides association measures, there are also other types of statistical measures that can be used as evidence for MWE discovery in corpora.
			Pecina (2005), for example, discovered that context measures that consider the adjacent words of the n-grams are more adequate to acquire idiomatic expressions.
			In terminology acquisition, contrastive measures like C-NC and csMWE are employed as a way of verifying the pertinence of the n-gram to the target domain (Frantzi et al. 2000; Bonin et al. 2010a).
			These measures estimate the difference between the occurrences of an n-gram in a specialised corpus and in a generic corpus (also called the contrastive corpus).
			They are recommended to identify terms that occur frequently in a specialised corpus but not in general language (see Sect.
			5.1.2.4) For further material on AMs, please refer to Evert (2004), Seretan (2011), and Pecina (2008).
			A summary of common association measures can also be found on Stefan Evert’s website http://www.collocations.de/.
			3.2 Methods for Automatic MWE Acquisition.
			The tasks involved in the computational treatment of MWEs have been structured by the organisers of the 2009 MWE workshop (Anastasiou et al. 2009) as follows.
			• Identification (or acquisition).
			Given a text as input, try to locate the interesting multiword units in it.
			• Interpretation.
			Given a multiword unit out of context, try to discover its internal structure both in terms of syntactic and semantic relations.
			• Disambiguation.
			Given a multiword unit in its context, try to classify it with respect to a closed set of categories.
			Typically, one tries to distinguish literal from idiomatic uses, but other disambiguation tasks are possible, for instance, distinguishing general-purpose from specialised uses and performing multiword sense disambiguation.
			• Application.
			Given a lexicon of MWEs, try to integrate it in another application such as parsing, information retrieval or MT. Interpretation and disambiguation are similar as both can be modelled as classification tasks.
			However, they are distinct as the former concerns MWE types whereas the latter deals with MWE tokens as they occur in text.
			In addition to the four topics above, we consider an additional task which lies between disambiguation and application, representation: • Representation.
			Given a lexicon containing MWEs (automatically or manually acquired), try to optimise their representation in a given formalism considering their properties and the target application.
			As pointed out in the call for papers of the MWE 2009 workshop14: The above topics largely overlap.
			For example, identification can require disambiguating between literal and idiomatic uses since MWEs are typically required to be non-compositional by definition.
			Similarly, interpreting three-word noun compounds like morning flight ticket and plastic water bottle requires disambiguation between a left and a right syntactic structure, while interpreting two-word compounds like English teacher requires disambiguating between (a) ‘teacher who teaches English’ and (b) ‘teacher coming from England (who could teach any subject, e.g., math)’.
			As a large part of the research developed and presented in this book focuses on the first task, the present section is entirely dedicated to MWE acquisition.
			We start with a summary of related work on monolingual acquisition in Sect.
			3.2.1, and on multilingual acquisition in Sect.
			3.2.2.
			Then, we present a more practical description of tools that perform automatic acquisition, distinguishing between those freely available developed by academic researchers and those which were developed and commercialised by companies.
			3.2.1 Monolingual Methods In this section, we discuss some relevant papers on monolingual MWE acquisition methods.
			The references discussed here are complemented by the work that has been developed for other MWE tasks (Sect.
			3.3).
			One of the goals of monolingual MWE acquisition techniques is to help and speed up the creation of lexical resources such as printed or machine-readable dictionaries and thesauri containing multiword entries.
			We distinguish two types of acquisition tasks: • MWE extraction.
			The input is a text and the expected output is a list of MWE candidates found in the text.
			The evaluation can be done on a type basis, as if each expression was an entry of a lexicon, independently of the input corpus.
			In extraction, it is usual to consider two separate steps: (a) candidate extraction and (b) candidate filtering and/or ranking.
			We consider that an MWE candidate is a sequence of words which has some of the characteristics described in Sect.
			2.3 as measured by some objective measure, but that was not yet validated by a manual or automatic evaluation process.
			• In MWE identification.
			The input is a text and the expected output is a markup indicating the places where MWEs occur.
			This may include the use of an existing dictionary or the discovery of new MWEs.
			What makes MWE identification more difficult than simple regular-expression matching is non- adjacency, morphological inflection and ambiguity of some MWEs that can be used both as compositional and idiomatic sequences (e.g., look up as consult 14 http://multiword.sourceforge.net/mwe2009 a dictionary or as staring towards a higher position).
			In MWE identification, a token-based evaluation is required, taking into account the context in which the expression occurs.
			Candidate extraction methods are usually based on some kind of pattern matching, where the patterns range from simple n-grams to structured sequences of part-of-speech tags and syntactic relations.
			The level of linguistic information employed in candidate extraction depends on various factors such as the language, the type and syntactic variability of the target MWEs and the available analysis tools.
			The use of surface forms alone is rare, as generally at least minimal patterns based on stopwords or POS are employed (Gurrutxaga and Alegria 2011).
			However, there might be cases where flat n-gram extraction is required, for instance, when the target MWEs are generic keyphrases for document description and indexation (Silva and Lopes 2010).
			The sliding window method consists of considering as MWE candidates pairs that co-occur in a window of at most w words, thus retrieving discontiguous n-grams (Smadja 1993).
			The extraction of candidates using sliding windows can pose a challenge in terms of computational performance.
			Indeed, optimised data structures and algorithms must be used because the number of possible combinations, even for relatively small sizes of n, explodes with the size of the corpus (Gil and Dias 2003).
			Part of speech sequences are one of the major approaches in candidate extraction because (i) many languages have available push-button POS taggers and (ii) this approach provides good results when the target constructions are relatively rigid in terms of word order, like fixed phrases and nominal MWEs.
			POS sequences have been used originally in multiword terminology acquisition (Justeson and Katz 1995; Daille 2003), but have also been applied to the extraction of other MWE types, specially noun compounds (Vincze et al. 2011).
			Even when dealing with more variable constructions such as verbal expressions, POS tag patterns can be used in the absence of syntactic information (Baldwin 2005; Duran et al. 2011).
			POS patterns can be defined based on various criteria, from linguistic intuition and expert knowledge (Bonin et al. 2010b) to systematic empirical observation of a sample (Duran et al. 2011).
			Sequences of POS can also be automatically learnt from annotated corpora, using the same methodology as for words, that is, by maximising some AM on the extracted POS n-grams (Dias 2003).
			When a parser is available, patterns based on syntactic relations can be used for candidate extraction and/or identification.
			For example, one may retrieve all candidates that are formed by a noun which is the direct object of a verb (take/V DOBJ time/N).
			According to the accuracy of the parser, simple syntactic patterns can be more precise than POS sequences, specially in the extraction of non-fixed MWEs like “true” collocations (Seretan 2008; Seretan and Wehrli 2009; Seretan 2011).
			Lexicons containing more or less refined MWE representations can be used to identify MWEs during parsing, with the advantage of disambiguating instances in context (Bejcˇek et al. 2013; Constant et al. 2013).
			Tree substitution grammars can also be used in order to learn syntactic MWE models from annotated corpora, as in the French version of the Stanford parser (Green et al. 2011).
			Regardless of the syntactic information and labels, structural regularities in parsing trees can also be used to retrieve MWE candidates using a minimal description length algorithm (Martens and Vandeghinste 2010).
			Often, these identification techniques require an annotated corpus with MWE markup.
			While some MWE types may be represented in existing treebanks, in most cases an additional annotation layer must be added (Laporte et al. 2008; Uresova et al. 2013).
			In addition to analysed corpora, other monolingual and multilingual resources can be used for MWE acquisition.
			For instance, by comparing the titles of Wikipedia pages using cross-language links, it is possible to detect multiword titles whose translation in one of the other languages is a single word (Attia et al. 2010).
			Another way to use the web as a source of information for MWE acquisition is to generate candidates according to generic combination rules and further validate them using web search engine hit counts (Villavicencio et al. 2005).
			This is explored in our experiments in Sect.
			6.2.3.2.
			The current trend is the integration of several complementary information sources (including linguistic analysis, statistics, the web) in order to maximise the recall of the extraction (de Medeiros Caseli et al. 2009; Attia et al. 2010).
			More complex candidate extraction methods, not based on pattern matching, have also been proposed.
			The LocalMaxs algorithm, for instance, performs extraction based on the maximisation of an AM applied to adjacent word pairs.
			Thus, it naturally handles nested expressions, extracting maximal sequences that recursively include adjacent words while the overall AM score increases (Silva and Lopes 1999).
			Similarly, a tightness measure is used in a Chinese IR system for the automatic identification, concatenation and optimised querying of strongly associated word sequences (Xu et al. 2010).
			Duan et al.
			(2006) propose a string matching algorithm, inspired by computational biology, to extract sequences that occur recurrently throughout the corpus.
			Sentences are viewed as DNA sequences and a dynamic programming algorithm is used to match corresponding parts for each sentence pair in the corpus, taking into account gaps that represent variable parts of the expression.
			These techniques generally do not distinguish candidate extraction from filtering, performing both simultaneously.
			As for candidate filtering in MWE extraction, some straightforward procedures are the use of stopword lists and of count thresholds to remove candidates for which statistical information is insufficient.
			Lexical association measures like those described in Sect.
			3.1.4 are also widely employed to rank the candidates and keep only those whose association score is above a certain threshold (Evert and Krenn 2005; Pecina 2005).
			When several AMs are available, they can be combined using machine learning, possibly considering additional information coming from auxiliary resources.
			Thus, it is necessary to annotate part of the MWE lists to obtain an annotated extraction dataset (Grégoire et al. 2008).
			Then, a supervised learning method can be used to build a classifier modelling the optimal weights of all the AMs and extra features (Ramisch et al. 2008; Pecina 2008).
			There is a strong predominance of methods based on 2-grams (or more generally on word pairs, not necessarily adjacent) in current techniques for monolingual MWE acquisition.
			This is justified because (i) the majority of the interesting and challenging MWEs are formed by two words and (ii) “experiments with longer expressions would require processing of much larger amount of data and [there is a] limited scalability of some methods to [handle] high order n-grams” (Pecina 2005).
			While this seems like a reasonable justification to keep the methodology simple, it does not correspond to the reality of NLP applications, where many MWEs longer than two words also require proper treatment (Nakov and Hearst 2005; Kim and Nakov 2011).
			Monolingual methods have been developed in several languages and are sometimes language independent.
			The advantage of language-independent methods is that they do not depend on the availability of a specific resource (POS tagger, parser) and can thus be applied to virtually any language, including poorly resourced ones.
			On the other hand, the use of linguistic information generally improves the precision and the coverage of the acquisition.
			Finding an adequate trade-off between language independence and quality when designing a method for monolingual acquisition is a challenging problem.
			However, as MWEs seem to be a universal phenomenon, being present in all human languages, it is important to build methods and evaluate them in multilingual contexts (Seretan and Wehrli 2006).
			3.2.2 Bi- and Multilingual Methods Even though many of the methods described in the previous section can be applied to arbitrary corpora, independently of the language, they are still considered as monolingual methods because the result is a list of MWEs or a marked up text with no cross-lingual correspondences.
			The extraction of bilingual MWEs is a task in which the resulting list of expressions is bilingual, that is, if a candidate is returned in one language, it contains translation links which relate it to its correspondent candidate(s) in the other language(s).
			Hence, bi- and multilingual MWE acquisition is different from language-independent MWE acquisition.
			Existing techniques for bilingual MWE acquisition are frequently based on parallel corpora.
			Automatic word alignment can provide lists of MWE candidates by themselves, as described in de Medeiros Caseli et al.
			(2010).
			They aligned a Portuguese–English corpus in both directions using GIZA++, and then joined the alignments using the grow-diag-final heuristic.
			Word sequences of two or more words on the source side aligned to sequences of one or more words on the target side were filtered using several stopword patterns and the resulting candidates were considered as MWEs.
			The comparison with a simple monolingual n-gram method showed that alignment- based extraction is much more precise, but has very limited recall.
			This technique has been further validated for the acquisition of Portuguese multiword terms in technical and scientific domains (Ramisch et al. 2010).
			Tsvetkov and Wintner (2010) extended this method with two main improvements: (a) they consider all non-bijective 1:1 alignments as candidate sequences and (b) they validate their candidates using an association measure, PMIn , calculated on a large monolingual corpus.
			Furthermore, they combine alignment information with other linguistic sources (Tsvetkov and Wintner 2011).
			They evaluate their method by integrating the automatically extracted bilingual lexicon in a statistical MT system, obtaining small but significant improvements in translation quality.
			Bai et al.
			(2009) present an algorithm capable of mining translations for a given MWE in a parallel aligned corpus.
			Then, the different translations are ranked according to standard association measures in order to choose the appropriate one.
			They integrated this extraction method into the statistical MT system Moses for the English–Chinese language pair, obtaining improved translations when compared to a baseline.
			The automatic discovery of non-compositional compounds from parallel data has been explored by Melamed (1997).
			Considering a statistical translation model, he introduced a feature based on mutual information and proposed an iterative algorithm that retrieves an increasing number of compounds.
			These can in turn be used to improve the quality of the statistical translation system itself.
			Conversely, it has been shown that MWEs can improve the quality of automatic word alignment.
			The EnglishHindi language pair presents large word order variation, and it has been shown that MWE-based features that model compositionality can help reduce alignment error rate (Venkatapathy and Joshi 2006).
			When compared with baseline GIZA++, a system enriched with MWE features obtains significantly lower error rates, from 68.92 to 50.45 %.
			The acquisition of bilingual verbal expressions requires not only the availability of parallel corpora, but also of syntactic analysis of both languages.
			Zarrieß and Kuhn (2009) used syntactically analysed corpora and GIZA++ alignments to extract verb-object pairs from a German–English parallel corpus.
			They considered a candidate as a true MWE if (i) a verb on the source side was aligned to a verb on the target side, (ii) the noun heading the object of the verb on the source side was tagged as a noun on the target side and (iii) there was a syntactic object relation on the target side between the target verb and the target noun.
			Their method retrieves 82.1 % of correct translations, and almost 60 % of translations which can be considered as MWEs.
			Bouamor et al.
			(2012) also perform bilingual MWE acquisition using word alignments, but instead of using them as a starting point, they use them a posteriori, after monolingual extraction has been performed on source and target corpora.
			Therefore, acquiring a bilingual MWE lexicon is seen as an alignment problem between expressions acquired separately in each language.
			Instead of relying on large parallel word-aligned corpora, which are not always available for a given language pair, it is possible to use comparable corpora as a source for acquisition.
			Daille et al.
			(2004) performed multiword term extraction independently in French and in English using comparable corpora in the environmental domain.
			Then, using the distances between the context vectors of the acquired terms, they obtained cross-lingual equivalences that were evaluated against a bilingual terminological dictionary.
			The dictionary reference translation occurred among the top-20 retrieved translations in 47–89 % of the translations, depending on the translation relation type (single word vs multiword).
			The acquisition of bilingual MWEs has been explored more often in the context of machine translation.
			In Sect.
			3.3.4.4, we provide an overview of attempts to integrate MWEs into different MT applications.
			This is further developed in the experiments described in Sect.
			7.2.
			3.2.3 Existing Tools The maturity of a research field depends not only on theoretical models and experimental results, but also on concrete tools and available software on the basis of which it is possible to reproduce results, build extensions and perform systematic evaluations.
			Thus, tools for the automatic acquisition of MWEs are very important for the evolution of this research field.
			Here, we distinguish two types of tools: those which are freely available for the community (Sect.
			3.2.3.1) and those that are either commercial or available in restricted contexts (Sect.
			3.2.3.2).
			3.2.3.1 Freely Available Tools To date, the existing research tools follow the main trends in the area, using linguistic analysis and statistical information as clues for finding MWEs in texts.
			Here, we present a list of freely available tools that can be used mostly for monolingual MWE acquisition.
			1.
			LocalMaxs: http://hlt.di.fct.unl.pt/luis/multiwords/.
			The “Multiwords” scripts are the reference implementation15 of the LocalMaxs algorithm.
			It extracts MWEs by generating all possible n-grams from a sentence and then further filtering them based on the local maxima of a customisable AM’s distribution (Silva and Lopes 1999).
			On the one hand this approach is based purely on word counts and is completely language independent.
			On the other hand, it is not possible to directly integrate linguistic information in order to target a specific type of construction or to remove noisy ungrammatical candidates.16 The tool includes a strict version, which prioritises high precision, and a relaxed version, which focuses on high recall.
			A separate tool is provided to deal with big corpora.
			A variation of the original algorithm, SENTA, has been proposed to deal with non-contiguous expressions (da Silva et al. 1999).
			However, it is computationally costly because it is based on the calculation of all possible n-grams in a sentence, which explodes when going from contiguous to 15 Recommended by the author of the algorithm in personal communication..
			16 Although this can be simulated by concatenating words and POS tags together in order to form.
			a token.
			non-contiguous n-grams.
			Furthermore, there is no freely available implementation.
	
	
			The N -gram Statistics Package (NSP) is a standard tool for the statistical analysis of n-grams in text files developed and maintained since 2003 (Pedersen et al. 2011; Banerjee and Pedersen 2003).
			It provides Perl scripts for counting n-grams in a text file and calculating AMs, where an n-gram is either a sequence of n contiguous words or n words occurring in a window of w :> n words in a sentence.
			While most of the measures are only applicable to 2-grams, some of them are also extended to 3- and 4-grams, notably the log-likelihood measure.
			The set of available AMs includes robust and theoretically sound measures such as Fischer’s exact test.
			The input to the NSP tool is a corpus and a parameter value fixing the size of the target n-grams.
			The output is a list of types extracted from the corpus along with the counts, which can further be used to calculate the AMs.
			Although there is no direct support to linguistic information such as POS, it is possible to simulate them to some extent using the same workaround as for LocalMaxs.16 The tool allows complex expressions in order to express what counts should be calculated in terms of the sub-n-grams contained in a given n-gram.
	
	
			The UCS toolkit provides a large set of sophisticated AMs, in addition to other mathematical procedures like dispersion test, frequency distribution models and evaluation methods.
			It was developed in Perl and uses the R statistics package.
			UCS focuses on high accuracy calculations for 2-gram AMs, but, unlike the other approaches, it does not properly perform MWE acquisition.
			Instead of a corpus, it receives a list of candidates and their respective counts, relying on external tools for corpus preprocessing and candidate extraction.
			Then, it calculates the measures and ranks the candidates.
			Therefore, the question about contiguous n-grams or support of linguistic filters is not relevant for UCS.
			4.
			jMWE: projects.csail.mit.edu/jmwe The jMWE tool (Kulkarni and Finlayson 2011) is aimed at dictionary-based in-context MWE token identification in running text, which makes it quite different from extraction tools.
			It is available in the form of a Java library, and expects a corpus as input, possibly annotated with lemmas and parts of speech.
			In addition, it requires an initial dictionary of valid known MWEs.
			The system then looks for instances (occurrences) in the corpus of the MWEs included in its internal dictionary.
			It does not perform any automatic discovery of new expressions, thus the quality of the output heavily depends on the availability of MWE dictionaries.
			While jMWE is not language independent, it can be configured and straightforwardly adapted to other languages for which a suitable dictionary is available.
			The system allows quite powerful instance search, similar to multilevel regular expressions.
			It is possible to deal with non-contiguous expressions and to apply successive filters on the output.
			jMWE also provides heuristics for disambiguating nested compounds.
			On the other hand, it is not possible to express constraints based on syntax, nor to apply AMs in order to remove words that co-occur by chance.
			5.
			Varro: http://sourceforge.net/projects/varro/.
			This tool is not specifically aimed at MWE acquisition, but rather at finding regularities in treebanks (Martens 2010).
			It implements an optimised version of the Apriori algorithm with many adaptations that allow for the efficient and compact representation of tree structures.
			Statistical scores based on description length gain have been proposed to rank regular subtrees returned by the tool, thus helping in the acquisition of MWEs (Martens and Vandeghinste 2010).
			In contrast with the preceding tools, the Varro toolkit is not based on word sequences, but it requires syntactically analysed corpora as input.
			It is thus well suited for the extraction of flexible expressions such as idioms, formulaic phrases, “true” collocations and verbal expressions.
			There are also numerous freely available web services and downloadable tools for automatic term extraction.
			These tools are generally language dependent, having versions for major European languages like English, Spanish, French and Italian.
			Although multiword terms are included in our definition of MWE, these tools are not appropriate for general-purpose extraction of expressions in everyday language.
			Examples of such tools are TermoStat,17 AntConc18 and TerMine.19 The Wikipedia page on terminology extraction20 lists many other freely available tools.
			The methodological framework introduced in the present work has also been implemented in a freely available tool, the mwetoolkit.21 This tool is described in detail in Chap.
			5.
			3.2.3.2 Commercial Tools There are numerous commercialised systems for automatic terminology extraction from specialised texts.
			As a great part of terminology is multiword, this kind of software performs MWE acquisition at some point.
			Déjean et al.
			(2002), for example, describe a method developed at Xerox that uses morphosyntactic patterns for monolingual term recognition.
			Afterwards, they perform automatic alignment and extract English–German terminology, reaching an F-measure of around 80 %.
			This kind of technique has certainly been integrated into their Xerox Terminology Suite (XTS).
			This software is not commercialised any more, since it has been 17 http://olst.ling.umontreal.ca/~drouinp/termostat_web/ 18 http://www.antlab.sci.waseda.ac.jp/software.html 19 http://www.nactem.ac.uk/software/termine/ 20 http://en.wikipedia.org/wiki/Terminology_extraction 21 http://mwetoolkit.sourceforge.net acquired by the text mining company Temis.22 Nowadays, it has become part of the LuxidQR information extraction package.23 Another large company which developed a tool for terminology extraction is Yahoo!.
			Their term extraction service is freely available for research and personal purposes, limited to 5,000 queries per day per IP address.24 However, this service is limited to short English texts and is probably based on term dictionaries and gazetteers.
			The Fips parser, developed at the University of Geneva, has been used for collocation extraction in several languages (Seretan and Wehrli 2009).
			Even though it is academic research, the collocation extraction tool FipsCo, based on Fips, is not freely available.
			The tool is able to extract collocations from monolingual corpora in English, French, Spanish and Italian, and there is a version for Greek (Michou and Seretan 2009).
			The tool has been used in MT experiments, suggesting that it is able to extract bilingual collocations from word-aligned parallel corpora.
			Although the system itself is not free, its visualisation tool, FipsCoView,25 is freely available as a web interface (Seretan and Wehrli 2011).
			Translation memory software may use MWEs as basic segments to retrieve.
			Indeed, MWEs are somehow in-between sentences and words.
			On the one hand, the retrieval of simple words in a hypothetical translation memory would be of little use.
			The number of possible translations for a word out of its context is potentially large and additional information is required to choose among the options.
			Therefore, it would lack precision.
			On the other hand, the retrieval of whole sentences would be highly precise, but an extremely large translation memory would be required in order to obtain reasonable recall.
			If the memory of previously translated segments is small, only from time to time (and with some luck) a sentence will be retrieved.
			Many sentences containing part of the translation would be useful, but will be ignored by a sentence-based exact match system.One example of system performing bilingual MWE extraction is Similis,26 previ ously commercialised by Lingua et Machina and now freely available.
			According to the official website, “Similis [...]
			includes a linguistic analysis engine, uses chunk technology to break down segments into intelligent terminological groups (chunks), and automatically generates specific glossaries.” The technique implemented in the system is an evolution of the one described in Planas and Furuse (2000).
			In this article, the authors describe a clever technique for retrieving similar segments in the source language and their correspondences in the target language.
			Their approach applies a dynamic programming algorithm on a multi-layered structure where sentences are represented as a sequence of surface forms, lemmas and parts 22 http://www.temis.com/ 23 http://www.temis.com/index.php?id=201&selt=1 24 http://developer.yahoo.com/search/content/V1/termExtraction.html 25 http://129.194.38.128:81/FipsCoView 26 http://similis.org/ of speech.
			The combination of the matchings in these three layers allows for a good balance between precision and recall for the retrieval of bilingual segments.
			3.3 Other Tasks Related to MWE Processing.
			Given that MWE acquisition is our main concern, the whole Sect.
			3.2 is dedicated to a detailed review of the state of the art.
			Here, we overview the state of the art in the other tasks involved in MWE treatment, according to the classification of MWE tasks, namely interpretation (Sect.
			3.3.1), disambiguation (Sect.
			3.3.2), representation (Sect.
			3.3.3) and application (Sect.
			3.3.4).
			3.3.1 Interpretation The interpretation and disambiguation of several types of MWEs are the focus of a large body of literature, specially in the computational semantics community.
			Both can be modelled as classification tasks, so that machine learning algorithms are often employed.
			Therefore, it is possible to distinguish supervised from unsupervised approaches.
			In the former, a large effort is usually dedicated to the annotation of a data set that is subsequently used to build classifiers.
			In the latter, the class attribution is made based on thresholds or rules directly applied to data features.
			Like for most solutions based on machine learning, supervised methods outperform unsupervised methods.
			However, unsupervised methods may sometimes perform as well as supervised methods when they are applied on very large corpora like, for instance, web-based corpora (Keller and Lapata 2003).
			MWE interpretation can be applied on expressions whose meaning does not change too much according to their occurrence contexts, like compound nouns and some specific types of phrasal verbs and support verb constructions.
			However, it is not suitable to interpret ambiguous expressions such as phrasal verbs (look up a word vs look up to the sky) and idioms (my grandfather kicked the bucket vs the cleaning lady accidentally kicked the bucket).
			These are explored in MWE disambiguation tasks (see Sect.
			3.3.2).
			Noun compounds (traffic light, nuclear transcription factor), on the other hand, are rarely ambiguous and their interpretation has been an active research area.
			We distinguish two types of noun compound interpretation: syntactic and semantic.
			The syntactic interpretation has been explored by Nicholson and Baldwin (2006), who distinguish three syntactic relations in noun–noun compounds: subject (product replacement), direct object (stress avoidance) and prepositional object (side show ! show on the side).
			For compounds in which the second noun is a nominalisation,27 they used the inflections of the corresponding verb to generate paraphrases that were looked up in Google.
			The paraphrases and additional features were fed into a nearest-neighbour classifier, but the results failed to improve over the state of the art.
			Three-word or longer noun compounds like liver cell line and liver cell antibody require syntactic interpretation of the constituent hierarchy.
			That is, one needs to distinguish left bracketing like in (liver cell) antibody from right bracketing like in liver (cell line).
			Therefore, Nakov and Hearst (2005) compare two models, based on adjacency and on dependency.
			They use a set of heuristics to generate surface- level paraphrases and then use search engine counts to estimate model probabilities.
			They obtain sizeable improvements over state of the art on a set of biomedical compounds.
			One of the most challenging interpretation problems is the semantic interpretation of the relations involved in noun compounds.
			The goal is to assign to each noun compound one (or several) tags that describe the semantic relation between the two nouns.
			Nakov and Hearst (2008) try to solve this task using a methodology similar to the one they employed for syntactic interpretation.
			First, they generate a large number of paraphrases involving verbs related to the semantic classes (e.g., causes, implies, generates for relation CAUSE) and the relative that.
			Then, they retrieve web counts for the paraphrases and assign the classes with maximal probability according to the corresponding paraphrases.
			Their method is completely unsupervised.
			The resource developed in their work, containing noun compounds and corresponding features, is freely available on the MWE community website (Nakov 2008b).
			Kim and Nakov (2011) revisited the problem, this time using a combination of data bootstrapping and web counts.
			The main difference is that they generated paraphrases not based on surface forms but on parse trees, thus obtaining more accurate results.
			A related method is proposed and evaluated by Kim and Baldwin (2013).
			Paraphrases can be used not only as means but also as ends.
			That is, they may be the actual representation of semantic classes instead of a set of (somehow arbitrary) abstract tags.
			The representation of semantic classes for noun–noun relations is discussed in depth by Girju et al.
			(2005), who compare Lauer’s eight prepositional tags with a proposed classification using 35 abstract tags.
			Moreover, they annotate a corpus sample using both schemes and investigate the correspondences between them.
			In addition, paraphrases can be used, for instance, in order to artificially generate new data for training MT systems (Nakov 2008a).
			Lapata (2002) focuses on the interpretation of noun compounds involving nominalisations.
			She reformulates noun compound interpretation as a disambiguation task, recreating missing evidence from corpus.
			She extracts the counts of the nouns and of the related verb from the BNC, and then uses them as features in a supervised machine learning tool that automatically learns association rules.
			She 27 A noun derived from a verb, like replacement is a nominalisation of the verb replace.
			also discusses and evaluates several smoothing techniques that help obtaining more realistic counts.
			Keller and Lapata (2003) used this task as one of their case studies in order to investigate the use of web counts in NLP disambiguation tasks.
			Latent semantic analysis has also been employed for the semantic classification of noun–noun compounds (Baldwin et al. 2003).
			In order to distinguish compositional from idiomatic constructions, the authors compare the context vectors of the compound with the context vectors of the individual nouns composing it.
			This approach can be generalised and has also been applied and evaluated on other types of MWEs.
			A similar technique is proposed by Séaghdha and Copestake (2013).
			However, they use string kernels to build more or less lexicalised semantic representations for the words in the compound.
			Then, they use standard composition techniques in order to infer the combined semantics of the compound.
			A comprehensive and detailed revision of the semantic interpretation of noun compounds can be found in Nakov’s Ph.D. thesis (Nakov 2007) and in his later survey article (Nakov 2013).
			Several techniques used in this task are described in the proceedings of SemEval 2010 and 2013, which feature shared tasks on this topic (Hendrickx et al. 2010; Butnariu et al. 2010).
			The Special Issue on noun compounds of the Natural Language Engineering journal (Szpakowicz et al. 2013) presents some advances in this area and includes an article that describes in detail the best Semeval system for noun compound interpretation (Nulty and Costello 2010, 2013).
			Besides noun compounds, other MWE types also require interpretation.
			English phrasal verbs are ambiguous and can be used both idiomatically (look up a word) and literally (look up to the sky).
			However, if we consider only the most usual sense, it is possible to perform type-based interpretation.
			Cook and Stevenson (2006) use support vector machines to classify the meaning of the particle up in English phrasal verbs.
			According to the verb, it can have a sense of vertical, completion, goal or reflexive.
			These are simplified using a 2-way and a 3-way classification.
			The features used are standard syntactic slots of the verb, particle characteristics such as distance from the verb, and word co-occurrences.
			Considering a larger range of constructions, Bannard (2005) investigates the extent to which the components of a phrasal verb contribute their meanings to the interpretation of the whole.
			He models compositionality through an entailment task, for instance, split up H) split?
			In a comparison between pmi, t-score and a newly proposed measure based on context cosine similarity, the latter correlates better with human judgements.
			A similar work is that of McCarthy et al.
			(2003).
			They propose several measures involving an automatically acquired distributional thesaurus in order to estimate the idiomaticity of phrasal verbs.
			Their annotated data set uses a numeric scale from 0 (totally opaque) to 10 (fully compositional).
			They show that the best association measure, mutual information, is less correlated to human judgements than a proposed measure which calculates the number of neighbours with the same particle as the phrasal verb minus the equivalent number of simple neighbours.
			A similar annotation scale has been proposed by Roller et al.
			(2013), who provide a data set with compositionality assessments for German expressions.
			They show that it is important to perform outlier identification and removal to obtain reliable data.
			In general, interpretation results based on human annotation of compositionality must always be carefully analysed, since this is a hard task even for humans.
			Venkatapathy and Joshi (2006) explore the type compositionality of verb–noun pairs.
			They describe the creation of an annotated data set with compositionality judgements ranging from 1 to 6.
			Then, they present seven distinct features to estimate compositionality which are further combined using a support vector machine.
			They evaluate the features separately and show that the Spearman correlation between the classifier results and human judgements is around 0.448, which is better than all individual features.
			Using a variation of the same data, McCarthy et al.
			(2007) investigate the use of selectional preferences in this task.
			They propose three different algorithms to obtain this information from parsed corpora: two based on Wordnet and one based on an automatically constructed thesaurus.
			They show that the best performance is obtained by combining selectional preferences and a subset of Venkatapathy and Joshi’s features through a support vector machine.
			3.3.2 Disambiguation Recall that the disambiguation of MWEs is analogous to their interpretation, except that they are considered together with the context in which they appear (sentences).
			Nicholson and Baldwin (2008) present a data set for noun–noun compound disambiguation where a large set of sentences has been manually annotated with syntactic and semantic information about the compounds contained in it.
			Girju et al.
			(2005) investigate methods for their disambiguation.
			They perform a separate analysis of two- and three-noun compounds, annotating their semantics according to two tagging schemes in a training set of around 3K sentences.
			In addition to a detailed analysis of the coverage and correspondences between the tagging schemes, they apply several supervised learning techniques.
			Like for the syntactic disambiguation of three-word compounds, they also employ classifiers.
			They achieves an accuracy of 83.10 % by using as features (a) the top three WordNet synsets for each noun, (b) derivationally related forms and (c) a flag telling whether the noun is a nominalisation.
			Whereas, for MWE interpretation, the majority of publications concerns noun compounds, when it comes to disambiguation a large number of MWE types has been studied.
			However, English still predominates.
			One of the rare works on a language different from English concerns the interpretation of German preposition– noun–verb triples (Fritzinger et al. 2010).
			Constructions like in Gang kommen have both a literal interpretation as to reach the hallway (in den Gang kommen), but also idiomatic interpretations as to be set in motion (in Gang kommen) and to get organised (in die Gänge kommen).
			They manually analysed a large set of such constructions retrieved by a parser, classifying them as either literal, compositional or unknown.28 Then, they investigated the correlation between these classes and morphosyntactic characteristics such as determiners, plural and passivisation.
			Light/support verbs in Japanese have also been studied in the past.
			They include sequences like donariageru (shout) and odosiageru (threaten), that is, formed by two lexical units where the verb is usually highly polysemous like ageru (raise).
			Uchiyama et al.
			(2005) propose two disambiguation methods: a statistical approach using a sense inventory, context and a support vector machine; and a rule-based method where the rules were manually defined based on syntax and on the semantics of the first verb.
			The rule-based method (94.6 %) outperforms the statistical method (82.6 %) in terms of accuracy, but the latter obtains a surprisingly high performance given its simplicity.
			The interpretation of expressions of the type verb–noun has also been explored in English.
			Cook et al.
			(2007) explore the idiomaticity of verb–noun pairs, where the noun is the direct object of the verb and may have an idiomatic (make a face) or literal (make a cake) interpretation.
			Their basic hypothesis is that idiomatic uses are syntactically more rigid.
			Thus, they describe a fully unsupervised approach which considers syntactic and context information in order to calculate the similarity with the canonical form of the idiom.
			In their evaluation, they report results comparable to a supervised approach.
			The data set used in their experiments is freely available (Cook et al. 2008).
			Fazly and Stevenson (2007) propose a more fine-grained classification for light verb–noun constructions.
			They use a supervised learning strategy based on decision trees in order to perform a 4-way semantic disambiguation.
			In their scheme, a light verb may be used with its literal meaning (make a cake), with its abstract meaning (make a living), in light-verb constructions (make a decision) or idiomatically (make a face).
			These classes are a mix of syntactic and semantic characteristics and could arguably be improved using more systematic criteria.
			Even though they perform type-based annotation of their data sets, this work can be considered as disambiguation because the noun is the context used to disambiguate the semantics of a closed set of polysemous light verbs.
			Considering a random baseline with 25 % accuracy, they obtain an overall accuracy of 58.3 %.
			F-measure varies according to the classe: abstract constructions are harder to classify (46 %) than light verb constructions (68 %).
			3.3.3 Representation The lexical representation of MWEs was one of the main goals of the MWE project at Stanford, and has for a long time haunted lexicographers in the compilation of lexical resources.
			Most NLP applications contain at least a small amount of 28 The context unit used for annotation was the sentence.
			However, due to anaphora, sometimes it.
			was impossible to know the intended meaning without looking at neighbour sentences.
			MWE entries, specially closed-class expressions.
			The Stanford parser, for instance, contains a list of several dozens of 2-word and 3-word conjunctions.
			However, when it comes to open-class expressions, this coverage is too limited and ways to efficiently represent MWEs in computational lexicons are required.
			Sag et al.
			(2002) proposed two approaches: words-with-spaces and compositional.
			However, between these extremes of the compositionality spectrum, there are some other possibilities, sometimes explored in related work.
			Laporte and Voyatzi (2008), for instance, describe a dictionary containing 6,800 French adverbial expressions like de nos jours (nowadays).
			A set of 15 flat sequences of parts of speech is used to describe the morphosyntactic pattern of each entry using the lexicon–grammar format.
			Gralin´ ski et al.
			(2010) present a qualitative and quantitative comparison between two structured representations for Polish MWEs: Multiflex and POLENG.
			While the former is designed to be generic and language independent, the latter has a more implicit structure aimed at specific applications.
			The authors focus on nominal compounds and analyse the power of each formalism to incorporate morphological inflection rules such as case, gender and number agreement.
			They also measure the time taken by one expert and two novice lexicographers to encode new MWEs.
			Multiflex does not allow the description of non-contiguous units nor units containing slots and it takes much longer for lexicographers to learn and use it.
			POLENG offers a complementary approach, allowing a faster description of MWEs including non-contiguous ones.
			A more corpus-based representation has been proposed for the representation of entries in the Dutch electronic lexicon of MWEs (Grégoire 2007, 2010).
			She uses an equivalence class method that groups similar expressions according to their syntactic characteristics.
			In addition to numbers of occurrences and examples, each entry contains a link to a pattern that describes the syntactic behaviour of the expression.
			This description is quite practical, as the lexicon is aimed for NLP systems such as the Alpino parser and Rosetta MT system.
			Izumi et al.
			(2010) suggest a rule-based method to normalise Japanese functional expressions in order to optimise their representation.
			They consider two separate problems: the insertion of omitted parts and the removal of satellite parts that do not contribute much to the meaning of the sentence.
			In a comparison with manually generated paraphrases, they obtain a precision of 77 %.
			If such normalised representation are adopted in the lexicon, the same paraphrasing rules can be applied to running text in order to align it with expressions contained in the lexicon.
			The use of tree-rewriting grammars for describing MWEs is proposed by Schuler and Joshi (2011).
			They provide arguments and formal proof that this formalism is adequate to represent non-fixed expressions such as raise X to the Yth power.
			The generalisation of their approach to other types of expressions, however, remains to be demonstrated.
			Finally, concerning the hierarchical structure among MWEs, SanJuan et al.
			(2005) explore three strategies (lexical inclusion, Wordnet similarity and clustering) to organise a set of multiword terms manually extracted from the Genia corpus.
			This kind of representation can be very useful to include extracted expressions in more sophisticated concept nets and ontologies.
			When it comes to bilingual and multilingual dictionaries, the problem becomes more complex since it is necessary to represent not only the internal structure of the entries but also cross-language links at global and local levels.
			To the best of our knowledge, there is little research concerning this problem.
			Section 3.3.4.4 contains a discussion on the representation of MWEs in MT systems.
			In short, due to the modest amount of research in this area and to the complexity of the problem, a model for the efficient lexical representation of MWEs in general remains an open problem.
			3.3.4 Applications A list of potential NLP applications where MWEs are relevant was introduced in Sect.
			1.1.2.
			Here, we provide a summary of these target applications for which concrete results have been obtained.
			Many results presented here concern pilot studies and techniques as simple as joining contiguous MWE components with an underscore character as a preprocessing step.
			From all the MWE tasks discussed in this section, application is by far the one with the least amount of published results.
			3.3.4.1 Syntactic Analysis A small set of fixed MWEs like conjunctions are represented in most existing parsers, chunkers and POS taggers.
			However, the further insertion of additional multiword entries can improve the coverage of the analysis, as more complex MWEs like noun compounds and verbal expressions are valuable information for syntactic disambiguation.
			Concerning POS tagging, Constant and Sigogne (2011) present an evaluation on French.
			They assign special tags to words corresponding to the beginning and to the ending of multiword units.
			Using a model based on conditional random fields, they learn the MWE-aware tagger from a corpus in which the training data was automatically annotated with entries coming from several lexica containing compounds and proper nouns.
			This technique obtains 97.7 % accuracy, improving considerably over standard taggers like TreeTagger and TnT.
			They also compare this strategy with the integration of the same lexicons into a parser trained using a probabilistic context-free grammar with latent annotations (PCFG-LA).
			Their results show that MWE recognition and parsing quality improve with both approaches (Constant et al. 2013).
			Korkontzelos and Manandhar (2010) obtain impressing improvements by enriching a baseline shallow parser with MWEs.
			Their method simply consists of joining contiguous nominal expressions with an underscore prior to parsing.
			This makes the system treat them as unknown tokens and assign them a parse based on the context.
			They analyse a set of 118 2-word MWEs from WordNet, classifying them by POS sequences and by compositionality.
			They conclude that, in all cases, the accuracy of the parses was improved, specially for non-compositional adjective–noun pairs, for which the substantial improvements ranged from 15.32 to 19.67 %.
			As for deep parsing, Zhang and Kordoni (2006) extended the lexicon of an English HPSG parser with 373 MWE entries represented as words-with-spaces.
			They obtained a significant increase in the coverage of the grammar, from 4.3 to 18.7 %.
			Using a compositional representation, Villavicencio et al.
			(2007) added 21 new MWEs to the same parser, obtaining an increase in the grammar coverage from 7.1 to 22.7 %, without degrading accuracy.
			However, MWEs do not always improve the performance of the parser, as shown by Hogan et al.
			(2011).
			They try to include a set of named entities in their parsing system, replacing them by placeholders.
			However, they did not obtain significant improvements over the baseline, even when tuning count thresholds.
			The use of lexical association measures to model collocational behaviour in dependency parsing has been investigated by Mirroshandel et al.
			(2012).
			They enrich the dependency relations in the parsing model, learnt from the treebank, with pairs collected from a very large parsed corpus.
			Their results show significant improvements in parsing accuracy for specific syntactic relations.
			As far as we know, the English and Italian parser Fips is one of the few systems dealing with variable MWEs (Wehrli et al. 2010).
			Its approach is more sophisticated than words-with-spaces, as it dynamically identifies expressions at the same time as it constructs the parse tree.
			This technique performs better than post-processing the trees after they are produced.
			The authors demonstrate that MWEs are not a “pain in the neck” but actually a valuable information to reduce syntactic ambiguity.
			3.3.4.2 Word Sense Disambiguation Given an occurrence of a polysemous word, word sense disambiguation consists of picking up a single sense among those listed in an inventory.
			For example, the verb fire can mean make somebody lose his/her job, pull the trigger of a gun, or make something burn.
			The sentence in which the verb occurs will determine which of these senses is intended.
			Although context information is used, MWEs are generally ignored in WSD tasks.
			As a consequence, not only the correct sense will be ignored but also wrong senses will be inferred for the individual words.
			For example, in Wordnet, none of the senses of voice and of mail indicates that voice mail means system that records messages on a telephone when nobody answers.
			As exemplified by Finlayson and Kulkarni (2011), while the word voice has 11 senses and the word mail has 5, the expression voice mail only has 1.
			They show that, in Wordnet 1.6, the average polysemy of MWEs is of 1.07 synsets, versus 1.53 for simple words.
			To the best of our knowledge, their work is the first to report a considerable improvement on word sense disambiguation performance due to the detection of MWEs.
			Despite its simplicity, their method reaches an improvement of 5 F-measure points given lower and upper bounds of 3.3 and 6.1.
			Bungum et al.
			(2013) perform collocation identification prior to translation in order to improve the quality of cross-lingual WSD.
			They argue that, while the coverage of the method depends on the quality of the collocation dictionaries, it helps improving precision.
			Thus, the use of even simple MWE resources like monolingual lists can help in a complex task like cross-lingual WSD.
			3.3.4.3 Information Retrieval (IR) Let us consider a simplified IR system, modelling documents as bags of words and not keeping track of co-occurrences.
			For instance, if a document contains the term rock star, it will probably be retrieved as an answer to queries on geology (rock) and astronomy (star).
			If this MWE was represented as a unit in the index of the system, the precision of the retrieved documents could increase.
			Most current IR systems allow more sophisticated queries to be expressed through quotes and wildcards.
			However, representing only relevant MWEs instead of all possible n-grams in the documents could speed up the searches.
			Joining the words of MWEs before indexation is a simple idea that was put in practice by Acosta et al.
			(2011).
			They tested the impact of a large set of automatically and manually acquired MWE dictionaries on standard IR test sets from the CLEF campaign.
			Their results show that there is a gain in mean average precision when MWEs are tokenised as single words.
			Choosing the appropriate granularity for units to be indexed can be complicated in languages like Chinese, which do not use spaces to separate words.
			In this case, a prior phase of segmentation generally takes place before traditional IR indexation.
			Xu et al.
			(2010) propose a new measure for the tightness of 4-character sequences, as well as three procedures for word segmentation based on this measure.
			Then, they compare a standard segmentation tool with their methods in an IR system.
			They show that two of their segmentation strategies improve mean average precision on a test set.
			A related task is topic modelling, a popular approach to joint clustering of documents and terms.
			The standard document representation in this task is a bag of words.
			However, as presented by Baldwin (2011), it is possible to consolidate the microlevel document representation with the help of MWEs.
			He argues that recent experimental results demonstrate that linguistically-richer document representations can enhance topic modelling.
			3.3.4.4 Machine Translation In current MT systems, various practical solutions have been implemented.
			The expert MT system ITS-2 handles MWEs at two levels (Wehrli 1998).
			Contiguous compounds are dealt with during lexical analysis and treated as single words in subsequent steps.
			Idiomatic, non-fixed units are treated by the syntax analysis module, requiring a much more sophisticated description.
			Once they are correctly identified, however, their transfer is executed in the same way as regular structures.
			The system implements a more sophisticated approach for non-fixed MWE identification in the syntactic analysis module (Wehrli et al. 2010).
			When evaluated on a data set of English/Italian!French translations, this strategy improved the quality of 10–16 % of a test set of 100 collocations (manual evaluation).
			Haugereid and Bond (2011) enriched the Jaen Japanese–English MT system with MWE rules.
			Jaen is a semantic transfer MT system based on the HPSG parsers JACY and ERG.
			The authors use GIZA++ and Anymalign in order to generate phrase tables from parallel corpora, from which they automatically extract the new transfer rules.
			These rules are then filtered and, when added to the system, improve translation coverage from 19.3 to 20.1% and translation quality from 17.8 to 18.2 % NEVA score – a variant of BLEU).
			Even though the improvements are quite modest, the authors argue that they can be further improved by learning even more rules.
			Morin and Daille (2010) obtain an improvement of 33 % in the French–Japanese translation of MWEs.
			They implement a morphologically-based compositional method for backing-off when there is not enough data in a dictionary to translate a MWE.
			For example, chronic fatigue syndrome can be decomposed as [chronic fatigue] [syndrome], [chronic] [fatigue syndrome] or [chronic] [fatigue] [syndrome].
			Grefenstette (1999) addresses the translation of noun compounds from German and Spanish into English.
			He uses web counts to select translations for com- positional noun compounds, and achieves an impressive accuracy of 0.86–0.87.
			Similarly, Tanaka and Baldwin (2003) compare two shallow translation methods for English–Japanese noun compounds.
			The first one is a static memory-based method where the compound needs to be present in the dictionary in order to be translated correctly.
			The second is a dynamic compositional method in which alternative translations are validated using corpus evidence.
			Their evaluation considers the compounds as test translation units (as opposed to traditional sentence-based evaluation).
			When they combine the two methods, they achieve 95 % coverage and potentially high translation accuracy.
			This method is further refined by the use of a support vector machine model to rank all possible translations (Baldwin and Tanaka 2004).
			The model learns the translation scores based on several features coming from monolingual and bilingual dictionaries and corpora.
			The popular phrase-based models of the Moses SMT toolkit represent MWEs as flat contiguous sequences of words (Koehn et al. 2007).
			Bilingual MWEs are bilingual sequences, called “bi-phrases”, and have several probabilities associated to them.
			Carpuat and Diab (2010) propose two complementary strategies in order to add monolingual MWEs from WordNet into an English–Arabic Moses system.
			The first strategy is a static single-tokenisation that treats MWEs as words-with- spaces.
			The second strategy is dynamic, adding to the translation model a count for the number of MWEs in the source part of the bi-phrase.
			They found that both strategies result in improvement of translation quality, which suggests that Moses bi-phrases alone do not model all MWE information.
			Nakov (2008a) propose another approach for minimizing data sparseness in MT, based on the generation of monolingual paraphrases to augment the training corpus.
			Parse trees are used as the basis for generating paraphrases that are nearly-equivalent semantically (e.g., ban on beef import for beef import ban and vice versa).
			The trees are syntactically transformed by a set of heuristics, looking at noun compounds and related constructions.
			Using Moses’ ancestor, Pharaoh, on an English–Spanish task, this technique generates an improvement equivalent to 33–50 % of that of doubling training data.
			Automatic word alignment can be more challenging when translating from and to morphologically rich languages.
			In German and in Scandinavian languages, for instance, a compound is in fact a single token formed through concatenation of words and special infixes (Hauptbahnhof is the concatenation of Haupt (main), Bahn (railway) and Hof (station)).
			Stymne (2011) develops a fine-grained typology for MT error analysis which includes concatenated definite and compound nouns.
			For definiteness, she makes the source text look more like the target text (or vice versa) during training, thus making the learning less prone to errors by using better word alignments.
			In Stymne (2009), she describes her approach to noun compounds, which she splits into their single word components prior to translation.
			Then, after translation, she applies some post-processing rules like the reordering or merging of the components.
			Pal et al.
			(2010) explore the extension of a Moses English–Bengali system.
			Significant improvements are brought by applying preprocessing steps like single- tokenisation for named entities and compound verbs.
			However, larger improvements (4.59 absolute BLEU points) are observed when using a statistical model for the prior alignment of named entities, allowing for their adequate transliteration.
			The domain adaptation of general-purpose MT systems can also be accomplished with the integration of multiword terms.
			Ren et al.
			(2009) adapt a Chinese–English standard Moses system using three simple techniques: appending the MWE lexicon to the corpus, appending it to the phrase table, and adding a binary feature to the translation model.
			They found significant BLEU improvements over the baseline, especially using the extra feature.
			In translation memory systems such as Similis, the translation unit can be considered as a MWE as it is an intermediary between words and sentences.
			The correspondences of word sequences are automatically learned from the translation memory and expressed in a multi-layer architecture including surface forms, lemmas and parts of speech (Planas and Furuse 2000).
			Hierarchical and tree-based translation systems like Joshua use tree rewriting rules in order to represent the correspondences between source and target structures (Li et al. 2009).
			However, it is difficult to implement special rules for MWEs and to distinguish them from rules that should be applied to ordinary word combinations.
			Promising results in the application of MWE resources such as lexicons and thesauri show that this is a recent and apparently growing topic in the MT community.
			Monti et al.
			(2011) compile a parallel corpus of sentences containing several types of expressions and compare the outputs of rule-based and SMT systems.
			While their discussion provides insightful examples, it does not help quantify the extent to which multiword expressions pose problems to MT systems.
			Moreover, it is not possible to know the exact details of the MT paradigms used in their experiments.
			3.4 Summary 91.
			The proceedings of the 2013 MUMTTT workshop provide some pointers on different approaches for MWE translation (Mitkov et al. 2013).
			Our experiments on the evaluation of automatic MWE translation are presented in Chap.
			7.
			3.4 Summary.
			The underlying hypothesis in MWE acquisition is that words that form an expression will co-occur more often than if they were randomly combined.
			This hypothesis is applied in the design of lexical association measures (AMs) for corpus-based MWE acquisition.
			There are numerous AMs available for MWE acquisition (Evert 2004; Seretan 2008; Pecina 2008).
			For an arbitrary n-gram, we estimate its probability under MLE and scale this estimate by the total number of n-grams in the corpus, obtaining the expected count.
			AMs are generally based on the difference between the expected count and the observed count, like t-score, pointwise mutual information and Dice coefficient.
			More robust and theoretically sound AMs based on contingency tables exist for the special case of 2-grams.
			Examples of such measures are x2 and log-likelihood.
			MWE acquisition comprises identification (in context) and extraction (out of context).
			Monolingual MWE acquisition is generally seen as a two-step process: candidate extraction and candidate filtering.
			In candidate extraction, POS sequences are one of the major approaches, specially for terminology (Justeson and Katz 1995; Daille 2003), but also for noun compounds (Vincze et al. 2011) and verbal expressions (Baldwin 2005).
			When a parser is available, syntactic patterns can be much more precise than POS sequences, specially in the extraction of non- fixed MWEs (Seretan and Wehrli 2009; Seretan 2008).
			Tree substitution grammars (Green et al. 2011) and structural regularities in parsing trees (Martens and Vandeghinste 2010) can also be used in order to learn syntactic MWE models from annotated corpora.
			During candidate filtering, some straightforward procedures are the use of stopword lists and of count thresholds.
			AMs are also widely employed to rank the candidates and keep only those whose association score is above a certain threshold (Evert and Krenn 2005; Pecina 2005).
			Supervised learning methods can be used to build a classifier modelling the optimal weights of several AMs and other features (Ramisch et al. 2008; Pecina 2008).
			As for bilingual acquisition, automatic word alignments can provide lists of MWE candidates by themselves (de Medeiros Caseli et al. 2010).
			Bai et al.
			(2009) present an algorithm capable of mining translations for a given MWE in a parallel aligned corpus.
			The automatic discovery of non-compositional compounds from parallel data has been explored by Melamed (1997).
			The EnglishHindi language pair presents large word order variation, and it has been shown that MWE-based features that model compositionality can help reducing alignment error rate (Venkatapathy and Joshi 2006).
			Zarrieß and Kuhn (2009) used syntactically analysed corpora and GIZA++ alignments to extract verb-object pairs from a German–English parallel corpus.
			Daille et al.
			(2004) performed multiword term extraction from comparable corpora in French and in English, and subsequently used the distances between the context vectors to obtain cross-lingual equivalences.
			The syntactic interpretation of nouns compounds has been explored by Nicholson and Baldwin (2006), who distinguish three syntactic relations in noun– noun compounds: subject, direct object and prepositional object.
			Three- word or longer noun compounds require syntactic interpretation of the constituent hierarchy.
			Nakov and Hearst (2005) compare two models, based on adjacency and on dependency.
			Nakov and Hearst (2008) perform unsupervised semantic interpretation of noun compounds by generating a large number of paraphrases involving verbs related to the semantic classes and then retrieving their web counts.
			Kim and Nakov (2011) used a combination of data bootstrapping and web counts, using paraphrases based on parse trees.
			Cook and Stevenson (2006) use support vector machines to classify the meaning of the particle up in English phrasal verbs.
			Bannard (2005) investigates the extent to which the components of a phrasal verb contribute their meanings to the interpretation of the whole.
			A similar work is that of McCarthy et al.
			(2003), who propose several measures to estimate the idiomaticity of phrasal verbs.
			The disambiguation of MWEs is analogous to their interpretation, except that they are considered together with the context in which they appear.
			Nicholson and Baldwin (2008) present a data set for noun–noun compound disambiguation where a large set of sentences has been manually annotated.
			Girju et al.
			(2005) investigate methods for their disambiguation by applying several supervised learning techniques.
			Fritzinger et al.
			(2010) manually analyse a large set of ambiguous German preposition–noun–verb constructions retrieved by a parser, classifying them as either literal, compositional or unknown.
			Light verbs in Japanese have also been studied by Uchiyama et al.
			(2005), who proposes two disambiguation methods: a statistical approach and a rule-based method.
			Cook et al.
			(2007) explore the idiomaticity of verb–noun pairs, where the noun is the direct object of the verb and may have an idiomatic (make a face) or literal (make a cake) interpretation.
			Fazly and Stevenson (2007) propose a more fine-grained classification for light verb–noun constructions, using a supervised learning strategy in order to perform a 4-way semantic disambiguation.
			Sag et al.
			(2002) proposed two approaches to represent MWEs in lexicons: words-with-spaces and compositional.
			However, between these extremes of the compositionality spectrum, there are some other possibilities, explored in related work.
			Laporte and Voyatzi (2008) describe a dictionary of French adverbial expressions and their corresponding morphosyntactic patterns in the lexicon– grammar format.
			Gralin´ ski et al.
			(2010) present a qualitative and quantitative comparison between two structured representations, Multiflex and POLENG, for Polish MWEs.
			Grégoire (2007, 2010) uses an equivalence class method that groups similar expressions according to their syntactic characteristics.
			Izumi et al.
			(2010) suggest a rule-based method to normalise Japanese functional expressions in order to optimise their representation.
			Schuler and Joshi (2011) propose the use of tree- rewriting grammars to describe MWEs.
			There are some target applications for which concrete results have been obtained.
			Constant and Sigogne (2011) present promising results for French parsing.
			Korkontzelos and Manandhar (2010) obtain impressing improvements by enriching a baseline shallow parser with MWEs.
			Zhang and Kordoni (2006) and Villavicencio et al.
			(2007) obtain a significant coverage increase by extending the lexicon of an English HPSG parser with MWEs.
			Wehrli et al.
			(2010) demonstrate that MWEs are not a “pain in the neck” but actually a valuable information to reduce syntactic ambiguity.
	

