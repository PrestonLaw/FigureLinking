
	
		We introduce a word segmentation approach to languages where word boundaries are not orthographically marked, with application to Phrase-Based Statistical Machine Translation (PBSMT).
		Instead of using manually segmented monolingual domain-specific corpora to train segmenters, we make use of bilingual corpora and statistical word alignment techniques.
		First of all, our approach is adapted for the specific translation task at hand by taking the corresponding source (target) language into account.
		Secondly, this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains.
		We evaluate the performance of our segmentation approach on PBSMT tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions.
	
	
			State-of-the-art Statistical Machine Translation (SMT) requires a certain amount of bilingual corpora as training data in order to achieve competitive results.
			The only assumption of most current statistical models (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005) is that the aligned sentences in such corpora should be segmented into sequences of tokens that are meant to be words.
			Therefore, for languages where word boundaries are not orthographically marked, tools which segment a sentence into words are required.
			However, this segmentation is normally performed as a preprocessing step using various word segmenters.
			Moreover, most of these segmenters are usually trained on a manually segmented domain specific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in a monolingual context.
			Consequently, such segmenters cannot produce consistently good results when used across different domains.
			A substantial amount of research has been carried out to address the problems of word segmentation.
			However, most research focuses on combining various segmenters either in SMT training or decoding (Dyer et al., 2008; Zhang et al., 2008).
			One important yet often neglected fact is that the optimal segmentation of the source (target) language is dependent on the target (source) language itself, its domain and its genre.
			Segmentation considered to be “good” from a monolingual point of view may be unadapted for training alignment models or PBSMT decoding (Ma et al., 2007).
			The resulting segmentation will consequently influence the performance of an SMT system.
			In this paper, we propose a bilingually motivated automatically domain-adapted approach for SMT.
			We utilise a small bilingual corpus with the relevant language segmented into basic writing units (e.g. characters for Chinese or kana for Japanese).
			Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidate “words”.
			We evaluate the reliability of these candidates using simple metrics based on co-occurrence frequencies, similar to those used in associative approaches to word alignment (Melamed, 2000).
			We then modify the segmentation of the respective sentences in the parallel corpus according to these candidate words; these modified sentences are then given back to the word aligner, which produces new alignments.
			We evaluate the validity of our approach by measuring the influence of the segmentation process on Chinese-to-English Machine Translation (MT) tasks in two different domains.The remainder of this paper is organised as fol Proceedings of the 12th Conference of the European Chapter of the ACL, pages 549–557, Athens, Greece, 30 March – 3 April 2009.
			Qc 2009 Association for Computational Linguistics lows.
			In section 2, we study the influence of word segmentation on PBSMT across different domains.
			Section 3 describes the working mechanism of our bilingually motivated word segmentation approach.
			In section 4, we illustrate the adaptation of our decoder to this segmentation scheme.
			The experiments conducted in two different domains are reported in Section 5 and 6.
			We discuss related work in section 7.
			Section 8 concludes and gives avenues for future work.
	
	
			on SMT: A Pilot Investigation The monolingual word segmentation step in traditional SMT systems has a substantial impact on the performance of such systems.
			A considerable amount of recent research has focused on the influence of word segmentation on SMT (Ma et al., 2007; Chang et al., 2008; Zhang et al., 2008); however, most explorations focused on the impact of various segmentation guidelines and the mechanisms of the segmenters themselves.
			A current research interest concerns consistency of performance across different domains.
			From our experiments, we show that monolingual segmenters cannot produce consistently good results when applied to a new domain.
			Our pilot investigation into the influence of word segmentation on SMT involves three off- the-shelf Chinese word segmenters including ICTCLAS (ICT) Olympic version1 , LDC segmenter2 and Stanford segmenter version 200605 113 . Both ICTCLAS and Stanford segmenters utilise machine learning techniques, with Hidden Markov Models for ICT (Zhang et al., 2003) and conditional random fields for the Stanford segmenter (Tseng et al., 2005).
			Both segmentation models were trained on news domain data with named entity recognition functionality.
			The LDC segmenter is dictionary-based with word frequency information to help disambiguation, both of which are collected from data in the news domain.
			We used Chinese character-based and manual segmentations as contrastive segmentations.
			The experiments were carried out on a range of data sizes from news and dialogue domains using a state-of-the-art Phrase-Based SMT (PBSMT) 1 http://ictclas.org/index.html 2 http://www.ldc.upenn.edu/Projects/ Chinese 3 http://nlp.stanford.edu/software/ segmenter.shtml system—Moses (Koehn et al., 2007).
			The performance of PBSMT system is measured with BLEU score (Papineni et al., 2002).We firstly measure the influence of word seg mentation on in-domain data with respect to the three above mentioned segmenters, namely UN data from the NIST 2006 evaluation campaign.
			As can be seen from Table 1, using monolingual segmenters achieves consistently better SMT performance than character-based segmentation (CS) on different data sizes, which means character-based segmentation is not good enough for this domain where the vocabulary tends to be large.
			We can also observe that the ICT and Stanford segmenter consistently outperform the LDC segmenter.
			Even using 3M sentence pairs for training, the differences between them are still statistically significant (p < 0.05) using approximate randomisation (Noreen, 1989) for significance testing.
			40K 160K 640K 3M CS 8.33 12.47 14.40 17.80 ICT LDC Stanford 10.17 9.37 10.45 14.85 13.88 15.26 17.20 15.86 16.94 20.50 19.59 20.64 Table 1: Word segmentation on NIST data sets However, when tested on out-of-domain data, i.e. IWSLT data in the dialogue domain, the results seem to be more difficult to predict.
			We trained the system on different sizes of data and evaluated the system on two test sets: IWSLT 2006 and 2007.
			From Table 2, we can see that on the IWSLT 2006 test sets, LDC achieves consistently good results and the Stanford segmenter is the worst.4 Furthermore, the character-based segmentation also achieves competitive results.
			On IWSLT 2007, all monolingual segmenters outperform character-based segmentation and the LDC segmenter is only slightly better than the other segmenters.
			From the experiments reported above, we can reach the following conclusions.
			First of all, character-based segmentation cannot achieve state-of-the-art results in most experimental SMT settings.
			This also motivates the necessity to work on better segmentation strategies.
			Second, monolingual segmenters cannot achieve consis 4 Interestingly, the developers themselves also note the sensitivity of the Stanford segmenter and incorporate external lexical information to address such problems (Chang et al., 2008).
			with Ci = {ci1 , . . .
			, cim } and ∀k ∈ [1, m − 1], ik+1 − ik = 1, then the alignment ai between eiand the sequence of words Ci is considered a candidate word.
			Some examples of such 1-to-n align ments between Chinese and English we can derive automatically are displayed in Figure 1.5 Table 2: Word segmentation on IWSLT data sets tently good results when used in another domain.
			In the following sections, we propose a bilingually motivated segmentation approach which can be automatically derived from a small representative data set and the experiments show that we can consistently obtain state-of-the-art results in different domains.
	
	
			Segmentation 3.1 Notation.
			While in this paper, we focus on Chinese–English, the method proposed is applicable to other language pairs.
			The notation, however, assumes Chinese–English MT. Given a Chinese sentence 1 consisting of J characters {c1 , . . .
			, cJ } and Figure 1: Example of 1-to-n word alignments between English words and Chinese characters 3.3 Candidate Reliability Estimation.
			Of course, the process described above is error- prone, especially on a small amount of training data.
			If we want to change the input segmentation to give to the word aligner, we need to make sure that we are not making harmful modifications.
			We thus additionally evaluate the reliability of the candidates we extract and filter them before inclusion in our bilingual dictionary.
			To perform this filtering, we use two simple statistical measures.
			In the following, ai = (Ci, ei ) denotes a candidate.
			The first measure we consider is co-occurrence frequency (C OOC (Ci, ei )), i.e. the number of times Ci and ei co-occur in the bilingual corpus.
			This very simple measure is frequently used in as an English sentence eI consisting of I words sociative approaches (Melamed, 2000).
			The sec{e1 , . . .
			, eI }, AC →E will denote a Chinese-to English word alignment between cJ and eI . Since ond measure is the alignment confidence (Ma et al., 2007), defined as 1 1 we are primarily interested in 1-to-n alignments, AC →E can be represented as a set of pairs ai =(Ci , ei) denoting a link between one single En glish word ei and a few Chinese characters Ci.The AC (ai) = C (ai) , C OOC (Ci, ei ) set Ci is empty if the word ei is not aligned to any character in cJ . 3.2 Candidate Extraction.
			In the following, we assume the availability of an automatic word aligner that can output alignments AC →E for any sentence pair (cJ , eI ) in a paral where C (ai) denotes the number of alignments proposed by the word aligner that are identical to ai.
			In other words, AC (ai) measures how often the aligner aligns Ci and ei when they co-occur.
			We also impose that | Ci | ≤ k, where k is a fixedinteger that may depend on the language pair (be tween 3 and 5 in practice).
			The rationale behind 1 1 lel corpus.
			We also assume that AC →E contain 1-to-n alignments.
			Our method for Chinese word segmentation is as follows: whenever a single English word is aligned with several consecutive Chinese characters, they are considered candidates for grouping.
			Formally, given an alignment AC →E this is that it is very rare to get reliable alignments between one word and k consecutive words when k is high.
			5 While in this paper we are primarily concerned with languages where the word boundaries are not orthographically marked, this approach, however, can also be applied to languages marked with word boundaries to construct bilingually between cJ and eI , if ai = (Ci , ei ) ∈ AC →E , motivated “words”.
			The candidates are included in our bilingual dictionary if and only if their measures are above some fixed thresholds tC OOC and tAC , which allow for the control of the size of the dictionary and the quality of its contents.
			Some other measures (including the Dice coefficient) could be considered; however, it has to be noted that we are more interested here in the filtering than in the discovery of alignments per se, since our method builds upon an existing aligner.
			Moreover, we will see that even these simple measures can lead to an improvement in the alignment process in an MT context.
			3.4 Bootstrapped word segmentation.
			Once the candidates are extracted, we perform word segmentation using the bilingual dictionaries constructed using the method described above; this provides us with an updated training corpus, in which some character sequences have been replaced by a single token.
			This update is totally naive: if an entry ai = (Ci , ei ) is present in the dictionary and matches one sentence pair (cJ , eI ) timation of the weights is to distribute the probability mass for each node uniformly to each outgoing edge.
			The single node having no outgoing edges is designated the “end node”.
			An example of word lattices for a Chinese sentence is shown in Figure 2.
			4.2 Word Lattice.
			Generation Previous research on generating word lattices relies on multiple monolingual segmenters (Xu et al., 2005; Dyer et al., 2008).
			One advantage of our approach is that the bilingually motivated segmentation process facilitates word lattice generation without relying on other segmenters.
			As described in section 3.4, the update of the training corpus based on the constructed bilingual dictionary requires that the sentence pair meets the bilingual constraints.
			Such a segmentation process in the training stage facilitates the utilisation of word lattice decoding.
			4.3 Phrase-Based Word Lattice.
			Decoding Given a Chinese input sentence cJ consisting of J 1 1 (i.e. Ci and ei are respectively contained in cJ and 1 ), then we replace the sequence of characters Ci with a single token which becomes a new lexical unit.6 Note that this replacement occurs even if no alignment was found between Ci and ei for the characters, the traditional approach is to determine the best word segmentation and perform decoding afterwards.
			In such a case, we first seek a single best segmentation: fˆK K J pair (cJ , eI ).
			This is motivated by the fact that the 1 = arg max{P r(f1 |c1 )} f K ,K 1 1 1 filtering described above is quite conservative; we trust the entry ai to be correct.
			This process can be applied several times: once we have grouped some characters together, they become the new basic unit to consider, and we can rerun the same method to get additional groupings.
			However, we have not seen in practice much benefit from running it more than twice (few new candidates are extracted after two iterations).
			Then in the decoding stage, we seek: eˆI = arg max{P r(eI |fˆK )} 1 1 1 e I , I In such a scenario, some segmentations which are potentially optimal for the translation may be lost.
			This motivates the need for word lattice decoding.
			The search process can be rewritten as: eˆI = arg max{ max P r(eI , f K |cJ )}
	
	
			1 eI ,I f K ,K 1 1 1 = arg max{ max P r(eI )P r(f K |eI , cJ )} 4.1 Word Lattices.
			1 ,I f K ,K 1 1 1 1 In the decoding stage, the various segmentation = arg max{ max P r(eI K I K J alternatives can be encoded into a compact representation of word lattices.
			A word lattice G = 1 ,I f1 ,K 1 )P r(f1 |e1 )P r(f1 |c1 )} (V, E) is a directed acyclic graph that formally is a weighted finite state automaton.
			In the case of word segmentation, each edge is a candidate word associated with its weights.
			A straightforward es 6 In case of overlap between several groups of words to replace, we select the one with the highest confidence (according to tAC ).
			Given the fact that the number of segmentations f K grows exponentially with respect to the number of characters K , it is impractical to firstly enumerate all possible f K and then to decode.
			However, it is possible to enumerate all the alternative segmentations for a substring of cJ , making the utilisation of word lattices tractable in PBSMT.
			Figure 2: Example of a word lattice
	
	
			5.1 Evaluation.
			The intrinsic quality of word segmentation is normally evaluated against a manually segmented gold-standard corpus using F-score.
			While this approach can give a direct evaluation of the quality of the word segmentation, it is faced with several limitations.
			First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).
			Second, an increase in F-score does not necessarily imply an improvement in translation quality.
			It has been shown that F-score has a very weak correlation with SMT translation quality in terms of BLEU score (Zhang et al., 2008).
			Consequently, we chose to extrinsically evaluate the performance of our approach via the Chinese–English translation task, i.e. we measure the influence of the segmentation process on the final translation output.
			The quality of the translation output is mainly evaluated using BLEU, with NIST (Doddington, 2002) and METEO R (Banerjee and Lavie, 2005) as complementary metrics.
			5.2 Data.
			The data we used in our experiments are from two different domains, namely news and travel dialogues.
			For the news domain, we trained our system using a portion of UN data for NIST2006 evaluation campaign.
			The system was de veloped on LDC Multiple-Translation Chinese (MTC) Corpus and tested on MTC part 2, which was also used as a test set for NIST 2002 evaluation campaign.
			For the dialogue data, we used the Chinese– English datasets provided within the IWSLT 2007 evaluation campaign.
			Specifically, we used the standard training data, to which we added devset1 and devset2.
			Devset4 was used to tune the parameters and the performance of the system was tested on both IWSLT 2006 and 2007 test sets.
			We used both test sets because they are quite different in terms of sentence length and vocabulary size.
			To test the scalability of our approach, we used HIT corpus provided within IWSLT 2008 evaluation campaign.
			The various statistics for the corpora are shown in Table 3.
			5.3 Baseline System.
			We conducted experiments using different segmenters with a standard log-linear PBSMT model: GIZA ++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error- rate training (Och, 2003), a 5-gram language model with KneserNey smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007; Dyer et al., 2008) to translate both single best segmentation and word lattices.
	
	
			6.1 Results.
			The initial word alignments are obtained using the baseline configuration described above by segmenting the Chinese sentences into characters.
			From these we build a bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into a single word, using the method presented in section 3.4.
			As previously mentioned, this process can be repeated several times.
			We then extract aligned phrases using the same procedure as for the baseline system; the only difference is the basic unit we are considering.
			Once the phrases are extracted, we perform the estimation of weights for the features of the log-linear model.
			We then use a simple dictionary-based maximum matching algorithm to obtain a single-best segmentation for the Chinese sentences in the development set so that T r a i n D e v . E v a l . Z h E n Z h E n Z h E n Di alo gue Se nte nce s 4 0 , 9 5 8 489 (7 ref.)
			489 (6 ref.)/ 489 (7 ref.)
			Ru nni ng wo rds Vo cab ula ry siz e 48 8, 30 3 2 , 7 4 2 38 5,0 65 9, 7 1 8 8, 14 1 8 3 5 46 ,9 04 1, 7 8 6 8, 79 3/ 4, 37 7 9 3 6 / 7 7 2 51, 50 0/2 3,1 81 2, 0 1 6/ 1, 3 3 9 Ne ws Se nte nce s 4 0 , 0 0 0 993 (9 ref.)
			8 7 8 ( 4 r e f . ) Ru nni ng wo rds Vo cab ula ry siz e 1, 41 2, 39 5 6 0 5 7 95 6,0 23 20 ,0 68 41, 46 6 1, 98 3 26 7,2 22 10 ,6 65 3 8 , 7 0 0 1 , 9 0 7 1 0 5 , 5 3 0 7 , 3 8 8 Table 3: Corpus statistics for Chinese (Zh) character segmentation and English (En) minimum-error-rate training can be performed.7 Finally, in the decoding stage, we use the same segmentation algorithm to obtain the single-best segmentation on the test set, and word lattices can also be generated using the bilingual dictionary.
			The various parameters of the method (k, tC OOC , tAC , cf.
			section 3) were optimised on the development set.
			One iteration of character grouping on the NIST task was found to be enough; the optimal set of values was found to be k = 3, tAC = 0.0 and tC OOC = 0, meaning that all the entries in the bilingually dictionary are kept.
			On IWSLT data, we found that two iterations of character grouping were needed: the optimal set of values was found to be k = 3, tAC = 0.3, tC OOC = 8 for the first iteration, and tAC = 0.2, tC OOC = 15 for the second.
			As can be seen from Table 4, our bilingually motivated segmenter (BS) achieved statistically significantly better results than character-based segmentation when enhanced with word lattice decoding.8 Compared to the best in-domain segmenter, namely the Stanford segmenter on this particular task, our approach is inferior according to BLEU and NIST.
			We firstly attribute this to the small amount of training data, from which a high quality bilingual dictionary cannot be obtained due to data sparseness problems.
			We also attribute this to the vast amount of named entity terms in the test sets, which is extremely difficult for our approach.9 We expect to see better results when a larger amount of data is used and the segmenter is enhanced with a named entity recogniser.
			On IWSLT data (cf.
			Tables 5 and 6), our 7 In order to save computational time, we used the same set of parameters obtained above to decode both the single- best segmentation and the word lattice.
			8 Note the BL E U scores are particularly low due to the number of references used (4 references), in addition to the small amount of training data available.
			9 As we previously point out, both ICT and Stanford segmenters are equipped with named entity recognition functionality.
			This may risk causing data sparseness problems on small training data.
			However, this is beneficial in the translation process compared to character-based segmentation.
			approach yielded a consistently good performance on both translation tasks compared to the best in- domain segmenter—the LDC segmenter.
			Moreover, the good performance is confirmed by all three evaluation measures.
			B L E U N IS T M E T E O R C S 8.
			4 3 4.
			62 72 0 . 3 7 7 8 St an for d 10 .4 5 5.
			06 75 0 . 3 6 9 9 BS Si ng le Be st BS W or dL att ice 7.
			9 8 9.
			0 4 4.
			43 74 4.
			66 67 0 . 3 5 1 0 0 . 3 8 3 4 Table 4: BS on NIST task B L E U N IS T M E T E O R C S 0.
			19 31 6.
			18 16 0 . 4 9 9 8 L D C 0.
			20 37 6.
			20 89 0 . 4 9 8 4 BS Si ng le Be st BS W or dL att ice 0.
			18 65 0.
			20 41 5.
			78 16 6.
			28 74 0 . 4 6 0 2 0 . 5 1 2 4 Table 5: BS on IWSLT 2006 task B L E U N IS T M E T E O R C S 0.
			29 59 6.
			12 16 0 . 5 2 1 6 L D C 0.
			31 74 6.
			24 64 0 . 5 4 0 3 BS Si ng le Be st BS W or dL att ice 0.
			30 23 0.
			31 71 6.
			04 76 6.
			35 18 0 . 5 1 2 5 0 . 5 6 0 3 Table 6: BS on IWSLT 2007 task 6.2 Parameter Search.
			Graph The reliability estimation process is computationally intensive.
			However, this can be easily parallelised.
			From our experiments, we observed that the translation results are very sensitive to the parameters and this search process is essential to achieve good results.
			Figure 3 is the search graph on the IWSLT data set in the first iteration step.
			From this graph, we can see that filtering of the bilingual dictionary is essential in order to achieve better performance.
			Figure 3: The search graph on development set of IWSLT task 6.3 Vocabulary Size.
			Our bilingually motivated segmentation approach has to overcome another challenge in order to produce competitive results, i.e. data sparseness.
			Given that our segmentation is based on bilingual dictionaries, the segmentation process can significantly increase the size of the vocabulary, which could potentially lead to a data sparseness problem when the size of the training data is small.
			Tables 7 and 8 list the statistics of the Chinese side of the training data, including the total vocabulary (Voc), number of character vocabulary (Char.voc) in Voc, and the running words (Run.words) when different word segmentations were used.
			From Table 7, we can see that our approach suffered from data sparseness on the NIST task, i.e. a large vocabulary was generated, of which a considerable amount of characters still remain as separate words.
			On the IWSLT task, since the dictionary generation process is more conservative, we maintained a reasonable vocabulary size, which contributed to the final good performance.
			V o c. C ha r.v oc R un . W or ds C S 2, 7 4 2 2 , 7 4 2 4 8 8 , 3 0 3 IC T L D C S ta n f o r d 11 ,4 41 9, 2 9 3 18 ,6 76 1 , 6 2 9 1 , 9 6 3 9 8 1 3 5 8 , 5 0 4 3 6 4 , 2 5 3 3 4 8 , 2 5 1 B S 3, 8 2 8 2 , 7 4 0 4 0 2 , 8 4 5 Table 8: Vocabulary size of IWSLT task (40K) proach when it is scaled up to larger amounts of data.
			Given that the optimisation of the bilingual dictionary is computationally intensive, it is impractical to directly extract candidate words and estimate their reliability.
			As an alternative, we can use the obtained bilingual dictionary optimised on the small corpus to perform segmentation on the larger corpus.
			We expect competitive results when the small corpus is a representative sample of the larger corpus and large enough to produce reliable bilingual dictionaries without suffering severely from data sparseness.
			As we can see from Table 9, our segmentation approach achieved consistent results on both IWSLT 2006 and 2007 test sets.
			On the NIST task (cf.
			Table 10), our approach outperforms the basic character-based segmentation; however, it is still inferior compared to the other in-domain monolingual segmenters due to the low quality of the bilingual dictionary induced (cf.
			section 6.1).
			Table 9: Scale-up to 160K on IWSLT data sets Table 7: Vocabulary size of NIST task (40K) 6.4 Scalability.
			The experimental results reported above are based on a small training corpus containing roughly 40,000 sentence pairs.
			We are particularly interested in the performance of our segmentation ap Table 10: Scalability of BS on NIST task 6.5 Using different word aligners.
			The above experiments rely on GIZA ++ to perform word alignment.
			We next show that our approach is not dependent on the word aligner given that we have a conservative reliability estimation procedure.
			Table 11 shows the results obtained on the IWSLT data set using the MTTK alignment tool (Deng and Byrne, 2005; Deng and Byrne, 2006).
			I W SL T0 6 I W SL T0 7 C S 2 1 . 0 4 3 1 . 4 1 IC T L D C S t a n f o r d 2 0 . 4 8 2 0 . 7 9 1 7 . 8 4 3 1 . 1 1 3 0 . 5 1 2 9 . 3 5 BS Si ng le Be st BS W or dL att ice 1 9 . 2 2 2 1 . 7 6 2 9 . 7 5 3 1 . 7 5 Table 11: BS on IWSLT data sets using MTTK
	
	
			(Xu et al., 2004) were the first to question the use of word segmentation in SMT and showed that the segmentation proposed by word alignments can be used in SMT to achieve competitive results compared to using monolingual segmenters.
			Our approach differs from theirs in two aspects.
			Firstly, (Xu et al., 2004) use word aligners to reconstruct a (monolingual) Chinese dictionary and reuse this dictionary to segment Chinese sentences as other monolingual segmenters.
			Our approach features the use of a bilingual dictionary and conducts a different segmentation.
			In addition, we add a process which optimises the bilingual dictionary according to translation quality.
			(Ma et al., 2007) proposed an approach to improve word alignment by optimising the segmentation of both source and target languages.
			However, the reported experiments still rely on some monolingual segmenters and the issue of scalability is not addressed.
			Our research focuses on avoiding the use of monolingual segmenters in order to improve the robustness of segmenters across different domains.
			(Xu et al., 2005) were the first to propose the use of word lattice decoding in PBSMT, in order to address the problems of segmentation.
			(Dyer et al., 2008) extended this approach to hierarchical SMT systems and other language pairs.
			However, both of these methods require some monolingual segmentation in order to generate word lattices.
			Our approach facilitates word lattice gener ation given that our segmentation is driven by the bilingual dictionary.
	
	
			In this paper, we introduced a bilingually motivated word segmentation approach for SMT.
			The assumption behind this motivation is that the language to be segmented can be tokenised into basic writing units.
			Firstly, we extract 1-to-n word alignments using statistical word aligners to construct a bilingual dictionary in which each entry indicates a correspondence between one English word and n Chinese characters.
			This dictionary is then filtered using a few simple association measures and the final bilingual dictionary is deployed for word segmentation.
			To overcome the segmentation problem in the decoding stage, we deployed word lattice decoding.
			We evaluated our approach on translation tasks from two different domains and demonstrate that our approach is (i) not as sensitive as monolingual segmenters, and (ii) that the SMT system using our word segmentation can achieve state-of-the-art performance.
			Moreover, our approach can easily be scaled up to larger data sets and achieves competitive results if the small data used is a representative sample.
			As for future work, firstly we plan to integrate some named entity recognisers into our approach.
			We also plan to try our approach in more domains and on other language pairs (e.g. Japanese– English).
			Finally, we intend to explore the correlation between vocabulary size and the amount of training data needed in order to achieve good results using our approach.
	
	
			This work is supported by Science Foundation Ireland (O5/IN/1732) and the Irish Centre for High- End Computing.10 We would like to thank the reviewers for their insightful comments.
	
