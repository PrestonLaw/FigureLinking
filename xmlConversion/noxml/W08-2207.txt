
	
		This paper presents a new fully automatic method for building highly denseand accurateknowledgebasesfromexisting semanticresources.
		Basically, the method uses a wide-coverage and accurate knowledgebasedWordSenseDisambiguation algorithm to assign the most appropriatesensestolargesets of topically related wordsacquiredfromthe web.
		KnowNet, the resultingknowledge-base which connectslarge sets of semantically-relatedconceptsis a major steptowardsthe autonomous acquisition ofknowledgefrom raw corpora.
		Infact,KnowNetis several times larger than any available knowledge resource encoding relations between synsets, andtheknowledgethatKnowNet contains outperform anyotherresourcewhenempiricallyevaluatedin a commonmultilingual framework.
		71
	
	
			Usinglargescaleknowledgebases,suchasWordNet(Fellbaum,1998),hasbecomea usual, often necessary,practicefor most currentNaturalLanguageProcessing(NLP) systems.
			Even now, building large and rich enough knowledge bases for broad coverage semantic processing takes a greatdeal of expensive manual effortinvolving large research groups during long periods of development.
			In fact, hundreds ofpersonyearshavebeeninvestedin thedevelopment of wordnetsfor variouslanguages(Vossen,1998).
			For example,in morethan tenyears ofmanual construction (from1995to2006,thatisfromversion1.5to3.0),WordNetpassedfrom103,445to 235,402semanticrelations1.Butthisdatadoesnotseemstoberichenoughtosupport advancedconceptbasedNLPapplicationsdirectly.It seemsthat applicationswill not scale upto workingin opendomains without moredetailed and richgeneral-purpose (andalsodomainspeci.c)semanticknowledgebuiltbyautomaticmeans.Obviously, thisfacthas severelyhamperedthe state-of-the-artofadvancedNLP applications.
			However, thePrincetonWordNetisbyfar the most widely-usedknowledgebase (Fellbaum,1998).
			Infact,WordNetisbeing used world-wideforanchoringdifferenttypesofsemanticknowledgeincludingwordnetsforlanguagesotherthanEnglish (Atserias etal.,2004),domainknowledge(Magnini andCavagli,2000) or ontologieslikeSUMO(Niles andPease,2001)ortheEuroWordNetTopConceptOntology (lvez etal.,2008).
			It contains manually codedinformation about nouns, verbs, adjectives andadverbs in English and is organised around the notion of a synset.A synsetisasetofwordswiththesamepart-of-speechthatcanbeinterchangedinacertain context.
			For example, <party, political_party> form a synsetbecause theycan be usedto refertothe same concept.Asynsetis oftenfurtherdescribedby agloss,in thiscase: "anorganisationtogainpoliticalpower"andby explicit semanticrelations to other synsets.
			Fortunately,duringthelastyearstheresearchcommunityhasdevisedalargesetof innovative methods andtoolsforlarge-scale automatic acquisition oflexicalknowledgefrom structured and unstructured corpora.
			Among others we can mention eXtendedWordNet(Mihalcea andMoldovan,2001),large collections of semanticpreferences acquiredfromSemCor(Agirre andMartinez,2001,2002) or acquiredfrom British National Corpus(BNC)(McCarthy,2001), large-scaleTopicSignatures for eachsynset acquiredfromtheweb(AgirreanddelaCalle,2004)orknowledgeabout individualsfromWikipedia(Suchaneketal.,2007).Obviously,allthese semantic resourceshavebeenacquiredusing averydifferentset ofprocesses(Snowetal.,2006), tools and corpora.
			Infact, each semantic resourcehasdifferent volume and accuracy .gures when evaluatedin a common and controlledframework(Cuadros andRigau, 2006).
			However,notallavailablelargescaleresourcesencodesemanticrelationsbetween synsets.Insomecases, onlyrelationsbetweensynsets and wordshavebeen acquired.
			Thisis the case oftheTopicSignatures(Agirre et al.,2000) acquiredfrom the web (AgirreanddelaCalle,2004).Thisis oneofthelargestsemanticresourceseverbuilt with around onehundred million relationsbetween synsets and semantically related 1Symmetric relations are countedonly once.
			words.2 A knowledge net or KnowNet, is an extensible, large and accurate knowledge base, whichhasbeenderivedby semanticallydisambiguating theTopicSignatures acquiredfromthe web.
			Basically,the method uses a robust and accurateknowledgebasedWordSenseDisambiguationalgorithmto assignthemost appropriatesenses to the topic words associated to a particular synset.
			The resulting knowledge-base whichconnectslargesets oftopicallyrelatedconceptsis a majorsteptowardsthe autonomous acquisition ofknowledgefrom rawtext.
			Infact,KnowNetis severaltimes largerthanWordNetandtheknowledgecontainedinKnowNetoutperformsWordNet when empiricallyevaluatedin a commonframework.
			Table1comparesthedifferent volumes of semantic relationsbetween synsetpairs of availableknowledgebases andthe newly createdKnowNets3.
			Table1:Numberofsynset relations Princeton WN3.0 235,402 SelectionalPreferences from SemCor 203,546 eXtendedWN 550,922 Cooccurringrelations from SemCor 932,008 New KnowNet5 231,163 New KnowNet10 689,610 New KnowNet15 1,378,286 New KnowNet20 2,358,927 Varyingfrom.vetotwentythenumberofprocessedwordsfromeachTopicSignature,wecreatedautomaticallyfourdifferentKnowNetswithmillionsofnewsemantic relationsbetweensynsets.
			Afterthisintroduction,Section2describestheTopicSignatures acquiredfromthe web.Section3presentsthe approachweplantofollowforbuildinghighlydense and accurateknowledgebases.
			Section4describesthe methods wefollowedforbuilding KnowNet.InSection5, wepresentthe evaluationframeworkusedinthis study.Section6describestheresultswhenevaluatingdifferentversionsofKnowNetand.nally, Section7presents some concludingremarksandfuturework.
	
	
			TopicSignatures(TS) are word vectors related to aparticular topic(Lin andHovy, 2000).
			TopicSignatures arebuiltby retrieving context words of a target topicfrom large corpora.
			In our case, we consider wordsenses astopics.
			Basically,the acquisition ofTSconsists of:  acquiringthebestpossiblecorpusexamplesforaparticularwordsense(usually characterisingeachwordsenseasaqueryandperformingasearchonthecorpus 2Available athttp://ixa.si.ehu.es/Ixa/resources/sensecorpus 3TheseKnowNetversions canbedownloadedfrom http://adimen.si.ehu.es tammany#n 0.0319 alinement#n 0.0316 federalist#n 0.0315 whig#n 0.0300 missionary#j 0.0229 Democratic#n 0.0218 nazi#j 0.0202 republican#n 0.0189 constitutional#n 0.0186 organization#n 0.0163 forthose examplesthatbestmatchthequeries)  buildingtheTSbyderivingthecontextwordsthatbestrepresentthewordsense fromthe selectedcorpora.
			TheTopicSignatures acquiredfromthe web(hereinafterTSWEB)constitutes one ofthelargestavailablesemanticresourceswitharound100millionrelations(between synsets and words)(Agirre anddelaCalle,2004).
			Inspiredby the work ofLeacock et al.(1998),TSWEB was constructed using monosemous relativesfromWN(synonyms,hypernyms,directandindirecthyponyms,andsiblings),queryingGoogleand retrieving up toonethousand snippetsperquery(thatis, aword sense), extracting the salient words withdistinctivefrequency usingTFIDF.Thus,TSWEB consist of a large orderedlistof words with weights associatedto each ofthe senses ofthepolysemous nouns ofWordNet1.6.The number of constructedtopic signaturesis35,250 withanaveragesizepersignatureof6,877words.WhenevaluatingTSWEB,weused atmaximumthe .rst700wordswhileforbuildingKnowNetweusedatmaximumthe .rst20 words.
			Forexample,Table2presentthe .rstwords(lemmasandpart-of-speech) and weights oftheTopicSignatureacquiredforparty#n#1.
	
	
			Itis ourbelief, that accurate semanticprocessing(such asWSD) would rely not only on sophisticated algorithms but on knowledge intensive approaches.
			In fact, the cyclingarquitecture oftheMEANING4projectdemonstratedthatacquiringbetterknowledge allow toperformbetterWordSenseDisambiguation(WSD) and that havingimprovedWSDsystems we areableto acquirebetterknowledge(Rigau et al., 2002).
			Thus, we plan to acquire by fully automatic means highly connected and dense knowledgebasesfromlargecorporaorthewebbyusingtheknowledgealreadyavailable,increasingthe total number of relationsfromless than one million(the current numberof available relations)to millions.
			4http://www.lsi.upc.edu/~nlp/meaning The currentproposalconsistof:  tofollowCuadroset al.(2005) andCuadrosandRigau(2006)foracquiring highly accurateTopicSignaturesfor all monosemous wordsin WordNet(for instance,usingInfoMap(DorowandWiddows,2003)).
			Thatis,toacquire wordvectorscloselyrelatedtoaparticularmonosemousword(forinstance,airport#n#1)fromBNC or otherlarge text collectionslikeGigaWord,Wikipedia orthe web.
			 to applya veryaccurateknowledgebasedallwordsdisambiguationalgorithm totheTopicSignaturesin orderto obtain sense vectorsinstead of word vectors (forinstance,usingaversionofStructuralSemanticInterconnectionsalgorithm (SSI)(NavigliandVelardi,2005)).
			Forinstance,considerthe .rsttenweighted words(withPart-of-Speech) appearingin theTopicSignature(TS) of the word sense airport#n#1 correspondingto the monosemous word airport, as shown in Table 3.
			This TS has been obtained from BNC usingInfoMap.
			FromthetenwordsappearingintheTS,twoof themdonot appearinWN(correspondingto theproper namesheathrow#n andgatwick#n),four words are monosemous(airport#n,air.eld#n,travelling#nandpassenger#n)andfour otherarepolysemous(.ight#n,train#n,station#n andferry#n).
			Table 3: First ten words with weigths and number of senses in WN of the Topic Signaturefor airport#n#1obtainedfromBNCusingInfoMap airport#n 1.000000 1 heathrow#n 0.843162 0 gatwick#n 0.768215 0 .ight#n 0.765804 9 air.eld#n 0.740861 1 train#n 0.739805 6 travelling#n 0.732794 1 passenger#n 0.722912 1 station#n 0.722364 4 ferry#n 0.717653 2 SSIDijkstra WehaveimplementedaversionoftheStructuralSemanticInterconnectionsalgorithm (SSI), aknowledgebasediterative approachtoWordSenseDisambiguation(Navigli andVelardi,2005).TheSSI algorithmis very simple and consists of aninitialisation stepandasetofiterativesteps.GivenW,anorderedlistofwordstobedisambiguated, theSSIalgorithmperformsasfollows.Duringtheinitialisation step,all monosemous wordsareincludedintothesetI of alreadyinterpreted words, and thepolysemous words areincludedinP(all ofthempendingtobedisambiguated).
			At each step,the Table4:Minimumdistancesfromairport#n#1 Synsets Distance
	
	
			4530 5 64713 4 29767 3 597 2 20 1 1 0 setIis usedtodisambiguate one word ofP, selectingthe word sense whichis closer tothe setIof alreadydisambiguated words.Once a senseisdisambiguated,the word senseisremovedfromP andincludedintoI.Thealgorithm .nisheswhennomore pendingwords remaininP.
			Initially,thelistIofinterpretedwordsshouldincludethesensesofthemonosemous wordsinW,ora.xedsetofwordsenses5.However,inthiscase,whendisambiguating aTSderivedfrom a monosemous word m, thelistIincludes since thebeginning at leastthe sense ofthe monosemouswordm (in ourexample,airport#n#1).
			Inordertomeasuretheproximityofonesynset(ofthewordtobedisambiguatedat each step)to a set of synsets(those word senses alreadyinterpretedinI),the original SSIuses aninhouseknowledgebasederived semi-automatically whichintegrates a variety of online resources(Navigli,2005).This very richknowledgebaseis usedto calculategraphdistancesbetweensynsets.Inordertoavoidtheexponentialexplosion ofpossibilities, notallpathsareconsidered.
			They used acontextfreegrammarof relationstrainedonSemCorto .lteroutinappropriatepathsandtoprovideweightsto the appropriatepaths.
			Instead, we usepartoftheknowledge already availabletobuild a verylarge connectedgraphwith99,635nodes(synsets)and636,077edges(thesetofdirectrelations between synsetsgatheredfromWordNetand eXtendedWordNet).Onthatgraph, we used a very ef.cientgraphlibrary to computetheDijkstra algorithm.6 TheDijkstra algorithmis agreedy algorithmthat computesthe shortestpathdistancebetween one node an the rest of nodes of a graph.
			In that way, we can compute very ef.ciently the shortestdistancebetween anytwogiven nodesofagraph.This versionoftheSSI algorithmis called SSIDijkstra.
			Forinstance,Table4showsthevolumesoftheminimumdistancesfromairport#n#1 to the rest of the synsets of thegraph.
			Interestingly,from airport#n#1 all synsets of the graph are accessible following paths of at maximum six edges.
			While there is only one synset atdistance zero(airport#n#1) andtwenty synsetsdirectly connected to airport#n#1,95%ofthetotalgraphis accessible atdistancefourorless.
			SSIDijkstrahasveryinterestingproperties.Forinstance,SSIDijkstraalwayspro 5Ifno monosemous words arefound orif noinitial senses areprovided, the algorithm could make an initialguessbasedonthe mostprobable sense oftheless ambiguous word ofW.
			6Seehttp://www.boost.org vides an answer when comparingthedistancesbetweenthe synsets of a word and all the synsets alreadyinterpretedin I.Thatis, theDijkstra algorithm always provides an answerbeingthe minimumdistance close orfar7.
			At each step, theSSIDijkstra algorithmselectsthe synset whichis closertoI(the set of alreadyinterpretedwords).
			Table5presentsthe resultofthewordsensedisambiguationprocesswiththeSSIDijkstra algorithm on the TS presented in Table 38.
			Now, part of the TS obtained fromBNC usingInfoMaphavebeendisambiguated at a synsetlevel resulting on a wordsensedisambiguatedTS.ThosewordsnotpresentinWN1.6havebeenignored (heathrow andgatwick).
			Some others,beingmonosemousinWordNet were considered alreadydisambiguated(travelling,passenger, airport and air.eld).
			But the rest, havebeen correctlydisambiguated(.ightwith nine senses,train with six senses, station withfourandferrywithtwo).
			Table5:SensedisambiguatedTSfor airport#n#1obtainedfromBNCusingInfoMap andSSIDijkstra . Word Offset-WN Weight Gloss .ight#n 00195002n 0.017 a scheduled tripbyplane between designated airports travelling#n 00191846n 0 the act ofgoingfrom one place to another train#n 03528724n 0.012 a line of railway cars coupledtogether anddrawn bya locomotive passenger#n 07460409n 0 a person travellingin a vehicle (a boat or bus or car or plane or train etc) who is not operatingit station#n 03404271n 0.019 a building equipped with special equipment and personnelfor a particular purpose airport#n 02175180n 0 an air.eld equipped with controltower andhangers as well as accommodations for passengers and cargo ferry#n 02671945n 0.010 a boat that transports people or vehicles across a bodyof water and operates on a regular schedule air.eld#n 02171984n 0 a place where planes take off andland This sensedisambiguatedTS representssevendirect newsemanticrelationsbetweenairport#n#1andthe .rst wordsoftheTS.It couldbedirectlyintegratedintoa newknowledgebase(forinstance,airport#n#1related> .ight#n#9),butalsoallthe indirectrelationsofthedisambiguatedTS(forinstance,.ight#n#9 related> travelling#n#1).
			In that way,having n disambiguatedword senses, a total of (n 2 - n)/2 relations couldbe created.
			Thatis,fortheteninitial words oftheTS of airport#n#1, twenty-eightnewdirectrelationsbetween synsets couldbe created.
			Thisprocess couldbe repeatedfor all monosemous words ofWordNet appearing inthe selectedcorpus.
			Thetotal number of monosemous wordsinWN1.6is98,953.
			Obviously, not all these monosemous words are expected to appear in the corpus.
			However, we expectto obtainin thatway several millions of new semantic relations between synsets.
			This methodwill allow toderivebyfully automatic means ahuge knowledgebase withmillions of new semanticrelations.
			7In contrast, the originalSSIalgorithm not alwaysprovides apathdistancebecauseitdepends on the grammar.
			8Ittook4.6secondstodisambiguate theTS on a modernpersonal computer.
			Furthermore,this approachis completelylanguageindependent.
			It couldberepeatedfor anylanguagehavingwords connectedtoWordNet.
			It remainsforfurther study and research,how to convert the relations createdin thatwayto morespeci.c andinformedrelations.
			4 Building KnowNet.
			As aproof of concept, wedevelopedKnowNet(KN), alargescaleand extensible knowledgebase obtainedbyapplyingtheSSIDijkstra algorithmto eachtopic signaturefromTSWEB.Thatis,insteadofusingInfoMapandalargecorporaforacquiring newTopicSignaturesforallthemonosemoustermsinWN,weusedthealreadyavailableTSWEB.Wehavegeneratedfourdifferent versions ofKonwNet applyingSSIDijkstratothe .rst5,10,15 and20 wordsforeachTS.SSIDijkstraused only the knowledgepresentinWordNetand eXtendedWordNet which consist of a verylarge connectedgraphwith99,635 nodes(synsets) and636,077edges(semanticrelations).
			WegeneratedeachKnowNetby applyingtheSSIDijkstra algorithmto the whole TSWEB(processingthe .rstwordsof each ofthe35,250topicsignatures).Foreach TS, we obtainedthedirectandindirect relationsfromthetopic(a word sense)to the disambiguatedword senses oftheTS.Then, as explainedinSection3, we alsogeneratedtheindirectrelationsforeachTS.Finally,we removedsymmetricandrepeated relations.
			Table6shows thepercentage ofthe overlappingbetween eachKnowNet with respecttheknowledgecontainedintoWordNetandeXtendedWordNet,thetotalnumber of relations and synsets of each resource.Forinstance,only an8,6% ofthetotal relationsincludedintoWN+XWN are alsopresentinKN20.This meansthatthe rest of relationsfromKN20 are new.Thistablealso showsthedifferentKnowNetvolumes.
			As expected, eachKnowNetis verylarge, rangingfromhundreds ofthousandsto millions of new semantic relationsbetween synsets amongincreasing sets of synsets.
			Surprisingly, the overlapping between the semantic relations of KnowNet and the knowledgebasesusedforbuilding theSSIDijkstragraph(WordNet and eXtended WordNet)is verysmall,possiblyindicatingdisjuncttypes ofknowledge.
			Table6:Size andpercentageofoverlappingrelationsbetweenKnowNetversionsand WN+XWN KB WN+XWN #relations #synsets KN5 3.2% 231,164 39,837 KN10 5.4% 689,610 45,770 KN15 7.0% 1,378,286 48,461 KN20 8.6% 2,358,927 50,705 Table 7 presents the percentage of overlapping relations between KnowNet versions.
			The uppertriangularpart of the matrixpresentsthe overlappingpercentage coveredbylargerKnowNet versions.Thatis, most of theknowledgefromKN5is also containedinlargerversionsofKnowNet.Interestingly,theknowledgecontained into KN10 is only partially covered by KN15 and KN20.
			The lower triangular KnowNet:AProposalforBuildingKnowledgeBasesfromtheWeb part of the matrixpresents the overlappingpercentage coveredby smallerKnowNet versions.
			Table7:PercentageofoverlappingrelationsbetweenKnowNet versions overlapping KN5 KN10 KN15 KN20 KN5 KN10 KN15 KN20 100 31,2 16,4 9,5 93,3 100 44,4 26,0 97,7 88,5 100 56,7 97,2 88,9 97.14 100
	
	
			In orderto empiricallyestablish the relativequality oftheseKnowNet versions with respect already available semantic resources, we usedthe noun-set ofSenseval3EnglishLexicalSampletaskwhich consists of20 nouns.
			Tryingtobeasneutralaspossiblewithrespecttotheresourcesstudied,weapplied systematicallythe samedisambiguation methodto all ofthem.
			Recall that our main goalistoestablishafaircomparisonoftheknowledgeresourcesratherthanproviding thebestdisambiguationtechniquefor aparticularresource.Thus,allthe semanticresources studied are evaluated asTopicSignatures.Thatis, word vectors with weights associatedto aparticular synset(topic) which are obtainedby collecting those word senses appearinginthe synsetsdirectly relatedtothetopics.
			A commonWSD methodhasbeen applied to allknowledge resources.
			A simple word overlapping counting is performed between the Topic Signature and the test example9.The synsethavinghigheroverlappingword countsis selected.Infact,this is a verysimpleWSD method which only considers the topicalinformation around the wordtobedisambiguated.
			Allperformances are evaluated on thetestdata using the .negrainedscoringsystemprovidedbytheorganisers.Finally,weshouldremark that the results are not skewed(forinstance,for resolvingties)by the mostfrequent senseinWN or any other statisticallypredictedknowledge.
			5.1 Baselines.
			Wehavedesigneda number ofbaselinesin order to establish a complete evaluation frameworkfor comparingtheperformance ofeach semantic resource on theEnglish WSDtask.
			RANDOM:For eachtarget word,this method selects a random sense.
			Thisbaseline canbe consideredas alower-bound.
			SEMCORMFS:Thisbaseline selects the mostfrequent sense ofthe target word inSemCor.
			WNMFS:Thisbaselineis obtainedby selectingthe mostfrequentsense(the .rst senseinWN1.6)ofthetargetword.WordNetwordsenseswererankedusingSemCor andothersenseannotatedcorpora.Thus,WNMFSandSemCor-MFSaresimilar,but not equal.
			9We also considerthe multiwordterms.
			TRAIN-MFS:Thisbaseline selectsthe mostfrequentsenseinthetraining corpus ofthetargetword.
			TRAIN:Thisbaseline usesthetraining corpustodirectlybuild aTopicSignature usingTFIDFmeasureforeachwordsense.NotethatinWSDevaluationframeworks, thisisaverybasicbaseline.However,inourevaluationframework,this "WSDbaseline" couldbe consideredas an upper-bound.Wedo not expectto obtainbettertopic signaturesfor aparticularsensethanfromits own annotatedcorpus.
			5.2 Large-scale Knowledge Resources.
			In orderto measurethe relativequalityofthe new resources, weincludeinthe evaluation a wide rangeoflargescaleknowledgeresources connectedtoWordNet.
			WN (Fellbaum,1998):This resource usesthedifferentdirectrelations encodedin WN1.6 andWN2.0.
			WealsotestedWN2 using relationsatdistance1 and2,WN3 using relationsatdistances1to3 andWN4 usingrelations atdistances1to4.
			XWN (Mihalcea andMoldovan,2001):This resource usesthedirectrelations encodedin eXtendedWN.
			WN+XWN:ThisresourceusesthedirectrelationsincludedinWNandXWN.We alsotested(WN+XWN)2(usingeitherWNorXWN relations atdistances1 and2).
			spBNC (McCarthy,2001):This resourcecontains707,618selectionalpreferences acquiredforsubjects and objectsfromBNC.
			spSemCor (Agirre and Martinez, 2002): This resource contains the selectional preferencesacquiredforsubjects andobjectsfromSemCor.
			MCR (Atserias etal.,2004):This resource usesthedirect relations ofWN,XWN and spSemCor(we excludedspBNCbecause ofitspoorperformance).
			TSSEM (Cuadros et al., 2007): These Topic Signatures have been constructed using the part of SemCor having all words tagged by PoS, lemmatized and sense taggedaccordingtoWN1.6totalizing192,639words.Foreachwordsenseappearing inSemCor, wegather all sentencesfor that word sense,building aTS usingTFIDF for allword-senses cooccurringinthose sentences.
	
	
			We evaluatedKnowNetusingtheframeworkofSection5,thatis,the nounpartofthe testsetfromtheSenseval3Englishlexical sampletask.
			Table8presents orderedbyF1 measure,theperformanceinterms ofprecision (P), recall(R)andF1 measure(F1,harmonic mean of recall andprecision) of each knowledgeresourceonSenseval3andits averagesize oftheTSperword-sense.The differentKnowNetversionsappearmarkedinboldandthebaselinesappearinitalics.
			Inthistable,TRAINhasbeencalculatedwithavectorsizeofatmaximum450words.
			Asexpected,RANDOMbaselineobtainsthepoorestresult.Themostfrequentsenses obtainedfromSemCor(SEMCORMFS) andWN(WNMFS) arebothbelowthe mostfrequent sense of the training corpus(TRAIN-MFS).However, all of them are farbelowtotheTopicSignaturesacquiredusingthetraining corpus(TRAIN).
			The best resources would be those obtainingbetter performances with a smaller number of related wordsper synset.
			Thebest results are obtainedbyTSSEM(with F1 of52.4).
			Thelowest resultis obtainedby theknowledgedirectlygatheredfrom WN mainlybecauseofitspoorcoverage(R of18.4 andF1 of26.1).
			Interestingly, theknowledgeintegratedintheMCR althoughpartlyderivedby automaticmeans performs muchbetterin terms ofprecision, recallandF1 measures than using them separately(F1 with18.4pointshigherthanWN,9.1thanXWN and3.7than spSemCor).
			Despiteitssmallsize,theresourcesderivedfromSemCorobtainbetterresultsthan its counterparts usingmuchlarger corpora(TSSEM vs. TSWEB and spSemCor vs. spBNC).
			Regarding the baselines, all knowledge resources surpass RANDOM, but none achieves neither WNMFS, TRAIN-MFS nor TRAIN.
			Only TSSEM obtains better resultsthanSEMCORMFSandisveryclosetothemostfrequentsenseofWN(WNMFS)andthetraining(TRAIN-MFS).
			ThedifferentversionsofKnowNetconsistently obtainbetterperformancesasthey increase the window size of processed words of TSWEB.
			As expected, KnowNet5obtainthelowerresults.
			However,itperformsbetterthanWN(and allitsextensions) and spBNC.
			Interestingly, from KnowNet10, all KnowNet versions surpass theknowledge resources usedfor their construction(WN,XWN,TSWEB and WN+XWN).
			Furthermore, the integration of WN+XWN+KN20 performs better than MCR andsimilarlytoMCR2(havinglessthan50timesitssize).Itisalsointerestingtonote that WN+XWN+KN20 has a better performance than their individual resources, indicatinga complementaryknowledge.Infact,WN+XWN+KN20performsmuch betterthanthe resourcesfromwhichitderives(WN,XWNandTSWEB).
			These initial results seem to be very promising.
			If we do not consider the resourcesderivedfrommanuallysenseannotateddata(spSemCor,MCR,TSSEM,etc.), KnowNet10performsbetter thatanyknowledge resourcederivedby manual or automatic means.Infact,KnowNet15andKnowNet-20 outperformsspSemCor which wasderivedfrom manually annotated corpora.
			Thisis a veryinteresting result since these KnowNet versions have been derived only with the knowledge coming from WN and the web(thatis, TSWEB), andWN andXWN as aknowledge sourcefor SSIDijkstra(eXtendedWordNetonlyhas17,185manuallylabelled senses).
	
	
			The initial results obtained for the different versions of KnowNet seem to be very promising, since they seem tobe of abetterquality than other availableknowledge resources encoding relationsbetween synsetsderivedfrom non-annotated sense corpora.
			We testedall these resources andthedifferent versions ofKnowNet onSemEval2007EnglishLexicalSampleTask(CuadrosandRigau,2008a).
			When comparing the ranking of thedifferentknowledge resources, thedifferent versions ofKnowNet seem tobe more robust and stable across corpora changesthan the rest of resources.
			Furthermore,we alsotestedtheperformanceofKnowNetwhenportedtoSpanish(as theSpanishWordNetis also integratedinto theMCR).StartingfromKnowNet10, allKnowNet versionsperformbetter than any otherknowledge resource onSpanish derivedby manual or automatic means(including theMCR)(Cuadros andRigau, 2008b).
			Table8:P,R andF1 .negrained resultsfortheresourcesevaluated atSenseval3, EnglishLexicalSampleTask KB P R F1 Av.
			Size TRAIN 65.1 65.1 65.1 450 TRAIN-MFS 54.5 54.5 54.5 WNMFS 53.0 53.0 53.0 TSSEM 52.5 52.4 52.4 103 SEMCORMFS 49.0 49.1 49.0 MCR2 45.1 45.1 45.1 26,429 WN+XWN+KN20 44.8 44.8 44.8 671 MCR 45.3 43.7 44.5 129 KnowNet20 44.1 44.1 44.1 610 KnowNet15 43.9 43.9 43.9 339 spSemCor 43.1 38.7 40.8 56 KnowNet10 40.1 40.0 40.0 154 (WN+XWN)2 38.5 38.0 38.3 5,730 WN+XWN 40.0 34.2 36.8 74 TSWEB 36.1 35.9 36.0 1,721 XWN 38.8 32.5 35.4 69 KnowNet5 35.0 35.0 35.0 44 WN3 35.0 34.7 34.8 503 WN4 33.2 33.1 33.2 2,346 WN2 33.1 27.5 30.0 105 spBNC 36.3 25.4 29.9 128 WN 44.9 18.4 26.1 14 RANDOM 19.1 19.1 19.1 In sum, thisis apreliminarystep towardsimprovedKnowNets weplan to obtain exploitingtheTopicSignaturesderivedfrommonosemouswordsasexplainedinSection3.
	
	
			We wanttothankAitorSoroaforhistechnicalsupport andthe anonymousreviewers fortheircomments.This workhasbeen supportedbyKNOW(TIN200615049-C0301)andKYOTO(ICT2007-211423).
	
