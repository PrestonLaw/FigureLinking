
	
		We describe a translation model adaptation approach for conversational spoken language translation (CSLT), which encourages the use of contextually appropriate translation options from relevant training conversations.
		Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations, which is used to bias translation choices towards the current context.
		A significant novelty of our adaptation technique is its incremental nature; we continuously update the topic distribution on the evolving test conversation as new utterances become available.
		Thus, our approach is well-suited to the causal constraint of spoken conversations.
		On an English-to-Iraqi CSLT task, the proposed approach gives significant improvements over a baseline system as measured by BLEU, TER, and NIST.
		Interestingly, the incremental approach outperforms a non-incremental oracle that has upfront knowledge of the whole conversation.
	
	
			Conversational spoken language translation (CSLT) systems facilitate communication between subjects who do not speak the same language.
			Current systems are typically used to achieve a specific task (e.g. vehicle checkpoint search, medical diagnosis, etc.).
			These task-driven Disclaimer: This paper is based upon work supported by the DARPA BOLT program.
			The views expressed here are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.
			Distribution Statement A (Approved for Public Release, Distribution Unlimited) conversations typically revolve around a set of central topics, which may not be evident at the beginning of the interaction.
			As the conversation progresses, however, the gradual accumulation of contextual information can be used to infer the topic(s) of discussion, and to deploy contextually appropriate translation phrase pairs.
			For example, the word ‘drugs’ will predominantly translate into Spanish as ‘medicamentos’ (medicines) in a medical scenario, whereas the translation ‘drogas’ (illegal drugs) will predominate in a law enforcement scenario.
			Most CSLT systems do not take high-level global context into account, and instead translate each utterance in isolation.
			This often results in contextually inappropriate translations, and is particularly problematic in conversational speech, which usually exhibits short, spontaneous, and often ambiguous utterances.
			In this paper, we describe a novel topic-based adaptation technique for phrase-based statistical machine translation (SMT) of spoken conversations.
			We begin by building a monolingual latent Dirichlet allocation (LDA) topic model on the training conversations (each conversation corresponds to a “document” in the LDA paradigm).
			At run-time, this model is used to infer a topic distribution over the evolving test conversation up to and including the current utterance.
			Translation phrase pairs that originate in training conversations whose topic distribution is similar to that of the current conversation are given preference through a single similarity feature, which augments the standard phrase-based SMT log-linear model.
			The topic distribution for the test conversation is updated incrementally for each new utterance as the available history grows.
			With this approach, we demonstrate significant improvements over a baseline phrase-based SMT system as measured by BLEU, TER and NIST scores on an English-to-Iraqi CSLT task.
			697 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 697–701, Sofia, Bulgaria, August 49 2013.
			Qc 2013 Association for Computational Linguistics
	
	
			Domain adaptation to improve SMT performance has attracted considerable attention in recent years (Foster and Kuhn, 2007; Finch and Sumita, 2008; Matsoukas et al., 2009).
			The general theme is to divide the training data into partitions representing different domains, and to prefer translation options for a test sentence from training domains that most resemble the current document context.
			Weaknesses of this approach include (a) assuming the existence of discrete, non-overlapping domains; and (b) the unreliability of models generated by segments with little training data.
			To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or ‘biTAM’ (Zhao and Xing, 2006).
			In contrast to our source language approach, these authors use both source and target information.
			Perhaps most relevant are the approaches of Gong et al.
			(2010) and Eidelman et al.
			(2012), who both describe adaptation techniques where monolingual LDA topic models are used to obtain a topic distribution over the training data, followed by dynamic adaptation of the phrase table based on the inferred topic of the test document.
			While our proposed approach also employs monolingual LDA topic models, it deviates from the above methods in the following important ways.
			First, the existing approaches are geared towards batch-mode text translation, and assume that the full document context of a test sentence is always available.
			This assumption is incompatible with translation of spoken conversations, which are inherently causal.
			Our proposed approach infers topic distributions incrementally as the conversation progresses.
			Thus, it is not only consistent with the causal requirement, but is also capable of tracking topical changes during the course of a conversation.
			Second, we do not directly augment the translation table with the inferred topic distribution.
			Rather, we compute a similarity between the current conversation history and each of the training conversations, and use this measure to dynamically score the relevance of candidate translation phrase pairs during decoding.
	
	
			We use the DARPA TransTac EnglishIraqi parallel two-way spoken dialogue collection to train both translation and LDA topic models.
			This data set contains a variety of scenarios, including medical diagnosis; force protection (e.g. checkpoint, reconnaissance, patrol); aid, maintenance and infrastructure, etc.; each transcribed from spoken bilingual conversations and manually translated.
			The SMT parallel training corpus contains approximately 773K sentence pairs (7.3M English words).
			We used this corpus to extract translation phrase pairs from bidirectional IBM Model 4 word alignment (Och and Ney, 2003) based on the heuristic approach of (Koehn et al., 2003).
			A 4-gram target LM was trained on all Iraqi Arabic transcriptions.
			Our phrase-based decoder is similar to Moses (Koehn et al., 2007) and uses the phrase pairs and target LM to perform beam search stack decoding based on a standard log- linear model, the parameters of which were tuned with MERT (Och, 2003) on a held-out development set (3,534 sentence pairs, 45K words) using BLEU as the tuning metric.
			Finally, we evaluated translation performance on a separate, unseen test set (3,138 sentence pairs, 38K words).
			Of the 773K training sentence pairs, about 100K (corresponding to 1,600 conversations) are marked with conversation boundaries.
			We use the English side of these conversations for training LDA topic models.
			All other sentence pairs are assigned to a “background conversation”, which signals the absence of the topic similarity feature for phrase pairs derived from these instances.
			All of the development and test set data were marked with conversation boundaries.
			The training, development and test sets were partitioned at the conversation level, so that we could model a topic distribution for entire conversations, both during training and during tuning and testing.
	
	
			Our approach is based on the premise that biasing the translation model to favor phrase pairs originating in training conversations that are contextu- ally similar to the current conversation will lead to better translation quality.
			The topic distribution is incrementally updated as the conversation history grows, and we recompute the topic similarity between the current conversation and the training conversations for each new source utterance.
			4.1 Topic modeling with LDA.
			We use latent Dirichlet allocation, or LDA, (Blei et al., 2003) to obtain a topic distribution over conversations.
			For each conversation di in the training collection (1,600 conversations), LDA infers a topic distribution θdi = p(zk |di) for all latent topics zk = {1, ..., K }, where K is the number of topics.
			In this work, we experiment with values of K ∈ {20, 30, 40}.
			The full conversation his tory is available for training the topic models and estimating topic distributions in the training set.
			At run-time, however, we construct the conversation history for the tuning and test sets in- crementally, one utterance at a time, mirroring a real-world scenario where our knowledge is limited to the utterances that have been spoken up to that point in time.
			Thus, each development/test utterance is associated with a different conversation history d∗, for which we infer a topic distribution θd∗ = p(zk |d∗) using the trained LDA model.
			We use Mallet (McCallum, 2002) for training topic models and inferring topic distributions.
			4.2 Topic Similarity Computation.
			For each test utterance, we are able to infer the topic distribution θd∗ based on the accumulated history of the current conversation.
			We use this to compute a measure of similarity between the evolving test conversation and each of the training conversations, for which we already have topic distributions θdi . Because θdi and θd∗ are proba REFEREN CE TRANSCR IPTIONS S YS TE M B LE U↑ T E R ↓ N I S T ↑ Ba se lin e 1 9.
			3 2 5 8.
			6 6 6 . 2 2 i n c r 2 0 1 9.
			3 9 5 8.
			4 4 6 . 2 6 * i n c r 3 0 1 9.
			3 6 58 .3 2* 6 . 2 6 i n c r 4 0 19 .6 8* 58 .1 9* 6 . 2 8 * c o n v 2 0 1 9.
			6 0* 58 .3 6* 6 . 2 7 * c o n v 3 0 1 9.
			4 8 58 .3 8* 6 . 2 7 * c o n v 4 0 1 9.
			5 0 58 .3 3* 6 . 2 8 * ASR TRANSCR IPTIONS S YS TE M B LE U↑ T E R ↓ N I S T ↑ Ba se lin e 1 6.
			9 2 6 2.
			5 7 5 . 7 5 i n c r 2 0 1 6.
			9 9 62 .2 8* 5 . 7 7 i n c r 3 0 1 6.
			9 6 62 .3 3* 5 . 7 8 i n c r 4 0 17 .3 1* 61 .9 7* 5 . 8 3 * c o n v 2 0 1 7.
			2 9* 62 .2 8* 5 . 8 1 * c o n v 3 0 1 7.
			1 2 62 .1 9* 5 . 8 0 * c o n v 4 0 1 7.
			0 0 62 .1 4* 5 . 7 9 * Table 1: Stemmed results on 3,138-utterance test set.
			Asterisked results are significantly better than the baseline (p ≤ 0.05) using 1,000 iterations of paired bootstrap re-sampling (Koehn, 2004).
			(Key: incrN = incremental LDA with N topics; convN = non-incremental, whole-conversation LDA with N topics.)
			X → Y added to the search graph, its topic similarity score as follows:bility distributions, we use the JensenShannon di vergence (JSD) to evaluate their similarity (Manning and Schu¨ tze, 1999).
			The JSD is a smoothed FX → Y = max i∈P ar(X →Y ) sim(θdi , θd∗ ) (1) and symmetric version of KullbackLeibler divergence, which is typically used to compare two probability distributions.
			We define the similar ity score as sim(θd , θd∗ ) = 1 − J SD(θd ||θd∗ ).1Thus, we obtain a vector of similarity scores in dexed by the training conversations.
			4.3 Integration with the Decoder.
			We provide the SMT decoder with the similarity vector for each test utterance.
			Additionally, the SMT phrase table tracks, for each phrase pair, the set of parent training conversations (including the “background conversation”) from which that phrase pair originated.
			Using this information, the decoder evaluates, for each candidate phrase pair 1 JSD(θd ||θd∗ ) ∈ [0, 1] when defined using log ..
			where P ar(X → Y ) is the set of training con versations from which the candidate phrase pair originated.
			Phrase pairs from the “background conversation” only are assigned a similarity score FX →Y = 0.00.
			In this way we distill the inferred topic distributions down to a single feature for each candidate phrase pair.
			We add this feature to the log-linear translation model with its own weight, which is tuned with MERT.
			The intuition behind this feature is that the lower bound of suitability of a candidate phrase pair should be directly proportional to the similarity between its most relevant conversational provenance and the current context.
			Phrase pairs which only occur in the background conversation are not directly penalized, but contribute nothing to the topic similarity score.
			i 2 Figure 1: Rank trajectories of 4 LDA inferred topics, with incremental topic inference.
			The x-axis indicates the utterance number.
			The y-axis indicates a topic’s rank at each utterance.
	
	
			The baseline English-to-Iraqi phrase-based SMT system was built as described in Section 3.
			This system translated each utterance independently, ignoring higher-level conversational context.
			For the topic-adapted system, we compared translation performance with a varying number of LDA topics.
			In intuitive agreement with the approximate number of scenario types known to be covered by our data set, a range of 2040 topics yielded the best results.
			We compared the proposed incremental topic tracking approach to a non-causal oracle approach that had upfront access to the entire source conversations at run-time.
			In all cases, we compared translation performance on both clean-text and automatic speech recognition (ASR) transcriptions of the source utterances.
			ASR transcriptions were generated using a high-performance two-pass HMM-based system, which delivered a word error rate (WER) of 10.6% on the test set utterances.
			Table 1 summarizes test set performance in BLEU (Papineni et al., 2001), NIST (Doddington, 2002) and TER (Snover et al., 2006).
			Given the morphological complexity of Iraqi Arabic, computing string-based metrics on raw output can be misleadingly low and does not always reflect whether the core message was conveyed.
			Since the primary goal of CSLT is information transfer, we present automatic results that are computed after stemming with an Iraqi Arabic stemmer.
			We note that in all settings (incremental and non-causal oracle) our adaptation approach matches or significantly outperforms the baseline across multiple evaluation metrics.
			In particular, the incremental LDA system with 40 topics is the top-scoring system in both clean-text and ASR settings.
			In the ASR setting, which simulates a real- world deployment scenario, this system achieves improvements of 0.39 (BLEU), -0.6 (TER) and 0.08 (NIST).
	
	
			We have presented a novel, incremental topic- based translation model adaptation approach that obeys the causality constraint imposed by spoken conversations.
			This approach yields statistically significant gains in standard MT metric scores.
			We have also demonstrated that incremental adaptation on an evolving conversation performs better than oracle adaptation based on the complete conversation history.
			Although this may seem counter-intuitive, Figure 1 gives clues as to why this happens.
			This figure illustrates the rank trajectory of four LDA topics as the incremental conversation grows.
			The accompanying text shows excerpts from the conversation.
			We indicate (in superscript) the topic identity of most relevant words in an utterance that are associated with that topic.
			At the first utterance, the top-ranked topic is “5”, due to the occurrence of “captain” in the greeting.
			As the conversation evolves, we note that this topic become less prominent.
			The conversation shifts to a discussion on “windows”, raising the prominence of topic “4”.
			Finally, topic “3” becomes prominent due to the presence of the words “project” and “contract”.
			Thus, the incremental approach is able to track the topic trajectories in the conversation, and is able to select more relevant phrase pairs than oracle LDA, which estimates one topic distribution for the entire conversation.
			In this work we have used only the source language utterance in inferring the topic distribution.
			In a two-way CLST system, we also have access to SMT-generated back-translations in the IraqiEnglish direction.
			As a next step, we plan to use SMT-generated English translation of Iraqi utterances to improve topic estimation.
	
