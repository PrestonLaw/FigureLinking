
	
		This paper presents a Chinese word segmentation system for CIPSSIGHAN 2010 Chinese language processing task.
		Firstly, based on Conditional Random Field (CRF) model, with local features and global features, the character-based tagging model is designed.
		Secondly, Hidden Markov Models (HMM) is used to revise the substrings with low marginal probability by CRF.
		Finally, confidence measure is used to regenerate the result and simple rules to deal with the strings within letters and numbers.
		As is well known that character-based approach has outstanding capability of discovering out-of-vocabulary (OOV) word, but external information of word lost.
		HMM makes use of word information to increase in-vocabulary (IV) recall.
		We participate in the simplified Chinese word segmentation both closed and open test on all four corpora, which belong to different domains.
		Our system achieves better performance.
	
	
			Chinese Word Segmentation (CWS) has witnessed a prominent progress in the first four SIGHAN Bakeoffs.
			Since Xue (2003) used character-based tagging, this method has attracted more and more attention.
			Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.
			Because the word-based models can capture the word-level contextual information and IV knowledge.
			Besides, many strategies are proposed to balance the IV and OOV performance (Wang et al., 2008).
			CRF has been widely used in sequence labeling tasks and has a good performance (Laffertyet al., 2001).
			Zhao and Kit (2007b; 2008) at tempt to integrate global information with local information to further improve CRF-based tagging method of CWS, which provides a solid foundation for strengthening CRF learning with unsupervised learning outcomes.
			In order to increase the accuracy of tagging using CRF, we adopt the strategy, which is: if the marginal probability of characters is lower than a threshold, the modified component based on HMM will be trigged; combining the confidence measure the results will be regenerated.
	
	
			In this section, we describe our system in more details.
			Three modules are included in our system: a basic character-based CRF tagger, HMM which revises the substrings with low marginal probability and confidence measure which combines them to regenerate the result.
			In addition, we also use some rules to deal with the strings within letters and numbers.
			2.1 Character-based CRF tagger.
			Tag Set A 6-tag set is adopted in our system.
			It includes six tags: B, B2, B3, M, E and S. Here, Tag B and E stand for the first and the last position in a multi-character word, respectively.
			S stands for a single-character word.
			B2 and B3 stand for the second and the third position in a The work described in this paper is supported by Microsoft Research Asia Funded Project.
			multi-character word.
			M stands for the fourth or more rear position in a multi-character word with more than four characters.
			The 6-tag set is proved to work more effectively than other tag sets in improving the segmentation performance of CRFs by Zhao et al.
			(2006).
			Feature templates In our system, six n-gram templates, namely, C-1, C0, C1, C-1C0, C0C1, that character.
			In the open test, we only add another feature of ‘FRE’, the basic idea of which is if a string matches a word in an existing dictionary, it may be a clue that the string is likely a true word.
			Then more word boundary information can be obtained, which may be helpful for CRF learning on CWS.
			The dictionary we used is ① C-1C1 are selected as features, where C stands for downloaded from the Internet and consists of a character and the subscripts -1, 0 and 1 stand for the previous, current and next character, respectively.
			Furthermore, another one is character type feature template T-1T0T1.
			We use four classes of character sets which are predefined as: class N represents numbers, class L represents non-Chinese letters, class P represents punctuation labels and class C represents Chinese characters.
			Except for the character feature, we also employ global word feature templates.
			The basic idea of using global word information for CWS is to inform the supervised learner how likely it is that the subsequence can be a word candidate.108,750 words with length of one to four char acters.
			We get FRE features similar to the AV features.
			2.2 HMM revises substrings with low mar-.
			ginal probability The MP (short for marginal probability) of each character labeled with one of the six tags can be got separately through the basic CRF tagger.
			Here, B replaces ‘B’ and ‘S’ , and I represents other tags (‘B2’, ‘B3’, ‘M’, ‘E’).
			So each character has corresponding new MP as defined in formula (3) and (4).
			The accessor variety (AV) (Feng et al., 2005) is opted as global word feature, which is integrated into CRF successfully in literatures (Zhao and Kit, 2007b; Zhao and Kit, 2008).
			The AV value P ( PS B ∑ PB ) Pt (3) of a substring s is defined as: ( P P PM P 2 3 PE ) (4) AV (s) min Lav (s), Rav (s) (1) I Where ∑ Pt t S, B, B2 , B3 , M, E and Pt can be Where the left and right AV values Lav (s) calculated by using forward-backward algorithm and more details are in Lafferty et al.
			(2001).
			and Rav (s) are defin ed, respe ctivel y, as the A low confi dent word refer s to a word with number of its distinct predecessors and the number of its distinct successors.
			Multiple feature templates are used to represent word candidates of various lengths identified by the AV criterion.
			Meanwhile, in order to alleviate the sparse data problem, we follow the feature function definition for a word candidate word boundary ambiguity which can be reflected by the MP of the first character of a word.
			That is, it’s a low confident word if the MP of the first character of the word is lower than a threshold (it’s an empirical value and can be obtained by experiments).
			After getting the new MP, all s with a score AV (s) namely: in Zhao and Kit (2008), these low confident candidate words are recom bined with their direct predecessors until the occurrence of a word that the MP of its first fn (s) t , 2 t AV (s) 2t 1 (2) character is above the threshold , and then a In order to improve the efficiency, all candidates longer than five characters are given up.
			The AV features of word candidates can’t directly be utilized to direct CRF learning before being transferred to the information of characters.
			So we only choose the one with the greatest AV score to activate the above feature function for new substring is generated for post processing.
			Then, we use class-based HMM to re-segment the substrings mentioned above.
			Given a word ①http://ccl.pku.edu.cn/doubtfire/Course/Chinese%20Inform ation%20Processing/Source_Code/Chapter_8/Lexicon_full.
			zip wi, a word class ci is the word itself.
			Let W be the word sequence, let C be its class sequence, W # all punctuations) is half-width and the string before or after are composed of letters and numbers,combine all into a string as a whole.
			For an ex and let be the segment ation result with the ample, ‘.’, ‘/’, ‘:’, ‘%’ and ‘\’ are usually recog maximum likelihood.
			Then, a class-based HMM model (Liu, 2004) can be got.
			W # arg max P(W ) W = arg max P(W | C )P(C ) W m nized as split tokens.
			So, it needs handling additionally.
	
	
			We evaluate our system on the corpora given by CIPSSIGHAN 2010.
			There are four test corpora which belong to different domains.
			The details = arg max w1w2 ...wm = arg max w1w2 ...wm p '(wi | ci )P(ci | ci 1 ) i 1 m P(ci | ci 1 ) i 1 (5) are showed in table 1.
			Where P(ci | ci 1 ) indicates the transitive probability from one class to another and it can be obtained from training corpora.
			The word boundary of results from HMM is also represented by tag ‘B’ and ‘I’ which meaning are the same as mentioned in above.
			2.3 Confidence measure and post process-.
			ing for final result There are two segmentation results for substrings with low MP candidates after reprocessing using HMM.
			Analyzing experiments data, we find wrong tags labeled by CRF are mainly: OOV words in test data, IV words and incorrect words recognized by CRF.
			Rectifying the tags with lower MP simply may produce an even worse performance in some case.
			For example, some OOV words are recognized correctly by CRF but with low MP.
			So, we can’t accept the revised results completely.
			A confidence measure approach is used to resolve this problem.
			Its calculation is defined as: Table 1.
			Test corpora details A, B, C and D represent literature, computer science, medical science and finance, respectively.
			3.1 Closed.
			test The rule for the closed test in Bakeoff is that no additional information beyond training corpora is allowed.
			Following the rule, the closed test is designed to compare our system with other CWS systems.
			Five metrics of SIGHAN Bakeoff are used to evaluate the segmentation results: F-score (F), recall (R), precision (P), the recall on IV words (RIV) and the recall on OOV words (Roov).
			The closed test results are presented in table 2.
			PC PC (1 P ) o (6) PC is the MP of the character as ‘I’, is the premium coefficient.
			Based on the new value, a threshold t was used, if the value was lower than t , the original tag ‘I’ will be rejected and changed into the tag ‘B’ which is labeled by HMM.
			Table 2.
			Evaluation closed results on all data sets At last, we use a simple rule to post-process the result directed at the strings that containing letters, ② In order to analyze our results, we got value of R from numbers and punctuations.
			If the punctuation (not the organizers because it can’t be obtained from the scoring system on http://nlp.ict.ac.cn/demo/CIPSSIGHAN2010/#.
			In each domain, the first line shows the results of our basic CRF segmenter and the second one shows the final results dealt with HMM through confidence measure, which make it clear that using the confidence measure can improve the overall F-score by increasing value of R and P. Do ma in I D R P F R oo v R IV A 5 0.
			94 5 0.
			94 6 0.
			94 6 0.
			81 6 0.
			95 4 o u r 0.
			94 0 0.
			94 2 0.
			94 1 0.
			64 9 0.
			96 1 1 2 0.
			93 7 0.
			93 7 0.
			93 7 0.
			65 2 0.
			95 8 B o u r 0.
			95 3 0.
			95 0 0.
			95 1 0.
			82 7 0.
			97 5 1 1 0.
			94 8 0.
			94 5 0.
			94 7 0.
			85 3 0.
			96 5 1 2 0.
			94 1 0.
			94 0 0.
			94 0 0.
			75 7 0.
			97 4 C o u r 0.
			94 2 0.
			93 6 0.
			93 9 0.
			75 0 0.
			96 5 1 8 0.
			93 7 0.
			93 4 0.
			93 6 0.
			76 1 0.
			95 9 5 0.
			94 0 0.
			92 8 0.
			93 4 0.
			76 1 0.
			96 2 D o u r 0.
			95 9 0.
			96 0 0.
			95 9 0.
			82 7 0.
			97 2 1 2 0.
			95 7 0.
			95 6 0.
			95 7 0.
			81 3 0.
			97 1 9 0.
			95 6 0.
			95 5 0.
			95 6 0.
			85 7 0.
			96 5 Table 3.
			Comparison our closed results with the top three in all test sets Next, we compare it with other top three systems.
			From the table 3 we can see that our system achieves better performance on closed test.
			In contrast, the values of RIV of our method are superior to others’, which contributes to the model we use.
			Whether the features of AV for character-based CRF tagger or HMM revising, they all make good use of word information of training corpora.
			3.2 Open test.
			In the open test, the only additional source we use is the dictionary mentioned above.
			We get one first and two third best.
			Our result is showed in table 4.
			Compared with closed test, the value of RIV is increased in all test corpora.
			But we only get the higher value of F in domain of literature.
			The reasons will be analyzed as follows: In the open test, the OOV words are split into pieces because our model may be more dependent on the dictionary information.
			Consequently, we get higher value of R but lower P. The training corpora are the same as closed test, but it is different that FRE features are added.
			The additional features enhance the original information of IV words, so the value of RIV is improved to some extent.
			However, they have side effects for OOV segmentation.
			We will continue to solve this problem in the future work.
			Do ma in R P F R oo v R IV A 0.
			95 6 0.
			94 7 0.
			95 2 0.
			63 6 0.
			98 0 0.958 0.95 3 0.95 5 0.65 5 0.9 81 B 0.
			94 3 0.
			92 1 0.
			93 2 0.
			71 6 0.
			98 5 0.948 0.92 9 0.93 9 0.73 5 0.9 86 C 0.
			94 7 0.
			91 5 0.
			93 1 0.
			65 9 0.
			98 3 0.951 0.9 2 0.93 5 0.6 7 0.9 86 D 0.
			96 2 0.
			94 8 0.
			95 5 0.
			76 0 0.
			98 1 0.964 0.9 5 0.95 7 0.76 3 0.9 83 Table 4.
			Evaluation open results on all test sets
	
	
			In this paper, a detailed description on a Chinese segmentation system is presented.
			Based on intermediate results from a CRF tagger, which employs local features and global features, we use class-based HMM to revise the substrings with low marginal probabilities.
			Then, a confidence measure is introduced to combine the two results.
			Finally, we post process the strings within letters, numbers and punctuations using simple rules.
			The results above show that our system achieves the state-of-the-art performance.
			The MP plays the important role in our method and HMM revises some errors identified by CRF.
			Besides, the word features are proved to be informative cues in obtaining high quality MP.
			Therefore, our future work will focus on how to make CRF generate more reliable MP of characters, including exploring other word information or more unsupervised segmentation information.
	
