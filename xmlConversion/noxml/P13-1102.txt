
	
		Probabilistic context-free grammars havethe unusual property of not always defining tight distributions (i.e., the sum of theprobabilities of the trees the grammargenerates can be less than one).
		This paperreviews how this non-tightness can ariseand discusses its impact on Bayesian estimation of PCFGs.
		We begin by presenting the notion of almost everywhere tightgrammars and show that linear CFGs follow it.
		We then propose three differentways of reinterpreting non-tight PCFGs tomake them tight, show that the Bayesianestimators in Johnson et al.
		(2007) arecorrect under one of them, and provideMCMC samplers for the other two.
		Weconclude with a discussion of the impactof tightness empirically.
	
	
			Probabilistic Context-Free Grammars (PCFGs)play a special role in computational linguistics because they are perhaps the simplest probabilisticmodels of hierarchical structures.
			Their simplicityenables us to mathematically analyze their properties to a detail that would be difficult with linguistically more accurate models.
			Such analysisis useful because it is reasonable to expect morecomplex models to exhibit similar properties aswell.
			The problem of inferring PCFG rule probabilities from training data consisting of yields orstrings alone is interesting from both cognitive andengineering perspectives.
			Cognitively it is implausible that children can perceive the parse trees ofthe language they are learning, but it is more reasonable to assume that they can obtain the terminalstrings or yield of these trees.
			Unsupervised methods for learning a grammar from terminal stringsalone is also interesting from an engineering perspective because such training data is cheap and plentiful, while the manually parsed data requiredby supervised methods are expensive to produceand relatively rare.
			Cohen and Smith (2012) show that inferringPCFG rule probabilities from strings alone is computationally intractable, so we should not expectto find an efficient, general-purpose algorithm forthe unsupervised problem.
			Instead, approximation algorithms are standardly used.
			For example, the Inside-Outside (IO) algorithm efficientlyimplements the Expectation-Maximization (EM)procedure for approximating a Maximum Likelihood estimator (Lari and Young, 1990).
			Bayesianestimators for PCFG rule probabilities have alsobeen attracting attention because they provide atheoretically-principled way of incorporating priorinformation.
			Kurihara and Sato (2006) proposeda Variational Bayes estimator based on a mean-field approximation, and Johnson et al.
			(2007) proposed MCMC samplers for the posterior distribution over rule probabilities and the parse trees ofthe training data strings.
			PCFGs have the interesting property (which weexpect most linguistically more realistic models toalso possess) that the distributions they define arenot always properly normalized or tight.
			In anon-tight PCFG the partition function (i.e., sumof the probabilities of all the trees generated bythe PCFG) is less than one.
			(Booth and Thompson, 1973, called such non-tight PCFGs inconsistent, but we follow Chi and Geman (1998)in calling them non-tight to avoid confusionwith the consistency of statistical estimators).
			Chi(1999) showed that renormalized non-tight PCFGs(which he called Gibbs CFGs) define the sameclass of distributions over trees as do tight PCFGswith the same rules, and provided an algorithm formapping any PCFG to a tight PCFG with the samerules that defines the same distribution over trees.An obvious question is then: how does tightnessaffect the inference of PCFGs?
			Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).
			This means that MLestimators can simply ignore issues of tightness,and rest assured that the PCFGs they estimate arein fact tight.
			The situation is more subtle with Bayesian estimators.
			We show that for the special case oflinear PCFGs (which include HMMs) with non-degenerate priors the posterior puts zero mass onnon-tight PCFGs, so tightness is not an issue withBayesian estimation of such grammars.
			However,because all of the commonly used priors (such asthe Dirichlet or the logistic normal) assign nonzero probability across the whole probability simplex, in general the posterior may assign nonzeroprobability to non-tight PCFGs.
			We discuss threedifferent possible approaches to this in this paper: 1.
			the only-tight approach, where we modify theprior so it only assigns nonzero probabilityto tight PCFGs, 2.
			the renormalization approach, where werenormalize non-tight PCFGs so they definea probability distribution over trees, and 3.
			the sink-element approach, where we reinterpret non-tight PCFGs as assigning nonzeroprobability to a sink element, so both tightand non-tight PCFGs are properly normalized.
			We show how to modify the Gibbs sampler described by Johnson et al.
			(2007) so it producessamples from the posterior distributions definedby the only-tight and renormalization approaches.Perhaps surprisingly, we show that Gibbs sampleras defined by Johnson et al. actually producessamples from the posterior distributions defined bythe sink-element approach.
			We conclude by studying the effect of requiring tightness on the estimation of some simplePCFGs.
			Because the Bayesian posterior convergesaround the (tight) ML estimate as the size ofthe data grows, requiring tightness only seems tomake a difference with highly biased priors or withvery small training corpora.
	
	
			LetG = (T,N, S,R) be a Context-Free Grammarin Chomsky normal form with no useless productions, where T is a finite set of terminal symbols, N is a finite set of nonterminal symbols (disjointfrom T ), S ? N is a distinguished nonterminalcalled the start symbol, andR is a finite set of productions of the form A ? BC or A ? w, whereA,B,C ? N and w ? T . In what follows we use as a variable ranging over (N N) ? T . A Probabilistic Context-Free Grammar (G,T)is a pair consisting of a context-free grammar Gand a real-valued vector T of length |R| indexedby productions, where ?A? is the productionprobability associated with the production A ? ? R. We require that ?A? = 0 and that forall nonterminals A ? N , ?A??RA ?A? = 1,where RA is the subset of rules R expanding thenonterminal A. A PCFG (G,T) defines a measure T overtrees t as follows: T(t) =?r?R ?fr(t)r where fr(t) is the number of times the productionr = A?
			 ? R is used in the derivation of t. The partition function Z or measure of all possible trees is: Z(T) =?t'?T ?r?R ?fr(t') r where T is the set of all (finite) trees generatedby G. A PCFG is tight iff the partition functionZ(T) = 1.
			In this paper we use T?
			to denote theset of rule probability vectors T for which G isnon-tight.
			Nederhof and Satta (2008) survey several algorithms for computing Z(T), and hencefor determining whether a PCFG is tight.1 Non-tightness can arise in very simple PCFGs,such as the Catalan PCFG S ? S S | a. Thisgrammar produces binary trees where all internalnodes are labeled as S and the yield of these treesis a sequence of as.
			If the probability of the ruleS ? S S is greater than 0.5 then this PCFG isnon-tight.
			Perhaps the most straightforward way to understand this non-tightness is to view this grammar asdefining a branching process where an S can eitherreproduce with probability ?S?S S or die out 1We found out that finding whether a PCFG is tight bydirectly inspecting the partition function value is less stablethan using the method in Wetherell (1980).
			For this reason,we used Wetherells approach, which is based on finding theprincipal eigenvalue of the matrix M . 1034 with probability ?S?a. When ?S?S S > ?S?a theS nodes reproduce at a faster rate than they dieout, so the derivation has a nonzero probability ofendlessly rewriting (Atherya and Ney, 1972).
	
	
			The goal of Bayesian inference for PCFGs is to infer a posterior distribution over the rule probability vectors T given observed data D. This posterior distribution is obtained by combining the likelihood P(D | T) with a prior distribution P(T)over T using Bayes Rule.
			P(T | D) ? P(D | T) P(T) We now formally define the three approaches tohandling non-tightness mentioned earlier: the only-tight approach: we only permit priorswhere P(T?) = 0, i.e., we insist that theprior assign zero mass to non-tight rule probability vectors, so Z = 1.
			This means we candefine: P(t | T) = T(t) the renormalization approach: we renormalizenon-tight PCFGs by dividing by the partitionfunction: P(t | T) = 1Z(T) T(t) (1) the sink-element approach: we redefine ourprobability distribution so its domain is a setT ' = T ? {?}, where T is the set of (finite)trees generated by G and ? 6?
			T is a newelement that serves as a sink state to whichthe missing mass 1 - Z(T) is assigned.Then we define:2 P(t | T) ={T(t) if t ? T1- Z(T) if t = ? 2This definition of a distribution over trees can be inducedby a tight PCFG with a special ? symbol in its vocabulary.Given G, the first step is to create a tight grammar G0 usingthe renormalization approach.
			Then, a new start symbol isadded to G0, S0, and also rules S0 ? S (where S is theold start symbol in G0) and S0 ? ?.
			The first rule is givenprobability Z(T) and the second rule is given probability 1-Z(T).
			It can be then readily shown that the new tight PCFGG0 induces a distribution over trees just like in Eq. 3, onlywith additional S0 on top of all trees.
			With this in hand, we can now define the likelihood term.
			We consider two types of data D here.In the supervised setting the data D consists of acorpus of parse trees D = (t1, . . .
			, tn) where eachtree ti is generated by the PCFG G, so P(D | T) =n?i=1 P(ti | T) In the unsupervised setting the data D consistsof a corpus of strings D = (w1, . . .
			, wn) whereeach string wi is the yield of one or more treesgenerated by G. In this setting P(D | T) =n?i=1 P(wi | T),where: P(w | T) =?
			t?T :yield(t)=wP(t | T)
	
	
			One way to handle the issue of tightness is to identify a family of CFGs for which practically any parameter setting will yield a tight PCFG.
			This is thefocus of this section, in which we identify a subset of CFGs, which are almost everywhere tight.This family of CFGs includes many of the CFGsused in NLP applications.
			We cannot expect that a CFG will yield a tightPCFG for any assignment to the rule probabilities(i.e. that T?
			= ).
			Even in simple cases, such asthe grammar S ? S|a, the assignment of probability 1 to S ? S and 0 to the other rule rendersthe S nonterminal useless, and places all of theprobability mass on infinite structures of the formS ? S ? S ? .
			However, we can weaken our requirement sothat the cases in which parameter assignmentyields a non-tight PCFG are rare, or have measurezero.
			To put it more formally, we say that a priorP(T) is tight almost everywhere for G if P(T?) =?
			T?T?P(T) dT = 0.
			We now provide a sufficient condition (linear-ity) for CFGs under which they are tight almosteverywhere with any continuous prior.
			For a nonterminal A ? N and  ?
			(N ? T )*,we use A?k  to denote that A can be re-writtenusing a sequence of rules from R to the sententialform  in k derivation steps.
			We use A ?+  todenote that there exists a k > 0 such thatA?k . 1035 Definition 1 A context-free grammarG is linear ifthere are no A ? N such that A?+ . . .
			A . . .
			A . . .
			.Definition 2 A nonterminal A ? N in a probabilistic context-free grammar G with parametersT is nonterminating if PG(A?+ . . .
			A . . .
			|T) = 1.Here P(A?+ . . .
			A . . .
			|T) is defined as:?
			:=...A...
			PG(A?+ |T).
			Lemma 1 A linear PCFG G with parameters Twhich does not have any nonterminating nonterminals is tight.
			Proof: Our proof relies on the properties of a certain |N |  |N | matrix M where: MAB =?
			A??RAn(,B) ?A? where n(,B) is the number of appearances of thenonterminal B in the sequence . MAB is the expected number of B nonterminals generated froman A nonterminal in one single derivational step,so [Mk]AB is the expected number ofB nonterminals generated from an A nonterminal in a k-stepderivation (Wetherell, 1980).
			Since M is a non-negative matrix, under someregularity conditions, the FrobeniusPerron theorem states that the largest eigenvalue of this matrix (in absolute value) is a real number.
			Let thiseigenvalue be denoted by ?.
			A PCFG is called subcritical if ? < 1 andsupercritical if ? > 1.
			Then, in turn, a PCFG istight if it is subcritical.
			It is not tight if it is supercritical.
			The case of ? = 1 is a borderline casethat does not give sufficient information to knowwhether the PCFG is tight or not.
			In the Bayesiancase, for a continuous prior such as the Dirichletprior, this borderline case will have measure zerounder the prior.
			Now let A ? N . Since the grammar is linear, there is no derivation A ?+ . . .
			A . . .
			A . . ..Therefore, any derivation of the form A ?+.
			A . . .
			includes A on the right hand-side exactlyonce.
			Because the grammar has no useless non-terminals, the probability of such a derivation isstrictly smaller than 1.
			For each A ? N , define: pA =?
			=...A...
			P(A?|N | |T).
			Since A is not useless, then pA < 1.
			Thereforeq = maxA pA < 1.
			Since any derivation of lengthk of the formA?
			A . . .
			can be decomposed toat least k 2|N | cycles that start at a terminal B ? Nand end in the same nonterminal B ? N , it holdsthat: [Mk]AA = qk 2|N| k?8?
			0. This means that trace(Mk) k?8?
			0. This meansthat the eigenvalue of M is strictly smaller than 1(linear algebra), and therefore the PCFG is tight.?Proposition 1 Any continuous prior P(T) on alinear grammar G is tight almost everywhere forG.
			Proof: Let G be a linear grammar.
			With a continuous prior, the probability ofG getting parametersfrom the prior which yield a useless non-terminalis 0  it would require setting at least one rule inthe grammar with rule probability which is exactly1.
			Therefore, with probability 1, the parameterstaken from the prior yield a PCFG which is linearand does not have nonterminating nonterminals.According to Lemma 1, this means the PCFG istight.
			Deciding whether a grammar G is linear canbe done in polynomial time using the constructionfrom Bar-Hillel et al.
			(1964).
			We can first eliminate the differences between nonterminals and terminal symbols by adding a rule A ? cA for eachnonterminal A ? N , after extending the set ofterminal symbols A with {cA|A ? N}.
			Let GAbe the grammar G with the start symbol being replaced with A. We can then intersect the grammarGA with the regular language T *cAT *cAT * (foreach nonterminal A ? N ).
			If for any nonterminal A the intersection is not the empty set (withrespect to the language that the intersection generates), then the grammar is not linear.
			Checkingwhether the intersection is the empty set or not canbe done in polynomial time.
			We conclude this section by remarking thatmany of the models used in computational linguistics are in fact equivalent to linear PCFGs, socontinuous Bayesian priors are almost everywheretight.
			For example, HMMs and many kinds ofstacked finite-state machines are equivalent to 1036 linear PCFGs, as are the example PCFGs given inJohnson et al.
			(2007) to motivate the MCMC estimation procedures.
	
	
			The first step in Bayesian inference is to specify aprior on T. In the rest of this paper we take P(T)to be a product of Dirichlet distributions, with onedistribution for each non-terminal A ? N , as thisturns out to simplify the computations considerably.
			The prior is parameterized by a positive realvalued vector a indexed by productionsR, so eachproduction probability ?A? has a correspondingDirichlet parameter aA? . As before, let RA bethe set of productions in R with left-hand side A,and let ?A and aA refer to the component subvec-tors of ? and a respectively indexed by productions in RA.
			The Dirichlet prior P(T | a) is: P(T | a) =?A?N PD(TA | aA), where PD(TA | aA) = 1C(aA) ?r?RA ?ar1r and C(aA) = ?r?RA G(ar) G(?
			r?RA ar) where G is the generalized factorial function andC(a) is a normalization constant that does not depend on TA.
			Dirichlet priors are useful because they are conjugate to the multinomial distribution, which isthe building block of PCFGs.
			Ignoring issues oftightness for the moment and setting P(t | T) =T(t), this means that in the supervised setting theposterior distribution P(T | t, a) given a set ofparse trees t = (t1, . . .
			, tn) is also a product ofDirichlets distribution.
			P(T | t, a) ? P(t | T) P(T | a) ?(?r?R ?fr(t)r )(?r?R ?ar1r )=?r?R ?fr(t)+ar1r which is a product of Dirichlet distributions withparameters f(t) + a, where f(t) is the vector ofrule counts in t indexed by r ? R. We can thuswrite: P(T | t, a) = P(T | f(t) + a) Input: Grammar G, vector of trees t, vector ofhyperparameters a, previous parameters T0.
			Result: A vector of parameters Trepeat draw ? from products of Dirichlet withhyperparameters a+ f(t) until T is tight for G;return T Algorithm 1: An algorithm for generating samples from P(T | t, a) for the only-tight approach.
			Input: Grammar G, vector of trees t, vector ofhyperparameters a, previous rule parametersT0.
			Result: A vector of parameters Tdraw a proposal T* from a product of Dirichlets withparameters a+ f(t).draw a uniform number u from [0, 1].
			if u < min{1,(Z(T(i-1))/Z(T*) )n} return T*.
			return T0.
			Algorithm 2: One step of Metropolis-Hastingsalgorithm for generating samples from P(T |t, a) for the renormalization approach.
			which makes it clear that the rule counts are directly added to the parameters of the prior to produce the parameters of the posterior.
	
	
			We first discuss Bayesian inference in the supervised setting, as inference in the unsupervised setting is based on inference for the supervised setting.
			For each of the three approaches to non-tightness we provide an algorithm that characterizes the posterior P(T | t), where t = (t1, . . .
			, tn)is a sequence of trees, by generating samples fromthat posterior.
			Our MCMC algorithms for the unsupervised setting build on these samplers for thesupervised setting.
			6.1 The only-tight approachThe only-tight approach requires that the priorassign zero mass to non-tight rule probability vectors T?.
			One way to define such a distribution isto restrict the domain of an existing prior distribution with the set of tight T and renormalize.
			Inmore detail, if P(T) is a prior over rule probabilities, then its renormalization is the prior P' definedas:.
			P'(T) =P(T)I(T /?
			T?) Z(T?).
			(2) where Z(T?) =?
			T P(T)I(T /?
			T?)dT.
			1037 Input: Grammar G, vector of trees t, vector ofhyperparameters a, previous parameters T0.
			Result: A vector of parameters Tdraw T from products of Dirichlet withhyperparameters a+ f(t)return T Algorithm 3: An algorithm for generating samples from P(T | t, a) for the sink-state approach.
			Perhaps surprisingly, it turns out that if P(T)belongs to a family of conjugate priors, then P'(T)also belongs to a (different) family of conjugatepriors as well.
			Proposition 2 Let P(T|a) be a prior with hyper-parameters a over the parameters of G such thatP is conjugate to the grammar likelihood.
			ThenP', defined in Eq. 2, is conjugate to the grammarlikelihood as well.
			Proof: Assume that trees t are observed, and theprior over the grammar parameters is the prior defined in Eq. 2.
			Therefore, the posterior is: P(T|t, a) ? P'(T|a)p(t|T) =P(T|a)p(t|T)I(T /?
			T?) Z(T?) ? P(T|t, a)I(T /?
			T?) Z(T?).
			Since P(T|a) is a conjugate prior to the PCFGlikelihood, then there exists a' = a'(t) such thatP(T|t, a) = P'(T|a').
			Therefore: P(T|t, a) ? P(T|a')I(T /?
			T?)Z(T?) .which exactly equals P'(T|a').
			?Sampling from the posterior over the parame ters given a set of trees t is therefore quite simple when assuming the base prior being renormalized is a product of Dirichlets.
			Algorithm 1 samples from a product of Dirichlets distribution withhyperparameters a + f(t) repeatedly, each timechecking and rejecting the sample until we obtaina tight PCFG.
			The more mass the Dirichlet distribution withhyperparameters a + f(t) puts on non-tightPCFGs, the more rejections will happen.
			In general, if the probability mass on non-tight PCFGs isq?, then it would require, on average 1/(1 - q?)samples from this distribution in order to obtain atight PCFG.
			6.2 The renormalization approachThe renormalization approach modifies the likelihood function instead of the prior.
			Here we use aproduct of Dirichlets prior P(T | a) on rule probability vectors T, but the presence of the partitionfunctionZ(T) in Eq. 1 means that the likelihood isno longer conjugate to the prior.
			Instead we have:.
			P(T | t) =n?i=1 T(ti) Z(T)P(T | a) ? 1Z(T)n P(T | a+ f(t)).
			(3) Note that the factor Z(T) depends on T, andtherefore cannot be absorbed into the constant.
			Algorithm 2 describes a Metropolis-Hastings sampler for sampling from the posterior in Eq. 3that uses a product of Dirichlets with parametersa+ f(t) as a proposal distribution.
			In our experiments, we use the algorithm fromNederhof and Satta (2008) to compute the partition function which is needed in Algorithm 2.
			6.3 The sink element approachThe sink element approach does not affect thelikelihood (since the probability of a tree t is justthe product of the probabilities of the rules usedto generate it), nor does it require a change to theprior.
			(The sink element ? is not a member of theset of trees T , so it cannot appear in the data t)..
			This means that the conjugacy argument givenat the bottom of section 5 holds in this approach,so the posterior P(T | t, a) is a product of Dirich-lets with parameters f(t) + a. Algorithm 3 givesa sampler for P(T | t, a) for the sink element approach.
	
	
			Johnson et al.
			(2007) provide two Markov chainMonte Carlo algorithms for Bayesian inference forPCFG rule probabilities in the unsupervised setting (i.e., where the data consists of a corpus ofstrings w = (w1, . . .
			, wn) alone).
			The algorithmswe give here are based on their Gibbs sampler,which in each iteration first samples parse treest = (t1, . . .
			, tn), where each ti is a parse forwi, from P(t | w,T), and then samples T fromP(T | t, a).
			Notice that the conditional distribution P(t |w,T) is unaffected in each of our three approaches (the partition functions cancel in the 1038 Input: Grammar G, vector of hyperparameters a,vector of strings w = (w1, . . .
			, wn), previousrule parameters T0.
			Result: A vector of parameters Tfor i?
			1 to n do draw ti from P(ti|wi,T0)enduse Algorithm 2 to sample T given G, t, a and T0return T Algorithm 4: One step of the Metropolis-within-Gibbs sampler for the renormalization approach.
			renormalization approach), so the algorithm forsampling from P(t | w,T) given by Johnson etal.
			applies in each of our three approaches as well.
			Johnson et al. ignored tightness and assumedthat P(T | t, a) is a product of Dirichlets withparameters f(t) + a. As we noted in section 6.3,this assumption holds for the sink-state approachto non-tightness, so their sampler is in fact correctfor the sink-state approach.
			In fact, we obtain samplers for the unsupervisedsetting for each of our approaches by pluggingin the corresponding sampling algorithm (Eq. 13) for P(T | t, a) into the generic Gibbs samplerframework of Johnson et al. The one complication is that because we use aMetropolisHastings procedure to generate samples from P(T | t, a) in the renormalization approach, we use the Metropolis-within-Gibbs procedure given in Algorithm 4 (Robert and Casella,2004).
	
	
			Probably the most important question to ask withrespect to the three different approaches to non-tightness is whether they differ in terms of expressive power.
			Clearly the three approaches differ interms of the grammars they admit (the only-tightapproach requires the prior to only assign nonzeroprobability to tight PCFGs, while the other two approaches permit the prior to assign nonzero probability to non-tight PCFGs as well).
			However, ifwe regard a grammar as merely a device for defining a distribution over trees and a prior as defininga distribution over distributions over trees, it is reasonable to ask whether the class of distributionsover distributions of trees that each of these approaches define are the same or differ.
			We believe,but have not proved, that all three approaches define the same class of distributions over distribu tions of trees in the following sense: any prior usedwith one of the approaches can be transformedinto a different prior that can be used with one ofthe other approaches, and yield the same posteriorover trees conditioned on a string, marginalizingout the parameters.
			This does not mean that the three approachesare equivalent, however.
			In this section we provide a grammar such that with a uniform prior overrule probabilities, the conditional distribution overtrees given a fixed string varies under each of thethree different approaches.
			The grammar we consider has three rules S ?S S S|S S|a with probabilities ?1, ?2 and 1- ?1-?2, respectively.
			The T parameters are required tosatisfy ?1 + ?2 = 1 and ?i = 0 for i = 1, 2.
			We compute the posterior distribution overparse trees for the string w = a a a. The grammar generates three parse trees for w1, namely: t1 = S S a S a S a t2 = S S a S S a S a t3 = S S S a S a S a The partition function Z for this grammar is thesmallest positive root of the cubic equation: Z = ?1Z3 + ?2Z 2 + (1- ?1 - ?2)We used Mathematica to find an analytic solutionfor Z in this equation, obtaining not only an expression for the partition function Z(T) but alsoidentifying the non-tight region T?.
			In order to compute P(t1|w), we used Mathematica to first compute the following quantities: qsinkElement(ti) = ?TT(ti) dT qtightOnly(ti) = ?TT(ti) I(T /?
			T?) dT qrenormalization(ti) = ?TT(ti)/Z(T) dT where i ? {1, 2, 3}.
			We used Mathematica to analytically compute q(ti) for each approach and eachi ? {1, 2, 3}.
			Then its easy to show that: P(ti | w) = q(ti)?3i'=1 q(ti') where the q used is based on the approach totightness desired.
			For the sink-element approach, 1039 0 10 20 30 0.35 0.40 0.45 0.50 0.55Average f-score Den sity Inferenceonly-tight sink-state renormalise Figure 1: The density of the F1-scores with thethree approaches.
			The prior used is a symmetricDirichlet with a = 0.1.
			P(t1|w) = 711  0.636364.
			For the only-tightapproach P(t1|w) = 1117917221  0.649149.
			Forthe renormalization approach the analytic expression is too complex to include in this paper,but it approximately equals 0.619893.
			A logof our Mathematica calculations is availableat http://www.cs.columbia.edu/scohen/acl13tightnessmathematica.pdf, and weconfirmed these results to three decimal places using the samplers described above (which required107 samples per approach).
			While the differences between these conditionalprobabilities are not great, the conditional probabilities are clearly different, so the three approaches do in fact define different distributionsover trees under a uniform prior on rule probabilities.
	
	
			In this section we present experiments using thethree samplers just described in an unsupervisedgrammar induction problem.
			Our goal here isnot to improve the state-of-the-art in unsupervisedgrammar induction, but to try to measure empirical differences in the estimates produced by thethree different approaches to tightness just described.
			The bottom line of our experiments is thatwe could not detect any significant difference inthe estimates produced by samplers for these threedifferent approaches.In our experiments we used the English Penntreebank (Marcus et al., 1993).
			We use the part of-speech tag sequences of sentences shorter than11 words in sections 221.
			The grammar we use isthe PCFG version of the dependency model withvalence (Klein and Manning, 2004), as it appearsin Smith (2006).
			We used a symmetric Dirichlet prior with hyperparameter a = 0.1.
			For each of the three approaches for handling tightness, we ran 100 timesthe samplers in 7, each for 1,000 iterations.
			Wediscarded the first 900 sweeps of each run, and calculated the F1-scores of the sampled trees every10th sweep from the last 100 sweeps.
			For eachrun we calculated the average F1-score over the10 sweeps we evaluated.
			We thus have 100 average F1-scores for each of the samplers.
			Figure 1 plots the density of F1 scores (compared to the gold standard) resulting from theGibbs sampler, using all three approaches.
			Themean value for each of the approaches is 0.41with standard deviation 0.06 (only-tight), 0.41with standard deviation 0.05 (renormalization)and 0.42 with standard deviation 0.06 (sink element).
			In addition, the only-tight approach resultsin an average of 437 (s.d., 142) rejected proposals in 1,000 samples, while the renormalizationapproach results in an average of 232 (s.d., 114)rejected proposals in 1,000 samples.
			(Its not surprising that the only-tight approach results in morerejections as it keeps proposing new T until a tightproposal is found, while the renormalization approach simply uses the old T).
			We performed two-sample KolmogorovSmirnov tests (which are non-parametric testsdesigned to determine if two distributions aredifferent; see DeGroot, 1991) on each of the threepairs of 100 F1-scores.
			None of the tests wereclose to significant; the p-values were all above0.5.
			Thus our experiments provided no evidencethat the samplers produced different distributionsover trees, although its reasonable to expect thatthese distributions do indeed differ.
			In terms of running time, our implementationof the renormalization approach was several timesslower than our implementations of the other twoapproaches because we used the naive fixed-pointalgorithm to compute the partition function: perhaps this could be improved using one of themore sophisticated partition function algorithmsdescribed in Nederhof and Satta (2008).
			1040
	
	
			In this paper we characterized the notion of an almost everywhere tight grammar in the Bayesiansetting and showed it holds for linear CFGs.
			Fornon-linear CFGs, we described three different approaches to handle non-tightness.
			The only-tight approach restricts attention to tight PCFGs,and perhaps surprisingly, we showed that conjugacy still obtains when the domain of a productof Dirichlets prior is restricted to the subset oftight grammars.
			The renormalization approach involves renormalizing the PCFG measure  overtrees when the grammar is non-tight, which destroys conjugacy with a product of Dirichlets prior.Perhaps most surprisingly of all, the sink-elementapproach, which assigns the missing mass in non-tight PCFG to a sink element ?, turns out to beequivalent to existing practice where tightness isignored.
			We studied the posterior distributions over treesinduced by the three approaches under a uniformprior for a simple grammar and showed that theydiffer.
			We leave for future work the importantquestion of whether the classes of distributionsover distributions over trees that the three approaches define are the same or different.
			We described samplers for the supervisedand unsupervised settings for each of these approaches, and applied them to an unsupervisedgrammar induction problem.
			(The code for theunsupervised samplers is available from http://web.science.mq.edu.au/mjohnson).
			We could not detect any difference in the posterior distributions over trees produced by thesesamplers, despite devoting considerable computational resources to the problem.
			This suggests thatfor these kinds of problems at least, tightness isnot of practical concern for Bayesian inference ofPCFGs.
	
	
	
