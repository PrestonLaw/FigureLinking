
	
	
	
			3.
			SB performs negative reinforcement on SA externally;.
			4.
			SB performs positive reinforcement on its own internally..
			Positive reinforcement captures the intuition that a sentence is more important if it associates to the other important sentences in the same collection.
			Negative reinforcement, on the other hand, reflects the fact that a sentence is less A B + + - - 492 important if it associates to the important sentences in the other collection, since such a sentence might repeat the same or very similar information which is supposed to be included in the summary generated for the other collection.
			Let RA and RB denote the ranking of the sentences in A and B, the reinforcement can be formally described as ??
			++= ++=+ + Bk BBBk ABAk B Ak BABk AAAk A pRMRMR pRMRMRr r 2)( 2)( 2)1( 1)( 1)( 1)1( ?a?a (1) where the four matrices MAA, MBB, MAB and MBA are the affinity matrices of the sentences in SA, in SB, from SA to SB and from SB to SA.
			?= 22 11 aa W is a weight matrix to balance the reinforcement among different sentences.
			Notice that 0, 21 < such that they perform negative reinforcement.
			Ap r and Bp r are two bias vectors, with 1,0 21 << ??
			as the damping factors.
			[ ]1 1 =n A npr , where n is the order of MAA.
			Bp r is defined in the same way.
			We will further define the affinity matrices in section 3.2 later.
			With the above reinforcement ranking equation, it is also true that 1.
			A sentence in SB correlates to many new sentences in SB is supposed to receive a high ranking from RB, and 2.
			A sentence in SB correlates to many old sentences in SA is supposed to receive a low ranking from RB.
			Let [ ]TBA RRR = and [ ]TBA ppp rrr = 21 ??
			, then the above iterative equation (1) corresponds to the linear system, ( ) pRMI r=- (2) where, ??
			?= BBBA ABAA MMMM M22 11 aa . Up to now, the PNR2 is still query-independent.
			That means only the content of the sentences is considered.
			However, for the tasks of query-oriented summarization, the reinforcement should obviously bias to the users query.
			In this work, we integrate query information into PNR2 by defining the vector pr as ( )qsrelp ii |=r , where ( )qsrel i | denotes the relevance of the sentence si to the query q. To guarantee the solution of the linear system Equation (2), we make the following two transformations on M. First M is normalized by columns.
			If all the elements in a column are zero, we replace zero elements with n1 (n is the total number of the elements in that column).
			Second, M is multiplied by a decay factor ?
			( 10 <<?
			), such that each element in M is scaled down but the meaning of M will not be changed.
			Finally, Equation (2) is rewritten as, ( ) pRMI r=- ?
			(3) The matrix ( )MI -?
			is a strictly diagonally dominant matrix now, and the solution of the linear system Equation (3) exists.
			3.2 Sentence Ranking based on PNR2 We use the above mentioned PNR2 framework to rank the sentences in both SA and SB simultaneously.
			Section 3.2 defines the affinity matrices and presents the ranking algorithm.
			The affinity (i.e. similarity) between two sentences is measured by the cosine similarity of the corresponding two word vectors, i.e. [ ] ( )ji sssimjiM ,, = (4) where ( ) ji jiji ss sssssim rr rr  =, . However, when.
			calculating the affinity matrices MAA and MBB, the similarity of a sentence to itself is defined as 0, i.e. [ ] ( )???
			= ?= ji jisssimjiM ji 0 , , (5) Furthermore, the relevance of a sentence to the query q is defined as ( )qs qsqsrel i ii rr rr * =, (6) Algorithm 1.
			RankSentence(SA, SB, q) Input: The old sentence set SA, the new sentence set SB, and the query q. Output: The ranking vectors R of SA and SB.
			1: Construct the affinity matrices, and set the weight matrix W; 2: Construct the matrix ( )MIA -= ? .
			3: Choose (randomly) the initial non-negative vectors TR ]11[)0( L= ; 4: 0?k , 0??
			; 5: Repeat 6: ( )?
			?< >++ --= ij ij kjijkjiji ij ki RaRap aR )()1()1( 1 r ; 7: ( ))()1(max kk RR -??
			+ ; 8: )1( +kR is normalized such that the maximal element in )1( +kR is 1.
			493 9: 1+?
			kk ; 10: Until ?<?
			1; 11: )(kRR ? ; 12: Return.
			Now, we are ready to adopt the GaussSeidel method to solve the linear system Equation (3), and an iterative algorithm is developed to rank the sentences in SA and SB.
			After sentence ranking, the sentences in SB with higher ranking will be considered to be included in the final summary.
			3.3 Sentence Selection by Removing Redundancy.
			When multiple documents are summarized, the problem of information redundancy is more severe than it is in single document summarization.
			Redundancy removal is a must.
			Since our focus is designing effective sentence ranking approach, we apply the following simple sentence selection algorithm.
			Algorithm 2.
			GenerateSummary(S, length) Input: sentence collection S (ranked in descending order of significance) and length (the given summary length limitation) Output: The generated summary ? {}??
			; ?l length; For i ? 0 to |S| do threshold ?
			( )( )??ssssim i ,max ; If threshold <= 0.92 do isU???
			; ll ? - ( )islen ; If ( l <= 0) break; End End Return ? .
			4 Experimental Studies 4.1 Data Set and Evaluation Metrics The experiments are set up on the DUC 2007 update pilot task data set.
			Each collection of documents is accompanied with a query description representing a users information need.
			We simply focus on generating a summary for the document collection B given that the.
			1 ? is a predefined small real number as the convergence threshold.
			2 In fact, this is a tunable parameter in the algorithm.
			We use the value of 0.9 by our intuition.
			user has read the document collection A, which is a typical update summarization task.
			Table 1 below shows the basic statistics of the DUC 2007 update data set.
			Stop-words in both documents and queries are removed 3 and the remaining words are stemmed by Porter Stemmer 4 . According to the task definition, system-generated summaries are strictly limited to 100 English words in length.
			We incrementally add into a summary the highest ranked sentence of concern if it doesnt significantly repeat the information already included in the summary until the word limitation is reached.
			A B Average number of documents 10 10 Average number of sentences 237.6 177.3 Table 1.
			Basic Statistics of DUC2007 Update Data Set As for the evaluation metric, it is difficult to come up with a universally accepted method that can measure the quality of machine-generated summaries accurately and effectively.
			Many literatures have addressed different methods for automatic evaluations other than human judges.
			Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations.
			Given the fact that judgments by humans are time-consuming and labor-intensive, and more important, ROUGE has been officially adopted for the DUC evaluations since 2005, like the other researchers, we also choose it as the evaluation criteria.
			In the following experiments, the sentences and the queries are all represented as the vectors of words.
			The relevance of a sentence to the query is calculated by cosine similarity.
			Notice that the word weights are normally measured by the document-level TF*IDF scheme in conventional vector space models.
			However, we believe that it is more reasonable to use the sentence-level inverse sentence frequency (ISF) rather than document-level IDF when dealing with sentence-level text processing.
			This has been verified in our early study.
			4.2 Comparison of Positive and Negative Reinforcement Ranking Strategy.
			The aim of the following experiments is to investigate the different reinforcement ranking strategies.
			Three algorithms (i.e. PR(B), 3 A list of 199 words is used to filter stop-words.
			4 http://www.tartarus.org/~martin/PorterStemmer.
			5 ROUGE version 1.5.5 is used..
			494 PR(A+B), PR(A+B/A)) are implemented as reference.
			These algorithms are all based on the query-sensitive LexRank (OtterBacher et al., 2005).
			The differences are twofold: (1) the document collection(s) used to build the text graph are different; and (2) after ranking, the sentence selection strategies are different.
			In particular, PR(B) only uses the sentences in B to build the graph, and the other two consider the sentences in both A and in B.
			Only the sentences in B are considered to be selected in PR(B) and PR(A+B/A), but all the sentences in A and B have the same chance to be selected in PR(A+B).
			Only the sentences from B are considered to be selected in the final summaries in PNR2 as well.
			In the following experiments, the damping factor is set to 0.85 in the first three algorithms as the same in PageRank.
			The weight matrix W is set to ??
			15.05.01 in the proposed algorithm (i.e. PNR2) and 5.021 == ??
			We have obtained reasonable good results with the decay factor ? between 0.3 and 0.8.
			So we set it to 0.5 in this paper.
			Notice that the three PageRank-like graph-based ranking algorithms can be viewed as only the positive reinforcement among the sentences is considered, while both positive and negative reinforcement are considered in PNR2 as mentioned before.
			Table 2 below shows the results of recall scores of ROUGE-1, ROUGE-2 and ROUGE-SU4 along with their 95% confidential internals within square brackets.
			ROUGE -1 ROUGE -2 ROUGE-SU4 PR(B) 0.3323 [0.3164,0.3501] 0.0814 [0.0670,0.0959] 0.1165 0.1053,0.1286] PR(A+B) 0.3059 [0.2841,0.3256] 0.0746 [0.0613,0.0893] 0.1064 [0.0938,0.1186] PR(A+B/A) 0.3376 [0.3186,0.3572] 0.0865 [0.0724,0.1007] 0.1222 [0.1104,0.1304] PNR2 0.3616 [0.3464,0.3756] 0.0895 [0.0810,0.0987] 0.1291 [0.1208,0.1384] Table 2.
			Experiment Results We come to the following three conclusions.
			First, it is not surprising that PR(B) and PR(A+B/A) outperform PR(A+B), because the update task obviously prefers the sentences from the new documents (i.e. B).
			Second, PR(A+B/A) outperforms PR(B) because the sentences in A can provide useful information in ranking the sentences in B, although we do not select the sentences ranked high in A.
			Third, PNR2 achieves the best performance.
			PNR2 is above PR(A+B/A) by 7.11% of ROUGE-1, 3.47% of ROUGE-2, and 5.65% of ROUGE-SU4.
			This result confirms the idea and algorithm proposed in this work.
			4.3 Comparison with DUC 2007 Systems Twenty-four systems have been submitted to the DUC for evaluation in the 2007 update task.
			Table 3 compares our PNR2 with them.
			For reference, we present the following representative ROUGE results of (1) the best and worst participating system performance, and (2) the average ROUGE scores (i.e. AVG).
			We can then easily locate the positions of the proposed models among them..
			PNR2 Mean Best / Worst ROUGE-1 0.3616 0.3262 0.3768/0.2621 ROUGE2 0.0895 0.0745 0.1117/0.0365 ROUGE-SU4 0.1291 0.1128 0.1430/0.0745 Table 3.
			System Comparison 4.4 Discussion In this work, we use the sentences in the same sentence set for positive reinforcement and sentences in the different set for negative reinforcement.
			Precisely, the old sentences perform negative reinforcement over the new sentences while the new sentences perform positive reinforcement over each other.
			This is reasonable although we may have a more comprehensive alternation.
			Old sentences may express old topics, but they may also express emerging new topics.
			Similarly, new sentences are supposed to express new topics, but they may also express the continuation of old topics.
			As a result, it will be more comprehensive to classify the whole sentences (both new sentences and old sentences together) into two categories, i.e. old topics oriented sentences and new topic oriented sentences, and then to apply these two sentence sets in the PNR2 framework.
			This will be further studied in our future work..
			Moreover, in the update summarization task, the summary length is restricted to about 100 words.
			In this situation, we find that sentence simplification is even more important in our investigations.
			We will also work on this issue in our forthcoming studies.
			5 Conclusion In this paper, we propose a novel sentence ranking algorithm, namely PNR2, for update summarization.
			As our pilot study, we simply assume to receive two chronologically ordered document collections and evaluate the summaries.
			495 generated for the collection given later.
			With PNR2, sentences from the new (i.e. late) document collection perform positive reinforcement among each other but they receive negative reinforcement from the sentences in the old (i.e. early) document collection.
			Positive and negative reinforcement are concerned simultaneously in the ranking process.
			As a result, PNR2 favors the sentences biased to the sentences that are important in the new collection and meanwhile novel to the sentences in the old collection.
			As a matter of fact, this positive and negative ranking scheme is general enough and can be used in many other situations, such as social network analysis etc.
	
	
	
