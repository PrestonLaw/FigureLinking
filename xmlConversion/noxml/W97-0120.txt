
	
		We present a self-organized method to build a stochastic Japanese word segmenter from a small number of basic words and a large amount of unsegmented training text.
		It consists of a word-based statistical language model, an initial estimation procedure, and a re-estimation procedure.
		Initial word frequencies are estimated by counting all possible longest match strings between the training text and the word list.
		The initial word list is augmented by identifying words in the training text using a heuristic rule based on character type.
		The word-based language model is then re-estimated to filter out inappropriate word hypotheses generated by the initial word identification.
		When the word segmenter is trained on 3.9M character texts and 1719 initial words, its word segmentation accuracy is 86.3% recall and 82.5% precision.
		We find that the combination of heuristic word identification and re-estimation is so effective that the initial word list need not be large.
	
	
			Word segmentation is a.n important problem for Japanese because word boundaries are not marked in its writing system.
			Other Asian languages such as Chinese and Thai have the same problem.
			Any Japanese NLP application requires word segmentation as the first stage because there are phonological and semantic units whose pronunciation and meaning is not trivially derivable from tha.t of the individual characters.
			Once word segmentation is done, all established techniques can be exploited to build practically important applications such as spelling correction [Nagata., 1996] and text retrieval [Nie a.nd Brisebois, 1996] In a sense, Japanese word segmentation is a solved problem if {and only if) we have plenty of segmented training text.
			Around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dyna.mic programing procedure [Nagata., 1994, Takeuchi and Matsumoto, 1995, Yamamoto, 1996].
			However, manually segmented corpora are not alwa.ys available in a. particular target domain and manual segmentation is very expensive.
			The goal of our research is unsupervised learning of Japanese word segmentation.
			That is, to build a Japanese word segmenter from a. list of initia.l words and unsegmented training text.
			Today, it is easy to obtain a 10K100K word list from either commercial or public domain online Japanese dictionaries.
			Gigabytes of Japanese text are readily available from newspapers, patents, HTML documents, etc..
			Few works have examined unsupervised word segmentation in Japanese.
			Both (Yamamoto, 1996] and [Takeuchi and Matsumoto, 1995] built a word-based language model from unsegmented text using are-estimation procedure whose initial segmentation was obtained by a rule-based word se1 menter.
			The utility of this approach is limited because it presupposes the existence of a rule-base word segmenter like JUMAN [Matsumoto et al., 1994].
			It is impossible to build a word segmentE for a new domain without human intervention.
			For Chinese word segmentation, more self-organized approaches have been tried.
			[Sproat et al., built a word unigram model using the Viterbi re-estimation whose initial estimates were derive from the frequencies in the corpus of the strings of each word in the lexicon.
			[Chang et al., 1991 combined a small seed segmented corpus and a large unsegmented corpus to build a word unigraJ model using the Viterbi re-estimation.
			[Luo and Roukos, 1996] proposed are-estimation procedw which alternates word segmentation and word frequency re-estimation on each half of the tra.inil:J text divided into halves.
			One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations offoreign words.
			[Chang et al., 1991 used a statistical method called "Two-Class Classifier'', which decided whether the string is actual1 a word based on the features derived from character N-gram.
			In this paper, we present a self-organized method to build a Japanese word segmenter frOJ a small number of basic words and a large amount of unsegmented training text using a noV re-estimation procedure.
			The major contribution of this paper is its treatment of unseen word: We devised a statistical word formation model for unseen words which can be re-estimated.
			" show that it is very effective to combine a heuristic initial word identification method with a r1 estimation procedure to filter out inappropriate word hypotheses.
			We also devised a new metho to estimate initial word frequencies.
			Figure 1 shows the configuration of our Japanese word segmenter.
			In the following sections, v; :first describe the statistical language model and the word segmentation algorithm.
			We then describ the initial word frequency estimation method and the initial word identification method.
			Finall: we describe the experiment results of unsupervised word segmentation under various conditions.
	
	
			2.1 Word Segmentation Model.
			Let the input Japanese character sequence be C = c1c2 ••• Cm· Our goal is to segment it int word· sequence W = w1w2 ••• Wn.
			The word segmentation task can be defined as :finding a wor segmentation W that maximizes the joint probability of word sequence given character sequenc P(WIC).
			Since the maximization is carried out with fixed character sequence C, the word segmentE only has to maximize the probability of the word sequence P(W).
			W = argmwa.xP(WIC) = argmwa.xP(W) (l We approximate the joint probability P(W) by the word unigram model, which is the product < word unigram probabilities P(Wi)· P(W) = IIP(Wi) ( i=l We used the word unigram model because of its computational efficiency.
			Word Segmentation Word Identification Segmented Text Figure 1: Block Diagram for the Self-Organizing Japanese Word Segmenter 2.2 Unknown Word Model.
			We defined a statistical word model to assign a reasonable word probability to an arbitrary substring in the input sentence.
			It is formally defined as the joint probability of the character sequence c1•..c c if Wi is a.n unknown word.
			We decompose it into the product of word length probability a.nd word spelling probability, P(wii<UNK>) = P(c1 ...c ci<UNK>) = P(k)P(c1 ...c clk) {3) where k is the length of the character sequence a.nd <UNK> represents unknown word.
			We assume that word length probability P(k) obeys a Poisson distribution whose parameter is the average word length A in the training corpus.
			This mea.ns that we regard word length as the interval between hidden word boundary markers, which are randomly placed with an average interval equal to the average word length.
			P(k) = (.A- 1)/c-1 -( -1) (k -1)!
			e We approximate the spelling probability given word length P(c1 ...c clk) by the product of character unigram probabilities regardless of word length.
			lc P(c1 ...c c) = IIP(Ci) (5) i=1 Character unigram probabilities can be estimated from unsegmented texts.
			The average word length A can be computed, once the word frequencies in the texts are obtained.
			A= L: lwiiC(wi) (G) L:C(wi) where lwil and C( Wi) a.re the length and the frequency of word Wi, respectively.
			Therefore, 1 only parameters we have to (re)estimate in the la.nguage model are the word frequencies.
			0.7 0.6 0.5 Word Length Distribution R aw Counts {all words) - Estimat es by Poisson {all words) -· Raw Counts (infrequent words) ·S··Estimates by Poisson (infrequent words) -M 0.4 j!
			e Q. 0.3 0.2 0.1 0 0 2 4 6 Word Character Length 8 10 Figure 2: Word Length Distribution of the EDR corpus Figure 2 shows the actual and estimated word length distributions in the corpus we used the experiment.
			It shows two pairs of distributions: word length of all words (.A = 1.6) and t of words appearing only once (.A= 4.8).
			The latter is expected to be close to the distribution unknown words.
			Although the estimates by Poisson distribution are not so accurate, they ena.b: us to make a. robust and computationally efficient word model.
			2.3 Viterbi Re-estimation.
			We used the Viterbi-like dynamic programing procedure described in [Nagata., 1994] to get t most likely word segmentation.
			The generalized Viterbi algorithm starts from the beginning of t input sentence, a.nd proceeds character by character.
			At each point in the sentence, it looks · the combination of the best partial word segmentation hypothesis ending a.t the point a.nd all W<l hypotheses starting at the point.
			We used the Viterbi re-estimation procedure to refine the word unigram model because its computational efficiency.
			It involves applying the above segmentation algorithm to a traini corpus, using a set of initial estimates of the word frequencies.
			The best analysis of the corpus taken to be the true analysis, the frequencies a.re re-estimated, a.nd the algorithm is repeated Ull it converges.
	
	
			3.1 Longest Match.
			We can get a set of initial estimates of the word frequencies by segmenting the training corpus using a heuristic (non-stochastic) dictionary-based word segmenter.
			In both Japanese and Chinese, one of the most popular non-stochastic dictionary-based approaches is the longest match method 1.
			There are many variations of the longest match method, possibly augmented with further heuristics.
			We used a simple greedy algorithm described in [Sproat et al., 1996].
			It starts at the beginning of the sentence, finds the longest word starting at that point, and then repeats the process starting at the next character until the end of the sentence is reached.
			We chose the greedy algorithm because it is easy to implement and guaranteed to produce only one segmentation.
			3.2 String Frequency.
			[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.
			It derives the initial estimates from the frequencies in the corpus of the strings of character making up each word in the dictionary whether or not each string is actually an instance of the word in question.
			The total number of words in the corpus is derived simply by summing the string frequency of each word in the dictionary.
			Finding (and counting) all instances of a string W in a large text T can be efficiently accomplished by making a data structure known as a suffix array, which is basically a. sorted list of all the suffixes ofT [Ma.nber and Myers, 1993].
			3.3 Longest Match String Frequency.
			The estimates of word frequencies by the above string frequency method tend to inflate a lot especially in short words, because of double counts.
			We devised a slightly improved version which we term the "longest match string frequency" method.
			It counts the instances of string W1in text T, unless the instance is also a substring of another string W2 in dictionary D. This method can be implemented by making two suffix arrays, ST and SD for text T and dictionary D. By using ST, we first make list Lw of all occurrences of string Win the text.
			By using SD, we then look up all strings Win the dictionary that include Was a substring, and make list Lw of all their occurrences in the text by using ST. The longest match string frequency of word W in text T with respect to dictionary D is obtained by counting the number of elements in the set difference LwLw For example, if the input sentence is " 11!iilli :fti;::-?\-'"Cli!-'!1.
			" (talk about the Asso­ ciation of English and the Association of Linguistics) and the dictionary has (linguistics), it mt (language),(language study),% (association), and mt (talk).
			Figure 3 shows the difference of the three methods.
			The longest match string frequency (lsf) method considers all possible longest matches in the text, while the greedy longest match (lm) algorithm considers only one possibility.
			It is obvious that the longest match string frequency method remedies the problem that the string frequency (sf) method consistently and inappropriately favors short words.
			The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it "maximum matching", we call this method "longest match" according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.
			longest match (1m-)----+- --+ 9t ?' C: :: L'"Lm o--------- string frequency (sf) longest match string frequency (lsf) II II IJ IJ II II IJ II' . Figure 3: Comparison of the initial word frequency estimation methods and in the above example, the frequency estimate of W1 becomes 0.
			Although this rarely happens for a large training text, we have to smooth the word frequencies.
	
	
			To a first approximation, a point in the text where character type changes is likely to be a word boundary.
			This is a popular heuristics in Japanese word segmentation.
			To help readers understand the heuristics, we have to give a. brief introduction to the Japanese writing system.
			In contemporary Japanese, there are at least five different types of characters other than punc­ tuation marks: kanji, hiragana, kata.kana, Roman alphabet, and Arabic numeral.
			Kanji which means 'Chinese character' is used for both Chinese origin words and Japanese words semantically equivalent to Chinese characters.
			There are two syllabaries hiragana and katakana.
			The former is used primarily for grammatical function words, such as particles and inflectional endings, while the latter is used primarily to transliterate Western origin words.
			Roman alphabet is also used for Western origin words and acronyms.
			Arabic numeral is used for numbers.
			By using just this character type heuristics, a non-stochastic and non-dictionary word segmenter can be made.
			In fact, using the estimated word frequencies obtained by the heuristics results in poor segmentation accuracy 2 • We found, however, that it is very effective to use the character type based word segmenter as a lexical acquisition tool to augment the initial word list.
			The initial word identification procedure is as follows.
			First, we segment the training corpus by the character type based word segmenter, and make a list of words with frequencies.
			We then filter out hiragana strings because they are likely to be function words.
			We add the extracted word 2The word segmentation accuracy of the c:haracter type based method was less than 60%, while other estimation methods achieves around 7080% M we show in the next section.
			list to the original dictionary witli associated frequencies, whether or not each string is actually a word.
			Although there are a lot of erroneous words in the augmented word list, most of them are filtered out by the re-estimation.
			This method works surprisingly well, as shown in the experiment.
	
	
			5.1 Language Data.
			We used the EDR Japanese Corpus Version 1.0 [EDR, 1995) to train and test the word segmenter.
			It is a corpus of 5.1 million words (208 thousand sentences).
			It contains a variety of Japanese sentences taken from newspapers, magazines, dictionaries, encyclopedias, textbooks, etc. It has a variety of annotations including word segmentation, pronunciation, and part of speech tag.
			In this experiment, we randomly selected two sets of training sentences, each consisting of 100 thousand sentences.
			The first training set (tra.ining0) is used to make initial word lists of various sizes.
			The second training set (training-1) is used to train various word segmenters.
			From the remaining of 8 thousand sentences, we randomly selected 100 test sentences to evaluate the accuracy of the word segmenters.
			Table 1 shows the number of sentences, words, and characters in the training and test sets 3• Table 1: The amount of training and test data tra ini ng0 tra .in ing 1 t e st Se nt en ce s W or d T o k e ns W or d T y p es C h ar ac te rs 1 0 0 0 0 0 2 4 6 0 1 8 8 8 5 9 6 6 3 8 9 7 7 1 8 1 0 0 0 0 0 2 4 6 5 4 4 1 8 5 9 6 7 3 9 0 6 2 6 0 1 0 0 25 38 9 1 9 39 84 Based on the frequency in the manually segmented corpus training0, we made 7 different initial word lists (D1D200) whose frequency threshold were 1, 2, 5, 10, 50, 100, 200, respectively.
			The size of the resulting word lists and their out-of-vocabulary rate (OOV rate) in the test sentences are shown in the second and third columns of Table 2.
			For example, D200 consists of words appearing more than 200 times in training0.
			Although D200 consists of only 826 words, it covers 76.6% (OOV rate 23.4%) of the test sentences.
			This is an example of the Zipf law.
			5.2 Evaluation Measures.
			Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).
			Let the number of words in the manually segmented corpus be Std, the number of words in the output of the word segmenter be Sys, and the number of matched words be M. Recall is defined as M/Std, and precision is defined as M/Sys.
			Since it is inconvenient to use both recall and precision all the we also use the F-mea.sure to indicate the overall performance.
			The F-measure was originally developed by the information 3'Irainillg-l was used as plain texts that are t.a.ken from the same information source as tra.ining-o.
			Its word segmentation information was never used to ensure that training was unsupervised.
			retrieval community.
			It is calculated by F - (.82 + 1.0} X p X R - ,B2xP+R (7) where P is precision, R is recall, and {3 is the relative importance given to recall over precision.
			We set fJ = 1.0 throughout this experiment.
			That is, we put equal importance on recall and precision.
			5.3 Comparison of Various Word Frequency Estimation Methods.
			We first compared the three frequency estimation methods described in the previous section: greedy longest match method (Im), string frequency method (sf}, and longest match string frequency method (lsf).
			The sixth, seventh, and eighth columns of Table 2 show the word segmentation accuracy (F-measure) of each estimation method using different sets of initial words (DlD200).
			For comparison, the word segmentation accuracy using real word frequency (wf}, computed from the manual segmentation of training-!(not training0!), is shown in the fifth column of Table 2.
			The results are also diagramed in Figure 4.
			Table 2: Word Segmentation Accuracies f r e q vo ca b o o v w f 1 m s f l s f l m + ct sf +c t lsf +c t D l D 2 D 5 D 1 0 D 5 0 Dl OO D 20 0 2 : : 1 2 : : 2 2 : : 5 2 :: 1 0 ;: : s o 2:: 10 0 2:: 20 0 85 96 6 39 99 4 18 68 9 10 94 1 3 1 5 9 1 7 1 9 8 2 6 0.
			01 0 0.
			01 7 0.
			03 7 0.
			06 0 0.
			13 4 0.
			18 1 0.
			23 4 0.
			8 9 3 0.
			8 9 1 0.
			8 7 7 0.
			8 5 9 0.
			7 8 5 0.
			7 5 8 0.
			72 9 0.
			8 1 0 0.
			8 1 7 0.
			81 2 0.
			7 9 7 0.
			73 4 0.
			69 9 0.
			64 4 0.
			80 1 0.
			81 5 0.
			81 4 0.
			81 3 0.
			77 4 0.
			74 9 0.
			64 3 0.
			80 7 0.
			82 2 0.
			81 9 0.
			81 5 0.
			77 6 0.
			76 1 0.
			73 1 0 . 7 9 6 0 . 8 0 8 0 . 8 1 8 0 . 8 2 8 0 . 8 3 7 0 . 8 3 9 0 . 8 2 8 0.
			78 9 0.
			80 2 0.
			81 1 0.
			82 5 0.
			83 7 0.
			84 0 0.
			83 0 0.
			7 9 4 0.
			8 1 1 0.
			8 1 6 0.
			8 2 8 0.
			8 4 1 0.
			8 4 3 0.
			8 3 2 First of all, word segmentation accuracy using real word frequencies (wf) significantly (510%) outperformed that of any frequency estimation methods.
			Among word frequency estimates, the longest match string frequency method (lsf) consistently outperformed the string frequency method (sf).
			The (longest match) string frequency method (sf and lsf) outperformed the greedy longest match method (lm) by about 25% when the initial word list size was under 20K (from D5 to D200).
			In all estimation methods, word segmentation accuracies of D1 are worse than D2, while D1 is slightly better than D2 in using real word frequencies.
			5.4 Effect of Augmenting Initial Dictionary.
			We then compared the three frequency estimation methods {lm, sf, and lsf) with the initial dic­ tionary augmented by the character type based word identification method (ct) described in the previous section.
			The word identification method collected a list of 108975 word hypotheses from training-!.
			The ninth, tenth, and eleventh columns of Table 2 show the word segmentation accu­ racies.
			Augmenting the dictionary yields a significant improvement in word segmentation accuracy.
			Although the difference between the underlying word frequency estimation methods is small, the longest match string frequency method generally performs best.
			Surprisingly, the best word segmen­ tation accuracy is achieved when the very small initial word list of 1719 words (D100) is augmented 0.9 0.85 ! ::0 :!
			Word Segmentation Accuracy D.8 f i3 :t 0.75 c =s..
			r e a l W o r d F r e q u e n c y Langest Match S t r i n g F r e q u e n c y · B · · Longest match String Frequency ..,._ E.
			(/) "E 0.7 Langest L a n g e s t M a t c h + C h a r a c t e r T y p e . . ­ S t r i n g F r e q u e n c y + C h a r a c t e r T y p e . . , . . • • match String Frequency+ CharactariYP., ·<>· 0.65 0.6 1000 10000 Vocaburary Size 100000 Figure 4: Initial word list size and word segmentation accuracies by the heuristic word identification method, where the recall and precision are 86.3% and 82.5% (F-measure 0.843).
			5.5 Effect of Re-estimatioo.
			To investigate the effect of re-estimation, we tested the combination of three initial word lists: Dl, D2, DlOO, and two initial word frequency estimation methods: string frequency method (sf) and longest match string frequency method augmented with the word identification method (lsf+ct).
			We applied the Viterbi re-estimation procedure three times.
			It seems further re-estimation brings no significant change.
			At each stage of re-estimation, we measured the word segmentation accuracy on the test sentences (not the training texts!).
			Figure 5 shows the word segmentation accuracy, the number of word tokens in the training texts, and the number of word types in the dictionary at each stage of re-estimation.
			In general, re-estimation has little impact on word segmentation accuracy.
			It gradually improves the accuracy when the initial word list is relatively large (Dl and D2), while it worsen the accuracy a little when the initial word list is relatively small (DlOO).
			This might correspond with the results on unsupervised learning performed by an English part of speech tagger.
			Although [Kupiec, 19921 presented a very sophisticated method of unsupervised learning, [Elworthy, 19941 reported that re-estimation is not always helpful.
			We think, however, our results are because we used a word unigra.m model; it is too early to conclude that re-estimation is useless for word segmentation, as discussed in the next section.
			It seems the virtue of re-estimation lies in its ability to adjust word frequencies and removing unreliable word hypotheses that are added by heuristic word identification.
			The abrupt drop in the number of word tokens at the first re-estimation step indicates that the inflated initial estimates of Word Segmentation Accura 0.85 ....--.--,--,,,..----.
			.•.•.,..............
			Number of Word Tokens 5.5e+D6 r--,,-- ,.-., Number of Word Types 180000 1 •0.84 f- 0.83 f- 1 8 0 0 0 0 \ \.
			:::> 81 0.82 CD 4.5er06\, sf1 -+ 140000 f- \ \E -----+-----r---- \ sf2 -+--· ! .
			...., _ !b. 0.81 ..........·-·-·•-·-·-: ....
			:::> 0.8 c"' '6 4er06 • sf100 ·B··· • lsf1+Ct -M­ Isf2+ct .,.._ p 100+ct ....:.., 120000 r:........
			-* '·.= ·-·-··-·-·-·-· .,., 100000 '- sf1 -+-_ "' ' sf2 -+--·lsf1+Ct -l< .!ic 0.79 '6 - a; sf100 ·8· 80000 f-2+Ct ........
			"E sf1 1l3.5e+06 l ..c -100+Ct ......
			CD sf2 -+--· E .\ sf100 ·8-- \ E ::>60000 f- e"CnD"lsi 1+Ct ··l<- r- lsf2+Ct .......
			'l'\.
			z "E 0.77 lsf100+d .....0.76 10.75 11:: ••••-El-··-··B··-·- 0.74 ' ' 0 1 2 3 Number of Reestimation 3e+06 f-\ \ 'e-····B·····i \ '· i\ \ 2.5er06 1- \ --·-·•-·-·v-----+--- l:-· :=.
			2er06 ' 0 1 2 3 Number of Reestimation40000 r----+---+--- Ia-.
			20000 1- / ···13-·-··• 0 .L 0 1 2 3 Number of Reestimation Figure 5: Word segmentation accuracy, the number of word tokens and word types at each re­ estima.tion sta.ge word frequencies a.re adjusted to more reasonable values.
			The drop in the number of word types indicates the removal of infrequent words a.nd unreliable word hypotheses from the dictionary.
	
	
			6.1 The Nature of the Word Unigram Model.
			First, we will clarify the nature of the word unigra.m model.
			ROughly speaking, word Ulligram based word segmenters maximize the product of the word frequencies under the fewest word principle which subsumes the longest match principle.
			If two word segmentation hypotheses differs in the number of words, the one with fewer words is almost always selected.
			For example, the input string is c1c2 and the dictionary includes three words C1C2, c1, c2• To prefer segmentation hypothesis c11c2 over c1c2, the following relation must hold.
			(8) where C(·) represents the word frequency and N is the number of word tokens in the training text.
			Suppose N is one million.
			Even if C(c1c2) = 1; c1c2 is preferred unless c1 and c2 are highly frequent, say C(c1) C(c2) > 1000.
			It is obvious that the segmentatiOn with fewer words are preferred.
			If two word segmentation hypotheses have the same number of words, the one with larger product of word frequencies is selected.
			For example, the input string is C1C2C3 and the dictionary includes four words c1c2, ca, c1, c2ca.
			To prefer segmentation hypothesis c1c2lca over c1lc2ca, the following relation must hold.
			C(c1c2) C(cs) C(c1) C(c2ca) N N < N N (9) Since the denominator N is cancelled, it is obvious that the segmentation with larger product of frequencies is preferred.
			6.2 Classification of Segmentation Errors.
			There are three major types of segmentation errors.
			The first type is not an error but the ambiguity resulting from inconsistent manual segmentation, or the intrinsic indeterminacy of Japanese word segmentation.
			For example, in the manually segmented corpus, we found the string OO.A. Ifl1!i'" (foreign laborer) is identified as one word in some places while in others it is divided into two words OO.A.
			(foreigner) and lfl1!i'" (laborer).
			However, the word unigram based segmenter consistently identifies it as a single word.
			We assume 35 % of the segmentation "errors" belong to this type.
			The second type is breakdown of unknown words.
			For example, the word k:P (funny) is segmented into two word hypotheses (rare) and 1!!P (strange).
			This is because 1!!P is included in the dictionary.
			When a substring of an unknown word coincides with other word in the dictionary, it is very likely to be broken down into the dictionary word and the remaining substring.
			This is a major flaw of our word model using character unigram.
			It assigns too little probability to longer word hypotheses, especially more than three characters.
			The third type is erroneous longest match.
			This happens frequently at the sequence of gram-_ matical function words written in hiragana.
			For example, the phrase -;; (gather) I -:> (INFL) j-c (and) I (come) I t::.
			(past-AUXV), which means "came and gathered", is segmented into** I -c (TOPIC) I t (north), because the number of words is fewer.
			The larger the initial word list is, the more often a hiragana word happens to coincide with a sequence of other hiragana words, because the number of character types in hirago.na is small ( < 100).
			This is the major reason why word segmentation accuracy levels off or decreases at a certain point, as the size of the initial word list increases.
			6.3 Classification of tbe Effects of Re-estimation.
			There are two types of major changes in segmentation with re-estimation: word boundary adjust­ ment and subdivision.
			The former moves a word boundary keeping the number of words unchanged.
			The latter break down a word into two or more words.
			Re-estimation usually improves a sequence of grammatical function words written in hiragana at the sentence final predicate phrase if the initial segmentation and the correct segmentation have the same number of words.
			For example, the incorrect initial segmentation :il:tt.
			(take away) I :tt.
			(INFL + passive-AUXV) I t -;; (ball) I -;;t!
			(not yet) is correctly adjusted to :il:tt.
			(take away) I h (INFL + passive-AUXV) I t (past-AUXV) I -;;-;; (still) I tt.
			(COPULA), which means "still be taken away".
			Re-estimation subdivides an erroneous longest match if the frequencies of the shorter words are significantly large.
			For example, the incorrect initial segmentation .:t (restrain) 1 t::.\-' (sea bream) is correctly subdivided into *.
			(restrain)I t:.
			(want-AUXV) I lt' (INFL), which means "want to restrain".
			One of the most frequent undesirable effects of re-estimation is subdividing an infrequent word into highly frequent words, or a frequent word and an unknown word.
			For example, the correct infrequent word li (ambassador) is subdivided into two frequent words, (use-ROOT) and ill (node).
			As we sa.id before, one of the major virtues of re-estimation is its ability to remove inappropriate word hypotheses generated by the initial word identification procedure.
			For example, from the phrase >'li (Soviet Union) I (made-SUFFIX) I Tetllr (tank), which means "Soviet Union-made tank", the initial word identifier extracts two word hypotheses >' and retllr, where the former is written in katakana and the latter is written in kanji.
			H >'it and !lll!
			is in the dictionary, the two erroneous word hypotheses Y and lll!!lll!TStlli are removed and the correct word JAl]J is added to the dictionary after re-estimation.
			1 Conclusion and Future Work.
			We have presented a self-organized method that builds a stochastic Japanese word segmenter from a small word list and a large unsegmented text.
			We found that it is very effective to augment the initial word list with automatically extracted words using character type heuristics.
			Re-estimation helps in adjusting word frequencies and removing inappropriate word hypotheses, although it has little impact on word segmentation accuracy if the word unigram model is used.
			The major drawbacks of the current word segmenter is its breakdown of unknown words whose substrings coincide with other words in the dictionary, and the erroneous longest match at the sequence of functional words written in hiragana.
			The first drawback results from the character unigram based word model that prefers short words, while the second drawback results from the nature of the word unigram model which prefers fewest words segmentation.
			One may argue that we could use the word bigram model.
			However, we don't know how we can estimate the initial word bigram frequencies from scratch.
			One may also argue that we could use the character bigram in the word model.
			However, the character bigram for the word model must be computed from segmented texts.
			Both of these suggest that we need a word segmenter to build a more sophisticated word segmenter.
			Therefore, as a next step of our research, we are thinking of using the proposed unigram based word segmenter to obtain the initial estimates of the word bigrams a.nd the word-based character bigrams which will then be refined by a re-estimation procedure.
	
