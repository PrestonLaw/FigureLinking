
	
	
			We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (CallisonBurch et al., 2012).
			The notable features of these systems are: • Moses phrase-based models with mostly de­ fault settings • training on all available parallel data, includ­ ing the large UN parallel data, the French­ English 109 parallel data and the LDC Giga­ worddata • very large tuning set consisting of the test sets from 20082010, with a total of 7,567 sen­ tences per language • GermanEnglish with syntactic pre­ reordering (Collins et al., 2005), compound splitting (Koehn and Knight, 2003) and use of factored representation for a POS target sequence model (Koehn and Hoang, 2007) • EnglishGerman with morphological target sequence model Note that while our final 2012 systems in­ cluded subsarnpling of training data with modified MooreLewis filtering (Axelrod et al., 2011), we did not use such filtering at the slatting point of our development.
			We will report on such filtering in Section 2.
			Moreover, our system development initially used the WMT 2012 data condition, since it took place throughout 2012, and we switched to WMT 2013 traitting data at a later stage.
			In this sec­ tion, we report cased BLEU scores (Papineni et al., 2001) on newstest20ll.
			1.1 Factored Backoff (GermanEnglish).
			We have consistently used factored models in past WMT systems for the GermanEnglish language pairs to include POS and morphological target se­ quence models.
			But we did not use the factored decomposition of translation options into multi­ ple mapping steps, since this usually lead to much slower systems with usually worse results.
			A good place, however, for factored decompo­ sition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a).
			Here, we used ouly factored backoff for unknown words, giving gains in BLEU of +.12 for GermanEnglish.
			1.2 Tuning with k-best MIRA.
			In preparation for training with sparse features, we moved away from MERT which is known to fall 114 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114121, Sofia, Bulgaria, August 89, 2013 @2013 Association for Computational Unguistics apart with many more than a couple of dozen fea­ tures.
			Instead, we used k-best MIRA (Cherry and Foster, 2012).
			For the different language pairs, we saw improvements in BLEU of -.05 to +.39, with an average of +.09.
			There was only a minimal change in the length ratio (Table 1) The lexical features were restricted to the 50 most frequent words.
			All these features together only gave minor improvements (Table 3).
			Table 1:Thning with k-best MIRA instead of MERT (cased BLEU scores with length ratio) 1.3 Translation Table Smoothing with.
			KneserNey Discounting Previously, we smoothed counts for the phrasal conditional probability distributions in the trans­ lation model with Good Turing discounting.
			We explored the use of KneserNey discounting, but resnlts are mixed (no difference on average, see Table 2), so we did not pursue this further.
			Table 3: Sparse features We also explored domain features in the sparse feature framework, in three different variations.
			Assume that we have three domains, and a phrase pair occurs in domain A 15 times, in domain B 5 times, and in domain C never.
			We compute three types of domain features: • binary indicator, if phrase-pairs occurs in do­ main (example: indA = 1, indB = 1, indo = 0) • ratio how frequent the phrase pairs occurs in domain (example: ratioA = 1i!s = .75, ratioB = 15 5 = .25, ratioc = 0) • subset of domains in which phrase pair oc­ curs (example: subsetAB = 1, other subsets 0) We tested all three feature types, and found the biggest gain with the domain indicator feature (+.11, Table 4).
			Note that we define as domain the different corpora (Europarl, etc.).
			The number of domains ranges from 2 to 9 (see column #d).1 Table 2: Translation model smoothing with KneserNey 1.4 Sparse Features.
			A significant extension of the Moses system over the last couple of years was the support for large numbers of sparse features.
			This year, we tested this capability on our big WMT systems.
			First, we used features proposed by Chiang et al.
			(2009): • phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) • target word insertion features • source word deletion features • word translation features • phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).
			We use the domain indicator fea­ ture and the other sparse features in subsequent ex­ periments.
			1In the final experiments on the 2013 data condition, one domain (commoncrawl) was added for all language pairs.
			bas elin e in di ca to r r a t i o s u b s e t de en fr e n es en cs en en de enfr en es en cs 22 .1 0 30 .1 1 30 .6 3 25 .4 9 1 6.
			1 2 29 .6 5 31 .9 5 1 7.
			4 2 22.
			18 +.0 8 30.
			41 +.3 0 30.
			75 +.1 2 25.
			56 +.0 7 15.
			95 .17 29.
			96 +.3 1 32.
			12 +.1 7 17.
			38 .04 22.
			10 ±.0 0 30.
			49 +.3 8 30.
			56 .07 25.
			63 +.1 4 15 .96 .1 6 29.
			88 +.2 3 32.
			16 +.2 1 17 .35 .0 7 22.
			16 +.0 6 30.
			36 +.2 5 30.
			85 +.2 2 25.43 .06 16.
			05 .07 29.
			92 +.2 7 32.
			08 +.2 3 17.
			40 .02 avg . + . 1 1 + . 0 9 + . 1 1 Table 5: Combining domain and other sparse features 1.5 Thning Settings.
			Given the opportunity to explore the parameter tuning of models with sparse features across many language pairs, we investigated a number of set­ tings.
			We expect tuning to work better with more iterations, longer n-best lists and bigger cube prun­ ing pop limits.
			Our baseline settings are 10 itera­ tions with 100-best lists (accumulating) and a pop limit ofI000 for tuning and 5000 for testing.
			Table 7: Maximum phrase length, reduced from baseline 1.7 Unpruned Language Models.
			Previously, we trained 5-gram language models using the default settings of the SRILM toolkit in terms of singleton pruning.
			Thus, training throws out all singletons n-grams of order 3 and higher.
			We explored whether unpruned language models could give better performance, even if we are ouly able to train 4-gram models due to memory con­ straints.
			At the time, we were not able to build un­ pruned 4-gram language models for English, but for the other language pairs we did see improve­ ments of -.07 to +.13 (Table 8).
			We adopted such models for these language pairs.
			5g pru ne d 4gu npr une den fr en es en cs 2 9 . 8 9 3 2 . 2 7 1 7 . 4 1 2 9 . 8 3 3 2 . 3 4 1 7 . 5 4 .0 7 +.0 7 +.1 3 Table 8: Language models without singleton pruning Table 6: Tuning settings (number of iterations, size of n-best list, and cube pruning pup limit) Results support running tuning for 25 iterations but we see no gains for 5000 pops.
			There is ev­ idence that ann-best list size of 1000 is better in tuning but we did not adopt this since these large lists take up a lot of disk space and slow down the MIRA optimization step (Table 6).
			1.6 Smaller Phrases.
			Given the very large corpus sizes (up to a billion words of parallel data for French-Englisb), the size of translation model and lexicalized reorder­ ing model becomes a challenge.
			Hence, we want to examine if restriction to smaller phrases is fea­ sible without loss in translation quality.
			Results in Table 7 suggest that a maximum phrase length of 5 gives almost identical results, and ouly with a phrase length limit of 4 significant losses occur.
			We adopted the limit of 5.
			1.8 Translations per Input Phrase.
			Finally, we explored one more parameter: the limit on how many translation options are considered per input phrase.
			The default for this setting is 20.
			However, our experiments (Table 9) sbow that we can get better results with a translation table limit of I00, so we adopted this.
			ttl 20 ttl 30 ttl 50 ttll OO de enfr en es en cs en en de enfr en es en cs 21.
			05 30.
			39 30.
			86 25.
			53 15.
			97 29.
			83 32.
			34 17.
			54 +.0 6 .0 2 ±.0 0 +.2 4 +.0 3 +.1 4 +.0 8 .0 5 +.0 9 +.0 5 .0 3 +.1 3 +.0 7 +.1 9 +.1 0 .0 2 +.
			01 +.
			07 .0 7 +.
			20 +.
			11 +.
			13 +.
			07 +.
			01 avg +.0 6 +.0 7 +.
			07 Table 9: Maximal number translations per input phrase 1.9 Other Experiments.
			We explored a number of other settings and fea­ tures, but did not observe any gains.
			• Using HMM alignment instead of IBM Model4 leads to losses of -.01 to -.27.
			• An earlier check of modified MooreLewis filtering (see also below in Section 3) gave very inconsistent results.
			• Filtering the phrase table with significance filtering (Johnson eta!., 2007) leads to losses of -.19 to -.63.
			• Throwing out phrase pairs with direct transla­ tion probability cf>( elf) of less than w-5 has almost no effect.
			• Double-checking the contribution of the sparse lexical features in the final setup, we observe an average losses of -.07 when drop­ ping these features.
			• For the GermanEnglish language pairs we saw some benefits to using sparse lexical fea­ tures over POS tags instead of words, so we used this in the final system.
			1.10 Summary.
			We adopted a number of changes that improved our baseline system by an average of +.30, see Ta­ ble 10 for a breakdown.
			avg.
			method +.01 factored backoff +.09 kbest MIRA +.11 sparse features and domain indicator +.03 tuning with 25 iterations -.03 maximum phrase length 5 +.02 unpruned4gramLM +.07 translation table limit 100 +.30 total Table 10: Summary of impact of clumges Minor improvements that we did not adopt was avoiding reducing maximum phrase length to 5 (average +.03) and turting with 1000-best lists (+.02).
			The improvements differed significantly by lan­ guage pair, as detailed in Table 11, with the biggest gains for English-French (+.70), no gain for EnglishGerman and no gain for English­ German.
			1.11 New Data.
			The final experiment of the initial system devel­ opment phase was to train the systems on the new data, adding newstest2011 to the turting set (now 10,068 sentences).
			Table 12 reports the gains on newstest2012 due to added data, indicating very clearly that valuable new data resources became available this year.
			bas elin e imp rov ed L J .de enfr en es en cs en en de enfr en es en cs 2 1.
			9 9 3 0.
			0 0 3 0.
			4 2 2 5.
			5 4 1 6.
			0 8 2 9.
			2 6 3 1.
			9 2 1 7.
			3 8 2 2.
			0 9 3 0 . 4 6 3 0 . 7 9 2 5.
			7 3 1 6 . 0 8 2 9.
			9 6 3 2 . 4 1 1 7 . 5 5 +.1 0 +.4 6 +.3 7 +.1 9 ±.0 0 +.7 0 +.4 9 +.1 7 Table 11: Overall improvements per language pair W M T2 01 2 W M T2 01 3 L J .de enfr en es en cs en ru en en de enfr en -es en -cs enru 2 3 . 1 1 2 9 . 2 5 3 2 . 8 0 2 2 . 5 3 1 6 . 7 8 2 7 . 9 2 3 3 . 4 1 1 5 . 5 1 2 4 . 0 1 3 0 . 7 7 3 3 . 9 9 2 2 . 8 6 3 1 . 6 7 1 7 . 9 5 2 8 . 7 6 3 4 . 0 0 1 5 . 7 8 2 3 . 7 8 +0.
			90 +1.
			52 +1.
			19 +0.
			33 +1.
			17 +0.
			84 +0.
			59 +0.
			27 Table 12: Training with new data (newstest2012 scores)
	
	
			We explored two additional domain adaptation techniques: phrase table interpolation and modi­ fied MooreLewis filtering.
			2.1 Phrase Table Interpolation.
			We experimented with phrase-table interpolation using perplexity minimisation (Foster eta!., 2010; Sennrich, 2012).
			1n particular, we used the im­ plementation released with Sennrich (2012) and available in Moses, comparing both the naive and modified interpolation methods from that paper.
			For each language pair, we took the alignments created from all the data concatenated, built sepa­ rate phrase tables from each of the individual cor­ para, and interpolated using each method.
			The re­ sults are shown in Table 13 bas elin e n a i v e m od ifi ed f r e n es en • cs en "' ru en e n f r e n e s e n c s e n r u 3 0.
			7 7 3 3.
			9 8 2 3.
			1 9 3 1.
			6 7 28 .7 6 3 4.
			0 0 1 5.
			7 8 23 .7 8 30.
			63 .14 33.
			83 .15 22.77 .42 31.
			42 .25 28.
			88 +.1 2 34.
			07 +.0 7 15.
			88 +.1 0 23.
			84 +.0 6 34.
			03 +.0 5 23.03 .17 31.59 .08 34.
			31 +.3 1 15.
			87 +.0 9 23.68 .10 Table 13: Comparison of phrase-table interpolation (two methods) with baseline (on newstest2012).
			The baselines are as Table 12 except for the starred rows where tuning with PRO was found to be better.
			The modified interpolation was not possible in fr++en as it uses to much RAM.
			The results from the phrase-table interpolation are quite mixed, and we ouly used the technique for the final system in en-es.
			An interpolation based on PRO has recently been shown (Haddow, 2013) to improve on perplexity minimisation is some cases, but the current implementation of this method is limited to 2 phrase-tables, so we did not use it in this evaluation.
			2.2 Modified MooreLewis Filtering.
			In last year's evaluation (Koehn and Haddow, 2012b) we had some success with modified MooreLewis filtering (Moore and Lewis, 2010; Axelrod et al., 2011) of the training data.
			This year we conducted experiments in most of the lan­ guage pairs using MML filtering, and also exper­ imented using instance weighting (Mansour and Ney, 2012) using the (exponential of) the MML weights.
			The results are show in Table 14 ba se lin e M M L 2 0 % Tn st. W t Tn st. W t (s c al e ) f r e n es en * cs en * r u e n e n f r e n e s e n c s e n r u 30.
			77 33.
			98 23.
			19 31.
			67 28.
			67 34.
			00 15.
			78 23.
			78 34.
			26 +.2 8 22 .62 .5 7 31 .58 .0 9 28.
			74 +.0 7 34.
			07 +.0 7 15 .37 .4 1 22 .90 .8 8 33.85 .13 23.17 .02 31.57 .10 28.
			81 +.1 7 34.
			27 +.2 7 15.
			87 +.0 9 23.
			82 +.0 5 33.
			98 ±.0 0 23.13 .06 31.62 .05 28.63 .04 34.
			03 +.0 3 15.
			89 +.I I 23.72 .06 Table 14: Comparison of MML filtering and weighting with baseline.
			The MML uses monolingual news as in-domain, and selects from all training data after alignment.The weight­ ing uses the MML weights, optionally downscaled by 10, then exponentiated.
			Baselines are as Table 13.
			As with phrase-table interpolation, MML filter­ ing and weighting shows a very mixed picture, and not the consistent improvements these techniques offer on IWSLT data.
			In the final systems, we used MML filtering only for es-en.
	
	
			We enhanced the phrase segmentation and re­ ordering mechanism by integrating OSM: an op­ eration sequence N-gram-based translation and re­ ordering model (Durrani et al., 2011) into the Moses phrase-based decoder.
			The model is based on minimal translation units (MTUs) and Markov chains over sequences of operations.
			An opera­ tion can be (a) to jointly generate a bi-language MTU, composed from sou rce and target words, or (b) to perform reordering by inserting gaps and do­ ing jumps.
			Model: Given a bilingual sentence pair < F, E > and its alignment A, we transform it to If h g aycht z/Jn hars I do o tfthhotse Figure 1: Bilingual Sentence with Alignments sequence of operations (01, 02 , . . .
			, OJ) and learn a Markov model over this sequence as: JPosm(F, E,A) = p(o{) = IJ p(ojlOj-n+l, ...,Oj 1) j = l By coupling reordering with lexical generation, each (translation or reordering) decision condi­ tions on n - 1 previous (translation and reorder­ ing) decisions spann ing across phrasal boundaries thus overcoming the problematic phrasal indepen­ dence assumption in the phrase-based model.
			In the OSM model, the reordering decisions influ­ ence lexical selection and vice versa.
			Lexical gen­ eration is strongly coupled with reordering thus improving the overall reordering mechanism.
			We used the modified version of the OSM model (Durrani et al., 2013b) that addition­ ally handles discontinuous and unaligned target MTUs3.
			We borrow 4 count based supportive fea­ tures, the Gap, Open Gap, Gap-width and Dele­ tion penalties from Durrani eta!.
			(2011).
			Training: During training, each bilingual sen­ tence pair is deterministically converted to a unique sequence of operations.
			Please refer to Durrani et al.
			(2011) for a list of operations and the conversion algorithm and see Figure 1 and Ta­ ble 15 for a sample bilingual sentence pair and its step-wise conversion into a sequence of oper­ ation.
			A 9-gram KneserNey smoothed operation sequence model is trained with SRILM.
			Search: Although the OSM model is based on minimal uni ts, phrase-based search on top of OSM model was found to be superior to the MTU-based decoding in Durrani et al.
			(2013a).
			Following this framework allows us to use OSM model in tandem with phrase-based models.
			We integrated the gen­ erative story of the OSM model into the hypothe­ sis extension of the phrase-based Moses decoder.
			Please refer to (Durrani et al., 2013b) for details.
			Results: Table 16 shows case-sensitive BLEU scores on newstest2012 and newstest2013 for fi 3 In the original OSM model these are removed from the alignments through a post-processing heuristic which hurt.s in some language pairs.
			See Durrani et al.
			(2013b) for deta1led experiments.
			393m 3,775m 17,629m 39,919m 59,794m Table 17: Counts of unique n-grams (m for millions) for the 5 orders in the unconstrained language model Table 15: Step-Wise Generation ofFtgore I The large language model was 1hen quantized to 10 bits and compressed to 643 GB wi1h KenLM (Heafield, 2011), loaded onto a machine wi1h 1 TB RAM, and used as an additional feature in unconstrained French-English, SpanishEnglish, and CzechEnglish submissions.
			This additional language model is 1he ouly difference between our final constrained and unconstrained submissions; no additional parallel data was used.
			Results are shown in Table 18.
			Improvement from large lan­ guage models is not a new result (Brants et al., 2007); 1he primary contribution is estimating on a single machine.
			Co nst rai ned Un con stra ine d f ) . fr e n es en cs en ru eo 3 1 . 4 6 3 0 . 5 9 2 7 . 3 8 2 4 . 3 3 3 2 . 2 4 3 1 . 3 7 2 8 . 1 6 2 5 . 1 4 +.7 8 +.7 8 +.7 8 +.8 1 Table 16: Results using the OSM Feature nal systems from Section 1 and 1hese systems aug­ mented wi1h 1he operation sequence model.
			The model gives gains for all language pairs (BLEU +.09 to +.90, average +.37, on newstest2013).
	
	
			To overcome 1he memory limitations of SRILM, we implemented modified KneserNey (Kneser and Ney, 1995; Chen and Goodman, 1998) smoothlng from scratch using disk-based stream­ ing algori1hms.
			This open-source4 tool is de­ scribed fully by Heafield et al.
			(2013).
			We used it to estimate an unpruned 5-gram language model on web pages from ClueWeb09.5 The corpus was preprocessed by removing spam (Cormack et al., 2011), selecting English documents, splitting sen­ tences, deduplicating, tokenizing, and truecasing.
			Estimation on 1he remaining 126 billion tokens took 2.8 days on a single machine wi1h 140 GB RAM (of which 123GB was used at peak) and sbr.
			hard drives in a RAIDS configuration.
			Statistics about 1he resulting model are shown in Table 17.
			4http://kheafield.com/code/ 5http://lemurproject.org/clueweb&9/ Table 18: Gain on newstest2013 from the unconstrained lan­ guage model.
			Our time on shared machines with 1 TB is limited so RussianEnglish was run after the deadline and GermanEnglish was not ready in time.
	
	
			Table 19 breaks down 1he gains over 1he final sys­ tem from Section 1 from using 1he operation se­ quence models (OSM), modified MooreLewis fil­ tering (MML), fixing a bug wi1h 1he sparse lex­ ical features (Sparse-Lex Bugfix), and instance weighting (Instance Wt.), translation model com­ bination (TM-Combine), and use of 1he huge lan­ guage model (ClueWeb09 LM).
	
	
			Thanks to Miles Osborne for preprocessing the ClueWeb09 corpus.
			The research leading to these results has re­ ceived funding from the European Union Seventh Framework Programme (FP7n0072013) under graot agreement 287658 (EU BRIDGE) aod grant agreement 288487(MosesCore).This work made use of the resources pruvided by the Edioburgh Compure aod Data Facility'.
			The ECDF is partilllly supported by the eDIKT initia­ tive7.
			This work also used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number OCI1053575.
			Specifically, Stampede was used under allocation TGCCR110017.
			6http://www.ecdf.ed.ac.uk/ 7http://www.edikt.org.uk/ System 1 2012 2013 SpanishEnglish BnglishSpamsh CzechEnglish
	

