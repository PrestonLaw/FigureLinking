
	
		We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009).
		This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model.
		We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser.
		For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices.
		We demonstrate performance improvements for ChineseEnglish and UrduEnglish translation over a phrase-based baseline.
		We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.
	
	
			Two approaches currently dominate statistical machine translation (MT) research.
			Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations.
			Models that employ syntax or syntax- like representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability.
			As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007).
			In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency syntax.
			Instead of standard dependency trees in which words are vertices, our trees have phrases as vertices.
			We describe a simple heuristic to extract phrase dependencies from an aligned parallel corpus parsed on the target side, and use them to compute target-side tree features.
			We define additional string-to-tree features and, if a source-side dependency parser is available, tree-to-tree features to capture properties of how phrase dependencies interact with reordering.
			To leverage standard phrase-based features alongside our novel features, we require a formalism that supports flexible feature combination and efficient decoding.
			Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).
			The decoder involves generating a phrase lattice (Ueffing et al., 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice.
			This approach allows us to feasibly explore the combined search space of segmentations, phrase alignments, and target phrase dependency trees.
			Our experiments demonstrate an average improvement of +0.65 BLEU in ChineseEnglish translation across three test sets and an improvement of +0.75 BLEU in UrduEnglish translation over a phrase-based baseline.
			We also describe experiments in which we replace supervised dependency parsers with unsupervised parsers, reporting promising results: using a supervised Chinese parser and a state-of-the-art unsupervised English parser provides our best results, giving an averaged gain of +0.79 BLEU over the baseline.
			We also discuss how our model improves translation quality and discuss future possibilities for combining approaches to ma 474 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 474–485, Edinburgh, Scotland, UK, July 27–31, 2011.
			Qc 2011 Association for Computational Linguistics chine translation using our framework.
	
	
			We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.
			Here we generalize that model to function on phrases, enabling a tighter coupling be
	
	
			Given a sentence s and its dependency tree τs, we formulate the translation problem as finding the target sentence t∗, the segmentation γ∗ of s into phrases, the segmentation φ∗ of t∗ into phrases, the dependency tree τ ∗ on the target phrases φ∗, and the one-to-one phrase alignment a∗ such that tween the phrase segmentation and syntactic structures.
			We also present a decoder efficient enough toscale to large data sets and present performance im (t∗, γ∗, φ∗, τ ∗ , a∗) = argmax (t,γ,φ,τφ ,a) p(t, γ, φ, τφ, a | s, τs) provements in large-scale experiments over a state- of-the-art phrase-based baseline.
			Aside from QG, there have been many efforts to use dependency syntax in machine translation.
			Quirk et al.
			(2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs.
			Shen et al.
			(2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model.
			More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage.
			Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right decoding procedure of phrase-based translation could be retained while a dependency language model is incorporated.
			Carreras and Collins (2009) presented a string-to- dependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table.
			We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model.
			While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score translations (DeNero et al., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1 Dymetman and Cancedda (2010) present a formal analy-.
			We use a linear model (Och and Ney, 2002): p(t, γ, φ, τφ, a | s, τs) ∝ exp{θTg(s, τs, t, γ, φ, τφ, a)} where g is a vector of arbitrary feature functions on the full set of structures and θ holds corresponding feature weights.
			Table 1 summarizes our notation.
			In modeling p(t, γ, φ, τφ, a | s, τs), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006).
			Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree.
			We denote this grammar by Gs,τs ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).
			We previously presented a word-based machine translation model based on a quasi-synchronous dependency grammar.
			However, it is well-known in the MT community that translation quality is improved when larger units are modeled.
			Therefore, we use a dependency grammar in which the leaves are phrases rather than words.
			We define a phrase dependency grammar as a model p(φ, τφ|t) over the joint space of segmen tations of a sentence into phrases and dependency trees on the phrases.2 Phrase dependency grammars sis of the problem of intersecting phrase-based and hierarchical translation models, but do not provide experimental results.
			2 We restrict our attention to projective trees in this paper,.
			but the generalization to non-projective trees is easily made.
			s = (s1 , . . .
			, sn ) so urc e lan gu ag e se nt en ce t = (t1 , . . .
			, tm ) tar get lan gu ag e se nte nc e, tra nsl ati on of s γ = (γ 1 , . . .
			, γn ) ∀ i, γ i = ( s j , . . .
			, s k ) s. t. γ 1 · . . .
			· γ n = s se gm ent ati on of s int o phr as es φ = (φ 1 , . . .
			, φm ) ∀ i, φ i = ( tj , . . .
			, t k ) s. t. φ 1 · . . .
			· φ m = t se gm ent ati on of t int o phr as es τs : {1, . . .
			, n} → {0, . . .
			, n} d e p e n d e nc y tr e e o n s o ur c e w or d s s, w h er e τs (i ) is th e in d e x of th e p ar e nt of w or d si (0 is th e ro ot , $) τφ : {1, . . .
			, ml } → {0, . . .
			, ml } d e p e n d e nc y tr e e o n ta rg et p hr as es φ , w h er e τφ (i ) is th e in d e x of th e p ar e nt of p hr as e φi a : {1, . . .
			, ml } → {1, . . .
			, nl } oneto on e ali gn me nt fro m phr as es in φ to phr as es in γ θ = (λ, ψ) par am ete rs of the full mo del (λ = phr ase ba se d, ψ = Q P D G) Table 1: Key notation.
			have recently been used by Wu et al.
			(2009) for feature extraction for opinion mining.
			When used for translation modeling, they allow us to capture phenomena like local reordering and idiomatic translations within each phrase as well as long-distance relationships among the phrases in a sentence.
			We then define a quasi-synchronous phrase dependency grammar (QPDG) as a conditionalmodel p(t, γ, φ, τφ, a | s, τs) that induces a prob abilistic monolingual phrase dependency grammar over sentences inspired by the source sentence and (lexical) dependency tree.
			The source and target sentences are segmented into phrases and the phrases are aligned in a one-to-one alignment.
			We note that we actually depart here slightly from the original definition of QG.
			The alignment variable in QG links target tree nodes to source tree nodes.
			However, we never commit to a source phrase dependency tree, instead using a source lexical dependency tree output by a dependency parser, so our alignment variable a is a function from target tree nodes (phrases in φ) to source phrases in γ, which might not be source tree nodes.
			The features in our model may consider a large number of source phrase dependency trees as long as they are consistent with τs.
	
	
			Our model contains all of the standard phrase-based features found in systems like Moses (Koehn et al., 2007), including four phrase table probability features, a phrase penalty feature, an n-gram language model, a distortion cost, six lexicalized reordering features, and a word penalty feature.
			We now describe in detail the additional features $ ← said : $ ← we should $ ← said that $ ← has been $ ← is a - us → relations $ ← will be $ ← he said $ ← it is cross - strait → relations $ ← this is $ ← pointed out that $ ← we must , and → is the → united states the chinese → government the → development of $ ← is the the two → countries $ ← said , he → said : one - china → principle $ ← he said : sino - us → relations Table 2: Most frequent phrase dependencies with at least 2 words in one of the phrases (dependencies in which one phrase is entirely punctuation are not shown).
			$ indicates the root of the tree.
			in our model that are used to score phrase dependency trees.
			We shall refer to these as QPDG features and will find it useful later to notation- ally distinguish their feature weights from those of the phrase-based model.
			We use λ for weights of the standard phrase-based model features and ψ for weights of the QPDG features.
			We include three categories of features, differentiated by what pieces of structure they consider.
			4.1 Target Tree Features.
			We first include features that only consider t, φ, and τφ.
			These features can be categorized as “syntactic language model” features (Shen et al., 2008; Galley and Manning, 2009), though unlike previous work our features model both the phrase segmentation and dependency structure.
			Typically, these sorts of features are probabilities estimated from a corpus parsed using a supervised parser.
			However, there do not currently exist treebanks with annotated phrase , → m a d e u p h e → m a d e u p s u p r e m e c o u r t → m a d e u p c o u r t → m a d e u p in s e pt e m b er 2 0 0 0 → m a d e u p in se pte mb er 20 00 , → ma de up 0.
			0 5 7 0.
			0 2 1 0.
			0 1 4 0.
			0 1 4 0.
			0 1 4 0.
			0 1 4 m a d e u p ← o f m a d e u p ← . m a d e u p ← , m a d e u p ← m i n d t o 0.
			0 6 5 0.
			0 2 9 0.
			0 1 6 0 . 0 1 Table 3: Most probable child phrases for the parent phrase “made up” for each direction, sorted by the conditional probability of the child phrase given the parent phrase and direction.
			phrase dependencies of the form (u, v, d), where u is the head phrase, v is the child phrase, and d ∈ {left , right } is the direction, we then estimate conditional probabilities p(v|u, d) using relative fre quency estimation.
			Table 3 shows the most probable child phrases for an example parent phrase.
			To combat data sparseness, we perform the same procedure with each word replaced by its word cluster ID obtained from Brown clustering (Brown et al., 1992).
			We include a feature in the model for the sum of the scaled log-probabilities of each attachment: m dependency trees.
			) max (0, C + log p(φi|φτ i=1 (i), d(i)l (1) Our solution is to use a standard supervised dependency parser and extract phrase dependencies using bilingual information.3 We begin by obtaining symmetrized word alignments and extracting phrase pairs using the standard heuristic from phrase-based MT (Koehn et al., 2003).
			Given the set of extracted phrase pairs for a sentence, denote by W the set of unique target-side phrases among them.
			We parse the target sentence with a dependency parser and, for each pair of phrases u, v ∈ W , we extract a phrase dependency (along with its direction) if u and v do not overlap and there is at least one lexical dependency between a word in u and a word in v. If there are lexical dependencies in both directions, we extract a phrase dependency only for the single longest one.
			Since we use a projective dependency parser, the longest lexical dependency between two phrases is guaranteed to be unique.
			Table 2 shows a listing of the most frequent phrase dependencies extracted (lexical dependencies are omitted).
			We note that during training we never explicitly commit to any single phrase dependency tree for a target sentence.
			Rather, we extract phrase dependencies from all phrase dependency trees consistent with the word alignments and the lexical dependency tree.
			Thus we treat phrase dependency trees analogously to phrase segmentations in standard phrase extraction.
			We perform this procedure on all sentence pairs in the parallel corpus.
			Given a set of extracted 3 For a monolingual task, Wu et al.
			(2009) used a shallow parser to convert lexical dependencies from a dependency parser into phrase dependencies.
			where d(i) = I [τφ(i) − i > 0] is the direction of the dependency arc. Although we use log-probabilities in this feature function, we first add a constant C to each to ensure they are all positive.4 The max expression protects unseen parent-child phrase dependencies from causing the score to be negative infinity.
			Our motivation is a desire for the features to be used to prefer one derivation over another but not to rule out a derivation completely if it merely happens to contain a dependency unobserved in the training data.
			We also include lexical weighting features similar to those used in phrase-based MT (Koehn et al., 2003).
			Whenever we extract a phrase dependency, we extract the longest lexical dependency containedwithin it.
			For all (parent, child, direction) lexical dependency tuples (x, y, d), we estimate conditional probabilities plex (y|x, d) from the parsed cor pus using relative frequency estimation.
			Then, for a phrase dependency with longest lexical dependency (x, y, d), we add a feature for plex (y|x, d) to the model, using a formula similar to Eq. 1.
			Different instances of a phrase dependency may have different lexical dependencies extracted with them.
			We add the lexical weight for the most frequent, breaking ties by choosing the lexical dependency that maxi mizes p(y|x, d), as was also done by Koehn et al.
			(2003).
			In all, we include 4 target tree features: one for phrase dependencies, one for lexical dependencies, 4 The reasoning here is that whenever we use a phrase dependency that we have observed in the training data, we want to boost the score of the translation.
			If we used log-probabilities, each observed dependency would incur a penalty.
			a b c d e x y z a b c d e x y z a b c d e x y z a b c d e x y z Input: s e n t e n c e s , d e p e n d e n c y p a r s e τ s , c o a r s e p a r a m e t e r s λ M , f i n e p a r a m e t e r s ( λ , ψ ) Output: translation t LMERT ← GenerateL attices (s, λM ); LFB ← FBPrune (LMERT , λM ); (t, γ, φ, τφ , a) ← QGDEPPARSE (LFB , (λ, ψ)); Figure 1: String-to-tree configurations; each is associated with a feature that counts its occurrences in a derivation.
			and the same features computed from a transformed version of the corpus in which each word is replaced by its Brown cluster.
			4.2 String-to-Tree Configurations.
			We consider features that count instances of reordering configurations involving phrase dependencies.
			In addition to the target-side structures, these features consider γ and a, though not s or τs. For example, when building a parent-child phrase dependency with the child to the left, one feature value is incremented if their aligned source-side phrases are in the same order.
			This configuration is the leftmost in Fig.
			1; we include features for the other three configurations there as well, for a total of 4 features in this category.
			4.3 Tree-to-Tree Configurations.
			We include features that consider s, γ, and τs in addition to t, φ, and τφ.
			We begin with features for each of the quasi-synchronous configurations from Smith and Eisner (2006), adapted to phrase dependency grammars.
			That is, for a parent-child pair(τφ(i), i) in τφ, we consider the relationship be tween a(τφ(i)) and a(i), the source-side phrasesto which τφ(i) and i align.
			We use the follow ing named configurations from Smith and Eisner:root-root, parent-child, child-parent, grandparent return t; Algorithm 1: CoarseToFineDecode we use a phrase dependency tree for the target side, a lexical dependency tree for the source side, and a phrase alignment.
			We use the following heuristic approach.
			Given a pair of source words, one with index j in source phrase a(τφ(i)) and the other with index k in source phrase a(i), we have a parent- child configuration if τs(k) = j; if τs(j) = k, a child-parent configuration is present.
			In order for the grandparent-grandchild configuration to be present, the intervening parent word must be outside both phrases.
			For sibling and other c-command configurations, the shared parent or ancestor must also be outside both phrases.
			After obtaining a list of all configurations present for each pair of words (j, k), we fire the feature for the single configuration corresponding to the maximum distance |j − k|.
			If no configurations are present between any pair of words, the “other” feature fires.
			Therefore, only one configuration feature fires for each phrase dependency attachment.
			Finally, we include features that consider the dependency path distance between phrases in the source-side dependency tree that are aligned to parent-child pairs in τφ.
			We include a feature that sums, for each target phrase i, the inverse of the minimum undirected path length between each word grandchild, sibling, and c-command.5 We define a in a(i) and each word in τφ (a(i)).
			The minimumfeature to count instances of each of these configu rations, including an additional feature for “other” configurations that do not fit into these categories.6 When using a QPDG, there are multiple ways to compute tree-to-tree configuration features, since
	
	
			6 We actually include two versions of each configuration feature other than “root-root”: one for the source phrases being in the same order as the target phrases and one for them being swapped.
			undirected path length is defined as the number of dependency arcs that must be crossed to travel from one word to the other in τs. We use one feature for undirected path length and one other for directed path length.
			If there is no (un)directed path from a word in a(i) to a word in τφ(a(i)), we use ∞ as the minimum length.
			There are 15 features in this category, for a total of 23 QPDG features.
			5 Decoding.
			For a QPDG model, decoding consists of finding the highest-scoring tuple (t, γ, φ, τφ, a) for an in put sentence s and its parse τs, i.e., finding the most probable derivation under the s/τs-specific grammar Gs,τs . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,τs and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.
			It has become common in recent years for MT researchers to exploit efficient data structures for encoding concise representations of the pruned search space of the model, such as phrase lattices for phrase-based MT (Ueffing et al., 2002; Macherey et al., 2008; Tromble et al., 2008).
			Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple (t, γ, φ, a) for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming.
			To also maximize over τφ, we perform lattice dependency parsing, which allows us to search over the space of tuples (t, γ, φ, a, τφ).
			Given the lattice and Gs,τs , lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (1996).
			The lattice parsing algorithm requires O(E2V ) time and O(E2 + V E) space, where E is the number of edges in the lattice and V is the number of nodes.7 Typical phrase lattices might easily contain tens of thousands of nodes and edges, making exact search prohibitively expensive for all but the smallest lattices.
			So, we use approximate search based on coarse-to-fine decoding.
			We now discuss each step of this procedure; an outline is shown as Alg.
			1.
			Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning (Six- tus and Ortmanns, 1999), which has also been used in previous work using phrase lattices (Tromble et al., 2008).
			This pruning method computes the max- marginal for each lattice edge, which is the score of the best full path that uses that edge.
			Maxmarginals 7 To prevent confusion, we use the term edge to refer to a phrase lattice edge and arc to refer to a parent-child dependency in the phrase dependency tree.
			offer the advantage that the best path in the lattice is preserved during pruning.
			For each lattice, we use a grid search to find the most liberal threshold that leaves fewer than 1000 edges in the resulting lattice.
			As complexity is quadratic in E, forcing E to be less than 1000 improves runtime substantially.
			After pruning, the lattices still contain more than 1016 paths on average and oracle BLEU scores are typically 1215 points higher than the model-best paths.
			Pass 2: Parent Ranking Given a pruned lattice, we then remove some candidate dependency arcs from consideration.
			It is common in dependency parsing to use a coarse model to rank the top k parents for each word, and to only consider these during parsing (Martins et al., 2009; Bergsma and Cherry, 2010).
			Unlike string parsing, our phrase lattices impose several types of constraints on allowable arcs.
			For example, each node in the phrase lattice is annotated with a coverage vector—a bit vector indicating which words in the source sentence have been translated—which implies a topological ordering of the nodes.
			To handle constraints like these, we first use the FloydWarshall algorithm (Floyd, 1962) to find the best score between every pair of nodes in the lattice.
			This algorithm also tells us whether each edge is reachable from each other edge, allowing us to immediately prune dependency arcs between edges that are unreachable from each other.
			After eliminating impossible arcs, we turn to pruning away unlikely ones.
			In standard (string) dependency parsing, every word is assigned a parent.
			In lattice parsing, however, most lattice edges will not be assigned any parent.
			Certain lattice edges are much more likely to be contained within paths, so we allow some edges to have more candidate parent edges than others.
			We introduce hyperparameters α, β, and µ to denote, respectively, the minimum, maximum, and average number of parent edges to be considered for each lattice edge (α ≤ µ ≤ β).
			We rank the full set of E2 arcs according to their scores (using the QPDG features and their weights ψ) and choose the top µE of these arcs while ensuring that each edge has at least α and at most β potential parent edges.
			This step reduces the time complexity from O(E2V ) to O(µEV ), where µ < E. In our experiments, we set µ = 300, α = 100, and β = 400.
			Input: tuning set D = (S, T ), initial weights λ0 for coarse model, initial weights ψ0 for additional features in fine model Output: coarse model learned weights: λM , fine model learned weights: (λ∗, ψ∗) λM ← MERT (S, T , λ0 , 100, MOSES); LMERT ← GenerateLattices (S, λM ); LFB ← FBPrune (LMERT , λM ); (λ∗, ψ∗) ← MERT (LFB , T , (λM , ψ0 ), 200, QGDEPPARSE); return λM , (λ∗, ψ∗); Algorithm 2: CoarseToFineTrain Pass 3: Lattice Dependency Parsing After completing the coarse passes, we parse using bottom-up dynamic programming based on the agenda algorithm (Nederhof, 2003; Eisner et al., 2005).
			We only consider arcs that survived the filtering in Pass 2.
			We weight agenda items by the sum of their scores and the FloydWarshall best path scores both from the start node of the lattice to the beginning of the item and the end of the item to any final node.
			This heuristic helps us to favor exploration of items that are highly likely under the phrase-based model.
			If the score of the partial structure can only get worse when combining it with other structures (e.g., in a PCFG), then the first time that we pop an item of type GOAL from the agenda, we are guaranteed to have the best parse.
			However, in our model, some features are positive and others negative, making this property no longer hold; as a result, GOAL items may be popped out of order from the agenda.
			Therefore, we use an approximation, simply popping G GOAL items from the agenda and then stopping.
			The items are sorted by their scores and the best is returned by the decoder (or the k best in the case of MERT).
			In our experiments, we set G = 4000.
			The combined strategy yields average decoding times in the range of 30 seconds per sentence, which is comparable to other syntax-based MT systems.
	
	
			For tuning the coarse and fine parameters, we use minimum error rate training (MERT; Och, 2003) in a procedure shown as Alg.
			2.
			We first use MERT to train parameters for the coarse phrase-based model used to generate phrase lattices.
			Then, after generating the lattices, we prune them and run MERT a second time to tune parameters of the fine model, which includes all phrase-based and QPDG parameters.
			The arguments to MERT are a vector of source sentences (or lattices), a vector of target sentences, the initial parameter values, the size of the k-best list, and finally the decoder.
			We initialize λ to the default Moses feature weights and for ψ we initialize the two target phrase dependency weights to 0.004, the two lexical dependency weights to 0.001, and the weights for all configuration features to 0.0.
			Our training procedure requires two executions of MERT, and the second typically takes more iterations to converge (10 to 20 is typical) than the first due to the use of a larger feature set and increased possibility for search error due to the enlarged search space.
	
	
			For experimental evaluation, we consider Chinese- to-English (ZHEN) and Urdu-to-English (UREN) translation and compare our system to Moses (Koehn et al., 2007).
			For ZHEN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14).
			We segmented the Chinese data using the Stanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words.
			For UREN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words.
			We trained a baseline Moses system using default settings and features.
			Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions and the grow-diag-final-and heuristic was used to symmetrize the alignments.
			We used a max phrase length of 7 when extracting phrases.
			Trigram language models were estimated using the SRI language modeling toolkit (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998).
			To estimate language models for each language pair, we used the English side of the parallel corpus concatenated with 200M words of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times).
			We used this baseline Moses system to generate phrase lattices for our system, so our model includes all of the Moses features in addition to the M T 03 (t un e) M T 0 2 M T 0 5 M T 0 6 A v e r a g e M os es Q PD G (T T) Q PD G (T T+ S2 T+ T2 T) 33 .8 4 34.
			63 (+ 0.
			79 ) 34.
			98 (+ 1.
			14 ) 33 .3 5 31.81 28.82 34.
			10 (+ 0.
			75 ) 32.15 (+0.34) 29.33 (+0.51) 34.
			26 (+ 0.
			91 ) 32.34 (+0.53) 29.35 (+0.53) 31 .3 3 31.
			86 (+ 0.
			53 ) 31.
			98 (+ 0.
			65 ) Table 4: ChineseEnglish Results (% BLEU).
			QPDG features described in §4.
			In our experiments, we compare our QPDG system (lattice parsing on each lattice) to the Moses baseline (finding the best path through each lattice).
			The conventional wisdom holds that hierarchical phrase-based translation (Chiang, 2005) performs better than phrase- based translation for language pairs that require large amounts of reordering, such as ZHEN and UREN.
			However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010).
			So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexical- ized reordering model (Koehn et al., 2005).
			We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009).
			We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side.
			We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3.
			To obtain Brown clusters for the target tree features in §4.1, we used code from Liang (2005).8 We induced 100 clusters from the English side of the parallel corpus concatenated with 10M words of randomly-selected Gigaword sentences.
			Only words that appeared at least twice in this data were considered during clustering.
			An additional cluster was created for all other words; this allowed us to use phrase dependency cluster features even for out-of- vocabulary words.
			We used a max phrase length of 7 when extracting phrase dependencies to match the max phrase length used in phrase extraction.
			Approximately 87M unique phrase dependencies were extracted from the ZHEN data and 7M from the UREN data.We tuned the weights of our model using the pro Table 5: UrduEnglish Results (% BLEU).
			cedure described in §6.
			For ZHEN we used MT03 for tuning and MT02, MT05, and MT06 for testing.
			For UREN we used half of the documents (882 sentence pairs) from the MT08 test set for tuning (“Dev”) and MT09 for testing.
			We evaluated translation output using case-insensitive IBM BLEU (Pa- pineni et al., 2001).
			7.1 Results.
			Results for ZHEN and UREN translation are shown in Tables 4 and 5.
			We show results when using only the target tree features from §4.1 (TT), as well as when adding the string-to-tree features from §4.2 (S2T) and the tree-to-tree features from §4.3 (T2T).
			We note that T2T features are unavailable for UREN because we do not have an Urdu parser.
			We find that we can achieve moderate but consistent improvements over the baseline Moses system, for an average increase of 0.65 BLEU points for ZHEN and 0.75 for UREN.
			Fig.
			2 shows an example sentence from the MT05 test set along with its translation output and derivations produced by Moses and our QPDG system with the full feature set.
			This example shows the kind of improvements that our system makes.
			In Chinese, modifiers such as prepositional phrases and clauses are generally placed in front of the words they modify, frequently the opposite of English.
			In addition, Chinese occasionally uses postpositions where English uses prepositions.
			The Chinese sentence in Fig.
			2 exhibits both of these, as the prepositional phrase “after the Palestinian election” appears before the verb “strengthen” in the Chinese sentence and “after” appears as a postposition.
			Moses (Fig.
			2(a)) does not properly reorder the prepositional phrase, while our system (Fig.
			2(b)) properly handles both reorderings.9 We shall discuss these 8 http://www.cs.berkeley.edu/˜pliang/ software 9 Our.
			system’s derivation is not perfect, in that “in” is incor $ bush united : states will in palestine elections after strengthen peace efforts (a) (b) 布希 : 美 将 在 巴勒斯坦 大 后 加强 和平 努力 pre sid enti al ele ctio n in the uni ted stat es wil l stre ngt hen the pea ce eff ort s 布 希 : 美 将 在 巴勒斯坦 大 后 加强 和平 努力 $ (c) Refere nces b u s h : u s s e t t o b o o s t p e a c e e f f o r t s a f t e r p a l e s t i n i a n e l e c t i o n b u s h : u s t o s t e p u p p e a c e e f f o r t s a f t e r p a l e s t i n i a n e l e c t i o n s b u s h : u . s . w i l l e n h a n c e p e a c e e f f o r t s a f t e r p a l e s t i n i a n e l e c t i o n u s t o b o o s t p e a c e e f f o r t s a f t e r p a l e s t i n i a n e l e c t i o n s : b u s h Figure 2: (a) Moses translation output along with γ, φ, and a. An English gloss is shown above the Chinese sentence and above the gloss is shown the dependency parse from the Stanford parser.
			(b) QPDG system output with additional structure τφ .
			(c) reference translations.
			types of improvements further in §8.
			7.2 Unsupervised Parsing.
			Our results thus far use supervised parsers for both Chinese and English, but parsers are only available for a small fraction of the languages we would like to translate.
			Fortunately, unsupervised dependency grammar induction has improved substantially in recent years due to a flurry of recent research.
			While attachment accuracies on standard treebank test sets are still relatively low, it may be the case that even though unsupervised parsers do not match treebank annotations very well, they may perform well when used for extrinsic applications.
			We believe that syntax-based MT offers a compelling platform for development and extrinsic evaluation of unsupervised parsers.
			In this paper, we use the standard dependency model with valence (DMV; Klein and Manning, 2004).
			When training is initialized using the output of a simpler, concave dependency model, the rectly translated and reordered, but the system was nonetheless DMV can approach state-of-the-art unsupervised accuracy (Gimpel and Smith, 2011).
			For English, the resulting parser achieves 53.1% attachment accuracy on Section 23 of the Penn Treebank (Marcus et al., 1993), which approaches the 55.7% accuracy of a recent state-of-the-art unsupervised model (Blunsom and Cohn, 2010).
			The Chinese parser, initialized and trained the same way, achieves 44.4%, which is the highest reported accuracy on the Chinese Treebank (Xue et al., 2004) test set.
			Most unsupervised grammar induction models assume gold standard POS tags and sentences stripped of punctuation.
			We use the Stanford tag- ger (Toutanova et al., 2003) to obtain tags for both English and Chinese, parse the sentences without punctuation using the DMV, and then attach punctuation tokens to the root word of the tree in a post- processing step.
			For English, the predicted parents agreed with those of TurboParser for 48.7% of the tokens in the corpus.
			We considered all four scenarios: supervised and unsupervised English parsing paired with supervised E N u n s u p e r v i s e d supervised Z H u n s u p e r v i s e d s u p e r v i s e d 31.
			18 (3 3.
			76 ) 31.86 (34.78) 32.
			12 (3 4.
			74 ) 31.98 (34.98) M o s e s 3 1 . 3 3 ( 3 3 . 8 4 ) Table 6: Results when using unsupervised dependency parsers.
			Cells contain averaged % BLEU on the three test sets and % BLEU on tuning data (MT03) in parentheses.
			Fe at ur e Ini tia l Le ar ne d Le ft chi ld, sa me or de r L ef t c hil d, s w a p p hr a s e s Ri g ht c hil d, s a m e o r d e r Ri g ht c hi ld , s w a p p hr a s e s 9 . 0 1 . 1 7 . 3 1 . 6 8 . 9 0 . 0 7 . 3 2 . 3 Root ro ot P a r e n t c h i l d C h i l d p a r e n t G r a n d p a r e n t g r a n d c h i l d S i b l i n gC co m m an d Ot he r 0 . 4 4 . 2 1 . 2 1 . 0 2 . 4 6 . 1 1 . 5 0 . 8 6 . 1 0 . 4 0 . 2 1 . 9 6 . 7 0 . 9 Table 7: Average feature values across best translations of sentences in the MT03 tuning set, both before MERT (column 2) and after (column 3).
			“Same” versions of tree- to-tree configuration features are shown; the rarer “swap” features showed a similar trend.
			BLEU scores averaged over the three test sets with tuning data BLEU in parentheses.
			Surprisingly, we achieve our best results when using the unsupervised English parser in place of the supervised one (+0.79 over Moses), while keeping the Chinese parser supervised.
			Competitive performance is also found by using the unsupervised Chinese parser and supervised English parser (+0.53 over Moses).
			However, when using unsupervised parsers for both languages, performance was below that of Moses.
			During tuning for this configuration, we found that MERT struggled to find good parameter estimates, typically converging to suboptimal solutions after a small number of iterations.
			We believe this is due to the large number of features (37), the noise in the parse trees, and known instabilities of MERT.
			In future work we plan to experiment with training algorithms that are more stable and that can handle larger numbers of features.
	
	
			To understand what our model learns during MER training, we computed the feature vectors of the best derivation for each sentence in the tuning data at both the start and end of tuning.
			Table 7 shows these feature values averaged across all tuning sentences.
			The first four features are the configurations from Fig.
			1, in order from left to right.
			From these rows, we can observe that the model learns to encourage swapping when generating right children and penalize swapping for left children.
			In addition to objects, right children in English are often prepositional phrases, relative clauses, or other modifiers; as we noted above, Chinese generally places these modifiers before their heads, requiring reordering during translation.
			Here the model appears to be learning this reordering behavior.
			From the second set of features, we see that the model learns to favor producing dependency trees that are mostly isomorphic to the source tree, by favoring root-root and parent-child configurations at the expense of most others.
	
	
			In looking at BLEU score differences between the two systems, the unigram precisions were typically equal or only slightly different, while precisions for higher-order n-grams contained the bulk of the improvement.
			This suggests that our system is not finding substantially better translations for individual words in the input, but rather is focused on reordering the existing translations.
			This is not surprising given our choice of features, which focus on syntactic language modeling and syntax-based reordering.
			The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al., 2005), target syntax (Shen et al., 2008), and syntax on both sides.
			Our framework allows integrating together all of these and other types of structures, with the ultimate goal of combining the strengths of multiple approaches to translation in a single model.
	
	
			We thank Chris Dyer and the anonymous reviewers for helpful comments that improved this paper.
			This research was supported in part by the NSF through grant IIS 0844507, the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF10-10533, and Sandia National Laboratories (fellowship to K. Gimpel).
	
