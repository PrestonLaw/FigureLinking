
	
		This paper presents systems submitted to the close track of Fourth SIGHAN Bakeoff.
		We built up three systems based on Conditional Random Field for Chinese Word Segmentation, Named Entity Recognition and Part-Of-Speech Tagging respectively.
		Our systems employed basic features as well as a large number of linguistic features.
		For segmentation task, we adjusted the BIO tags according to confidence of each character.
		Our final system achieve a F-score of 94.18 at CTB, 92.86 at NCC, 94.59 at SXU on Segmentation, 85.26 at MSRA on Named Entity Recognition, and 90.65 at PKU on Part-Of-Speech Tagging.
	
	
			Fourth SIGHAN Bakeoff includes three tasks, that is, Word Segmentation, Named Entity Recognition (NER) and Part-Of-Speech (POS) Tagging.
			In the POS Tagging task, the testing corpora are pre- segmented.
			Word Segmentation, NER and POSTagging could be viewed as classification prob We attended the close track of CTB, NCC, SXU on Segmentation, MSRA on NER and PKU on POS Tagging.
			In the close track, we cannot use any external resource, and thus we extracted several word lists from training corpora to form multiple features beside basic features.
			Then we trained CRF models based on these feature sets.
			In CRF models, a margin of each character can be gotten, and the margin could be considered as the confidence of that character.
			For the Segmentation task, we performed the Maximum Probability Segmentation first, through which each character is assigned a BIO tag (B represents the Beginning of a word, I represents In a word and O represents Out of a word).
			If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).
	
	
			Conditional Random Fields (CRFs) are a class of undirected graphical models with exponent distribution (Lafferty et al., 2001).
			A common used special case of CRFs is linear chain, which has a distribution of: T lems.
			In a Segmentation task, each character P ( yr | xr) = 1 exp(∑ ∑ λ f k ( y t −1 , y , xr, t )) (1)should be classified into three classes, B, I, O, in Z xr t =1 k r dicating whether this character is the Beginning of a word, In a word or Out of a word.
			For NER, each wheref k ( yt −1 , yt x, t ) is a function which is usu character is assigned a tag indicating what kind of ally an indicator function; λk is the learned weight Named Entity (NE) this character is (Beginning of of feature f k ; and Z xr is the normalization factor.a Person Name (PN), In a PN, Beginning of a Lo cation Name (LN), In a LN, Beginning of an Organization Name (ON), In an ON or not-a-NE).
			In POS tagging task defined by Fourth SIGHAN Bakeoff, we only need to give a POS tag for each given word in a context.
			The feature function actually consists of two kinds of features, that is, the feature of single state and the feature of transferring between states.
			Features will be discussed in section 3.
			Several methods (e.g. GIS, IIS, L-BFGS) could be used to estimate λk , and L-BFGS has been showed to converge faster than GIS and IIS.
			To build up our system, we used Pocket CRF1.
	
	
			We used three feature sets for three tasks respectively, and will describe them respectively.
			3.1 Word Segmentation.
			We mainly adopted features from (H. T. Ng et al., 2004, Y. Shi et al., 2007), as following: a) Cn(n=-2, -1, 0, 1, 2) b) CnCn+1(n=-2,-1,0,1) c) C-1C1 d) CnCn+1Cn+2 (n=-1, 0, 1) e) Pu(C0) f) T(C-2)T(C-1)T(C0)T(C1)T(C2) g) LBegin(C0), Lend(C0) h) Single(C0) where C0 represents the current character and Cn represents the nst character from the current character.
			Pu(C0) indicates whether current word is a punctuation.
			this feature template helps to indicate the end of a sentence.
			T(C) represents the type of character C. There are four types we used: (1) Chi nese Number (“一/one”, “二/two”, “十/ten”); (2) Chinese Dates (“日/day”, “月/month”, “年/year”); (3) English letters; and (4) other characters.
			The (f) feature template is used to recognize the Chinese dates for the construction of Chinese dates may cause the sparseness problem.
			LBegin(C0) represents the maximum length of the word beginning with the character C0, and Lend(C0) presents the maximum length of the word ending with the character C0.
			The (g) feature template is used to decide the boundary of a word.
			Single(C0) shows whether current character can form a word solely.
			3.2 Named Entity Recognition.
			Most features described in (Y. Wu et al., 2005) are used in our systems.
			Specifically, the following is the feature templates we used: a) Surname(C0): Whether current character is in a Surname List, which includes all first characters of PNs in the training corpora.
			1 http://sourceforge.net/project/showfiles.php?group_id=201943 b) PersonName(C0C1C2, C0C1): Whether C0C1C2, C0C1 is in the Person Name List, which contains all PNs in the training corpora.
			c) PersonTitle(C-2C-1): Whether C-2C-1 is in the Person Title List, which is extracted from the previous two characters of each PN in the training corpora.
			d) LocationName(C0C1,C0C1C2,C0C1C2C3): Whether C0C1,C0C1C2,C0C1C2C3 is in the Location Name List, which includes all LNs in the training corpora.
			e) LocationSuffix(C0): Whether current character is in the Location Suffix List, which is constructed using the last character of each LN in the training corpora.
			f) OrgSuffix(C0): Whether current character is in the Organization Suffix List, which contains the last-two-character of each ON in the training corpora.
			3.3 Part-Of-Speech Tagging.
			We employed part of feature templates described in (H. T. Ng et al., 2004, Y. Shi et al., 2007).
			Since we are in the close track, we cannot use morphological features from external resources such as HowNet, and we used features that are available just from the training corpora.
			a) Wn, (n=-2,-1,0,1,2) b) WnWn+1, (n=-2,-1,0,1) c) W-1W1 d) Wn1WnWn+1 (n=-1, 1) e) Cn(W0) (n=0,1,2,3) f) Length(W0) where Cn represents the nth character of the current word, and Length(W0) indicates the length of the current word.
	
	
			In the task of Word Segmentation, the label of each character is adjusted according to their reliability.
			For each sentence, we perform Maximum Probability Segmentation first, through which we can get a BIO tagging for each character in the sentence.
			After that, the features are extracted according to the feature templates, and the weight of each feature has already been estimated in the step of training.
			Then marginal probability for each character can be computed as follows: p( y | xr) = 1 Z ( x) exp(λi f ( xr, y)) (2) The value of p( y | x ) becomes the original re liability value of BIO label y for the current character under the current contexts.
			If the probability of y with the largest probability is lower than 0.75, which is decided according to the experiment results, the tag given by Maximum Probability Seg mentation will be used instead of tag given by CRF.
			The motivation of this method is to use the Maximum Probability method to enhance the F-measure of In-Vocabulary (IV) Words.
			According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.
			One simplest way to combine them is the method we described.
			Besides, there are some complex methods, such as estimation using Support Vector Machine (SVM) for CRF, CRF combining boosting and combining Margin Infused Relaxed Algorithm (MIRA) with CRF, that might perform better.
			However, we did not have enough time to implement these methods, and we will compare them detailedly in the future work.
	
	
			5.1 Results on Fourth SIGHAN Bakeoff.
			We participated in the close track on Word Segmentation on CTB, NCC and SXU corpora, NER on MSRA corpora and POS Tagging on PKU corpora.
			For Word Segmentation and NER, our memory was enough to use all features.
			However, for POS tagging, we did not have enough memory to use all features, and we set a frequency cutoff of 10; that is, we could only estimate variables for those features that occurred more than ten times.
			Our results of Segmentation are listed in the Tabel 1, the results of NER are listed in the Tabel 2, and the results of POS Tagging are listed in the Tabel 3.
			Tabel 1.
			Results of Word Segmentation Tabel 2.
			Results of NER Total-A IV-R OOV-R MT-R PKU 0.9065 0.9259 0.5836 0.8903 Tabel 3.
			Results of POS Tagging 5.2 Errors Analysis.
			Observing our results of Word Segmentation and POS Tagging, we found that the recall of OOV is relatively low, this may be improved through introducing features aiming to enhance the performance of OOV.
			On NER task, we noticed that precision of PN recognition is relative low, and we found that our system may classify some ONs as PNs, such as “吉 尼斯(Guinness)/ORG” and “世界记录(World Re cord)/)”.
			Besides, the bound of PN is sometimes confusing and may cause problems.
			For example, “胡绳/PER 曾/ 有/ 题词” may be segmented as“胡绳曾/PER 有/ 题词”.
			Further, some words be ginning with Chinese surname, such as “丁丑盛 夏”, may be classified as PN.
			For List may not be the real suffix.
			For example, “玉峰山麓” should be a LN, but it is very likely that “玉峰山” is recognized as a LN for its suffix “山”.
			Another problem involves the characters in the Location Name list may not a LN all the time.
			In the context “华裔/ 作家/”, for example, “华” means Chinese rather than China.
			For ONs, the correlative dictionary also exists.
			Consider sequence “人大代表”, which should be a single word, “人大” is in the Organization Name List and thus it is recognized as an ON in our system.
			Another involves the subsequence of a word.
			For example, the sequence “湖北钟祥市工业局 长”, which should be a person title, but “湖北钟祥 市工业局” is an ON.
			Besides, our recall of ON is low for the length of an ON could be very long.
	
	
			We built up our systems based on the CRF model and employed multiple linguistics features based on the knowledge extracted from training corpora.
			We found that these features could greatly improve the performance of all tasks.
			Besides, we adjusted the tag of segmentation result according to the reliability of each character, which also helped to enhance the performance of segmentation.
			As many other NLP applications, feature plays a very important role in sequential labeling tasks.
			In our POS tagging task, we could only use features with high frequency, but some low-frequency fea tures may also play a vital role in the task; good non-redundant features could greatly improve classification performance while save memory re quirement of classifiers.
			In our further research, we will focus on feature selection on CRFs.
	
	
			This research was sponsored by National Natural Science Foundation of China (No. 60773124, No. 60503070).
	
