
	
	
			form a multi-pass A* search with an iteratively improved heuristic function allows us to translate even long sen alignment aJ is introduced as a hidden variable.
			Typically, the search is performed using the so- called maximum approximation:tences.
			We compare the A* search al gorithm with a beam-search approach on the Hansards task.
			eˆI   I  J J I  = arg max I 1 P r(e1 ) ·  J 1 ( P r(f1 , a1 |e1 )  1 Introduction.
			= arg max P r(eI ) · max P r(f J , aJ |eI )The goal of machine translation is the transla I 1 J 1 1 1 1 1 tion of a text given in some source language into a target language.
			We are given a source string The search space consists of the set of all possible target language strings eI and all possible align 1 = f1...fj ...fJ , which is to be translated into a target string eI = e1...ei...eI . Among all possible target strings, we will choose the string with the highest probability: ments aJ .
	
	
			Various statistical alignment models of the form P r(f J , aJ |eI ) have been introduced in (Brown eˆI = arg max P r(eJ |f I ) 1 1 1 1 I 1 1 1 et al., 1993; Vogel et al., 1996; Och and Ney, 2000a).
			In this paper we use the so-called Model = arg max P r(eI ) · P r(f J |eI ) I 1 1 1 1 The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.
			P r(eI ) is the language model of the target language, whereas P r(f J |eI ) 4 from (Brown et al., 1993).
			In Model 4 the statistical alignment model is decomposed into five sub-models: • the lexicon model p(f |e) for the probability that the source word f is a translation of the target word e, 1 1 denotes the translation model.
			Many statistical translation models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000b) • the distortion model p=1(j−jt|C (fj ), E) for the probability that the translations of two difference j − jt where C (fj ) is the word class of fj and E is the word class of the first of the two consecutive target words, • the distortion model p>1(j − jt|C (fj )) for the probability that the words aligned to one target words have the position difference j − jt, • the fertility model p(φ|e) for the probability that a target language word e is aligned to φ source language words, • the empty word fertility model p(φ0|e0) for the probability that exactly φ0 words remain unaligned to.
			The final probability p(f J , aJ |eI ) for Model 4 is scribed at the end of this subsection.
			• A scoring function Q(n) + h(n) has to be defined which assigns a score to every node n. For beam search, this is the score Q(n) of a best path to this node.
			In the A* algorithm, an estimation h(n) of the score of a best path from node n to a goal node is added.
			(Berger et al., 1996) presented a method to structure the search space.
			Our search algorithm for Model 4 uses a similar structuring of the search space.
			We will shortly review the basic concepts of this search space structure: Every partial hypothesis consists of a prefix of the target sentence and a corresponding alignment.
			A partial hypothesis is extended by accounting for exactly one ad 1 1 1 obtained by multiplying the probabilities of the sub-models for all words.
			For a detailed description for Model 4 the reader is referred to (Brown et al., 1993).
			We use Model 4 in this paper for two reasons.
			First, it has been shown that Model 4 produces a very good alignment quality in comparison to various other alignment models (Och and Ney, 2000b).
			Second, the dependences in the distortion model along the target language words make it quite easy to integrate standard n-gram language models in the search process.
			This would be more difficult in the HMM alignment model (Vogel et al., 1996).
			Yet, many of the results presented in the following are also applicable to other alignment models.
	
	
			The following tasks have to be performed both using A* and beam search (BS):ditional word of the source sentence.
			Every exten sion yields an extension score which is computed by taking into account the lexicon, distortion, and fertility probabilities involved with this extension.
			A partial hypothesis is called open if more source words are to be aligned to the current target word in the following extensions.
			A hypothesis that is not open is said to be closed.
			Every extension of an open hypothesis will extend the fertility of the previously produced target word and an extension of a closed hypothesis will produce a new word.
			Therefore, the language model score is added as well if a closed hypothesis is extended.
			It is prohibitive to consider all possible translations of all words.
			Instead, we restrict the search to the most promising candidates by calculating “inverse translations” (AlOnaizan et al., 1999).
			The inverse translation probability p(e | f ) of a source word f is calculated as p (f | e) p (e) • The search space has to be structured into a search graph.
			This search graph typically p(e | f ) = L- p (f el | et) p (et) , includes an initial node, intermediary nodes (partial hypotheses), and goal nodes (completed hypotheses).
			A node contains the following information: – the predecessor words u, v in the target language, – the score of the hypothesis, – a backpointer to the preceding partial hypothesis,where we use a unigram model p (e) to esti mate the prior probability of a target word being used.
			Like (AlOnaizan et al., 1999), we use only the top 12 translations of a given source language word.
			In addition, we remove from this list all words whose inverse translation probability is lower than 0.01 times the best inverse translation probability.
			This observation pruning is the only pruning involved in our A* search algorithm.
			Experiments showed this does not impair translation quality, but the search becomes much more efficient.
			In order to keep the search space as small as possible it is crucial to perform a recombination of search hypotheses.
			Every two hypotheses which can be distinguished by neither the language model state nor the translation model state can be recombined, only the hypothesis with a better score of the two needs to be considered in the subsequent search process.
			We use a standard trigram language model, so the relevant language model state of node n consists of the current word w(n) and the previous word v(n) (later on we will describe an improvement to this).
			The translation model state depends on the specific model dependencies of Model 4: • a coverage set C(n) containing the already translated source language positions, • the position j(n) of the previously translated source word, • a flag indicating whether the hypothesis is open or closed, • the number of source language words which are aligned to the empty word, • a flag showing whether the hypothesis is a complete hypothesis or not.
			Efficient language model recombination The recombination procedure which is described above can be improved by taking into account the backing-off structure of the language model.
			The trigram language model we use has the property that if the count of the bigram N (u, v) = 0, then the probability P (w|u, v) depends only on v. In this case the recombination can be significantly improved by recombining all nodes whose language model state has the property N (u, v) = 0 only with respect to v. Obviously, this could be generalized to other types of language models as well.
			Experiments have shown that by using this efficient recombination, the number of needed hypotheses can be reduced by about a factor of 4.
			Search algorithms We evaluate the following two search algorithms: • beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.
			The search algorithm is based on a dynamic programming approach and applies various pruning techniques in order to restrict the number of considered hypotheses.
			For more details see (Tillmann, 2001).
			• A* search algorithm: In A*, all search hypotheses are managed in a priority queue.
			The basic A* search (Nils- son, 1971) can be described as follows: 1.
			initialize priority queue with an empty hypothesis 2.
			remove the hypothesis with the highest score from the priority queue 3.
			if this hypothesis is a goal hypothesis: output this hypothesis and terminate 4.
			produce all extensions of this hypothesis and put the extensions to the queue 5.
			goto 2 The so-called heuristic function estimates the probability of a completion of a partial hypothesis.
			This function is called admissible if it never underestimates this probability.
			Thus, admissible heuristic functions are always optimistic.
			The A* search algorithm corresponds to the Dijkstra algorithm if the heuristic function is equal to zero.
	
	
			In order to perform an efficient search with the A* search algorithm it is crucial to use a good heuristic function.
			We only know of the work by (Wang and Waibel, 1997) dealing with heuristic functions for search in statistical machine translation.
			They developed a simple heuristic function for Model 2 from (Brown et al., 1993) which was non admissible.
			In the following we develop a guaranteed admissible heuristic function for Model 4 taking into account distortion probabilities and the coupling of lexicon, fertility, and language model probabilities.
			The basic idea for developing a heuristic function for the alignment models is the fact that all source sentence positions which have not been to complete the sentence.
			Therefore, the value of the heuristic function H X (n) for a node n can be deduced if we have an estimation hX (j) of the optimal score of translating position j (here X denotes different possibilities to choose the heuristic function): bilities depends on the used model.
			For Model 4, we obtain: hD (j) = max p(j jt E, C (fj )) jl ,E Here, E refers to the class of the previously aligned target word.
			H X (n) = n jj∈C(n) hX (j) , The heuristic functions hD (j) involve maximizations over the source positions jt.
			The do main of this variable shrinks during search as where C(n) is the coverage set.
			The simplest realization of a heuristic function, denoted as hT (j), takes into account only the translation probability p(f |e): hT (j) = max p(fj e) e This heuristic function can be refined by introducing also the fertility probabilities (symbol F) of a target word e: hT F (j) = more and more words get translated.
			Therefore, it is possible to improve this heuristic function during search to perform a maximization only over the free source language positions jt.
			For Model 4 we compute the following heuristic function with two arguments: hD (jt, j) = max p(j − jt|E, C (fj )) Thus, we obtain as an estimation of the distortion probability = max max p(f |e) .φ (φ|e), p(f |e ) hD (j) = max jl j∈ C( n) hD (jt, j) . ej=e0 ,φ Thereby, a coupling between the translation and fertility probabilities is achieved.
			We have to This yields the following heuristic functions taking into account translation, fertility, language, and distortion model probabilities: take the φ-th root in order to avoid that the fertility probability of a target word whose fertility is higher than one is taken into account for every H T F LD (n) = n jj∈C(n) hT F L(j) · hD (j) (1) source word aligned to it.
			For words which are translated by the empty word e0, no fertility probability is used.
			The language model can be incorporated by considering that for every target word there exists an optimal language model probability: pL(e) = max p(e u, v) u,v Here, we assume a trigram language model.
			Thus, a heuristic function including a coupling between translation, fertility, and language model probabilities (TFL) is given by: hT F L(j) =Using these heuristic functions we have the over head of performing this rest cost estimation for every coverage set in search.
			The experiments will show that these additional costs are overcompensated by the gain in reducing the search space that has to be expanded during the A* search.
			To assess the predictive power of the various components in the heuristic, we compare the value of the heuristic function of the empty hypothesis with the score of the optimal translation.
			A heuristic function is better if the difference between these two values is small.
			Table 1 contains a comparison of various heuristic functions.
			We compare the average costs (nega . tive logarithm of the probabilities) of the optimal = max max p(fj e) φ p(φ e)pL(e), p(f e0) e,φ translatio n and the average of the estimated costs of the empty hypothesi s. Typically , the estimated This value can be precomputed efficiently before the search process itself starts.
			costs of T F LD and the real costs differ by factor 3.
			se nt en ce le ng th HF for initial node e m pir ica l sc or e go al no de sc or e T TF TF L TF L D 6 5.
			1 7.
			2 12 .7 13 .0 25 .9 35 .5 8 5.
			7 8.
			2 16 .0 16 .3 29 .8 43 .7 10 8.
			1 11 .6 19 .4 19 .7 36 .5 55 .8 12 9.
			5 13 .7 20 .7 21 .1 43 .9 63 .4 We will see later in Section 6 that the guaranteed admissible heuristic functions described above result in dramatically more efficient search.
	
	
			tence position first.
			Likewise, hE (j, J + 1) is the empirical score of finishing a sentence with j as the last source sentence position that was covered.
			This yields In this section we describe a new method to obtain an almost admissible heuristic function by a hE (j) = max jl j∈C(n)∨jl =J +1 hE (j, jt) . multi pass search.
			This yields a significantly more efficient search than using the admissible heuristic functions.
			Thus, we lose the strict guarantee to avoid search errors, but obtain a significant time gain.
			The idea of an empirical heuristic function is to perform a multi-pass search.
			In the first pass a good admissible heuristic function (here: H T F LD ) is used.
			If this search does not need too much memory the search process is finished.
			If the search failed, it is restarted using an improved heuristic function which had been obtained during the initial search process.
			This heuristic function is computed such that it has the property that it is admissible with respect to the explored search In this calculation of hE (j), we maximize over the columns of a matrix.
			The translation of the source sentence can be viewed as a Traveling Salesman Problem where the source sentence positions are the cities that have to be visited.
			Thus, the maximization over the columns is equivalent to assuring that the position j will be left after the visit.
			We design an improved heuristic function using the following principle (Aigner, 1993): Each city has to be both reached and left.
			Therefore, in order to take an upper bound of reaching a city into account, we divide each column of the matrix by its maximum and maximize over the rows of the matrix (Aigner, 1993): space.
			That means, the heuristic function is optimistic with respect to every node in the search hE+(j) = max jl j∈C(n)∨jl =j(n) hE (jt, j)/hE (jt) . space explored in the first pass.
			Specifically, during the first pass, we maintain a two-dimensional matrix hE (j, jt) with (J + 2) · We obtain the following empirical heuristic functions: (J + 2) entries which are all initialized with ∞. The entry hE (j, jt) is the best score that was com puted for translating the source language word inposition jt if the previously covered source sen H E (n) = n jj∈C(n)∨j=j(n) H E+(n) = hE (j) tence position is j. The matrix entry is updated for every extension of a node n → nt: = n jj∈C(n)∨j=j(n) hE (j) · n jl j∈C(n)∨jl =J +1 hE+(jt) hE (j(n), j(nt)) := = max hE (j(n), j(nt)), p(n, nt) Here, p(n, nt) is the probability of the extensionn → nt.
			hE (0, j) is the empirical score of starting a sentence by covering the j-th source sen If the search fails in the first pass due to the restriction of the number of hypotheses – which was 1 million in all experiments – the search can be started again using H E+(n) as a heuristic.
			To avoid an overestimation of the actual costs, we multiply the empirical costs by a factor lower than Table 3: Training corpus statistics (* without punctuation marks).
			Fr en ch En gli sh se nt en ce s 4 9 0 0 0 4 9 0 0 0 w or ds 74 39 03 81 69 64 w or ds * 66 40 58 73 08 80 av er ag e se nt en ce le ng th 1 6 . 9 1 4 . 6 vo ca bu lar y siz e 1 9 8 3 1 2 4 8 9 2 Table 4: Test corpora statistics.
			Co rp us # Se nt en ce s # Word s F E T 6 5 0 30 0 32 9 T 8 5 0 40 0 40 3 T 1 0 5 0 50 0 50 9 T 1 2 5 0 60 0 60 1 T 1 4 5 0 70 0 64 4 1.
			We found in our experiments that a factor of.
			0.7 is sufficient.
			The search was restarted up to 4 times if it failed.
			Using this method, it is possible to translate sentences that are longer than 10 words with a restriction to 1 million hypotheses.
			Table 1 shows the value of the empirical heuristic function of the empty node compared to the score of the optimal goal node.
			The estimated costs and the real costs now differ only by a factor of 1.5 instead of a factor of 3 for the TFLD heuristic function before.
	
	
			We present results on the HA N S A R D S task which consists of proceedings of the Canadian parliament that are kept both in French and in English.
			Table 3 shows the details of our training corpus.
			We used different the test corpora with sentences of length 614 words (Table 4).
			In all experiments, we use the following two error criteria: • WER (word error rate): The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the target string.
			• PER (position independent word error rate): The word order of a French/English sentence pair can be quite different.
			As a result, the word order of the automatically generated target sentence can be different from that of the given target sentence, but nevertheless acceptable so that the WER measure alone could be misleading.
			In order to overcome this problem, we introduce the position independent word error rate (PER) as additional measure.
			This measure compares the words in the two sentences without taking the word order into account.
			In the following experiments we restricted the maximum number of active search hypotheses in A* search to 1 million.
			Every hypothesis has an effective memory requirement of about 100 Byte.
			Therefore, we obtain a dynamic memory requirement of about 100 MByte.
			In order to speed up the search, we restricted the reordering of words in IBM-style (Berger et al., 1996; Tillmann, 2001).
			According to this restriction, up to 3 source sentence positions may be skipped and translated later, i. e. during the search process there may be up to 3 uncovered positions left of the rightmost covered position in the source sentence.
			The word error rate does not increase compared to a non-restricted reordering, but the search becomes much more efficient.
			Table 5 shows how many sentences with different sentence lengths can be translated using beam search and A* with various heuristic functions.
			Obviously, the BS approach is able to translate any sentence length, therefore the search success rate is 100%.
			Without any heuristic function A* is only able to translate all 8-word sentences (with the restriction of a maximum number of 1 million hypotheses).
			Using more sophisticated heuristic functions we are also able to translate all 10-word sentences with A*.
			Table 6 compares the search errors of A* and BS.
			During the BS search, translation pruning is carried out.
			The different hypotheses are distinguished according to the set of covered positions of the source sentence.
			For every set, the best score of all hypotheses is computed.
			Only those hypotheses are kept whose score is greater than this best score multiplied with a threshold.
			We chose the threshold to be 2.5, 5.0, 7.5 and 10.0 (see Table 6).
			Table 2: Effect of observation pruning on the translation quality (average over all test sets).
			# in ve rse tra nsl ati on s 1 0 1 2 1 4 1 6 1 8 2 0 W E R 73 .8 1 73 .3 3 75 .5 0 76 .2 3 76 .1 9 76 .5 9 PE R 68 .0 2 66 .9 3 70 .0 7 71 .1 6 71 .2 4 71 .1 6 Table 5: Search Success Rate (1 million hypotheses) [%].
			Table 7: Average search time [s] per sentence.
			(cf.
			below) which lead to a success for all the 12- word sentences.
			Table 6: Search errors [%].
			se nt en ce le ng th 6 8 10 12 14 B S 2.5 26 28 38 50 38 5 . 0 2 0 2 6 4 7 . 5 0 0 0 4 2 1 0 . 0 0 0 0 4 2 A * 0 0 0 0 0 For A* we never observe any search errors.
			In the case of the admissible heuristic functions, this is guaranteed by the approach.
			As can be seen from Table 6, the BS algorithm with a large beam rarely produces search errors.
			Table 7 compares the translation efficiency of the various search algorithms.
			We see that beam search even with a very large beam producing only very few search errors is much more efficient than the used A* search algorithm.
			Table 8 contains an assessment of translation quality comparison of A* and BS using the T6, T8, T10, T12-test corpus.
			For A*, we use the E+ rest cost estimation as this gives optimal results.
			From the 200 sentences of these test corpora we can translate 192 sentences using the 1 million hypotheses constraint.
			For the remaining sentences we performed a search with 4 million hypotheses The number of hypotheses in A* search We restricted the maximal number of hypotheses to 1 million.
			This was sufficient for translating 10-word sentences, as the search algorithm success rate in Table 5 shows.
			For longer sentences it is necessary to allow for a larger number of hypotheses.
			For the sentences of lengths 12 and 14, we performed an A* search (E+) with 2, 4 and 8 million possible hypotheses.
			The search algorithm success rate for those searches is contained in Table 9.
			We see a significant effect on the number of successful searches.
	
	
			We have developed sophisticated admissible and almost admissible heuristic functions for statistical machine translation.
			We have focussed on Model 4, but most of the computations could be easily extended to other statistical alignment models (like HMM or Model 5).
			We especially have observed the following effects: • The heuristic function has a strong effect on the efficiency of the A* search.
			Without any heuristic function only 75 % of the test corpus sentences can be translated (using the 1 million hypotheses constraint).
			Using the Table 8: Translation quality.
			B S (2.
			5) B S (5.
			0) B S (7.
			5) B S (1 0.
			0) A * (E +) W E R 6 9 . 6 5 6 8 . 7 8 6 8 . 6 8 6 8 . 6 8 6 8 . 6 8 P E R 6 2 . 6 5 6 1 . 6 2 6 1 . 5 1 6 1 . 5 1 6 1 . 4 5 Table 9: A* (E+) Success Rate for 12- and 14-word sentences [%].
			# hy po th es es 1 mi lli on 2 mi lli on 4 mi lli on 8 mi lli on 12 4 2 8 0 1 0 0 1 0 0 14 2 2 0 7 0 1 0 0 best admissible heuristic function T F LD we can translate 82 %.
			• Using the empirical heuristic function we can translate 96 % of the sentences with A* search.
			This heuristic function does not guarantee to avoid search errors, but this case never occurred in our experiments.
			From these results we conclude that it is often possible to faster compute acceptable results using a beam search approach.
			Therefore, this is the method of choice in practice.
			From a theoretical viewpoint it is interesting that using A* it is possible to translate guaranteed without search errors.
			In addition, without having a chance to perform search without search errors it is almost impossible to assess if errors in translation should be assigned to the model/training or to the search heuristics.
			Therefore, the A* algorithm is especially useful during the development of a statistical machine translation system.
	
	
			This paper is based on work supported partly by the VE R B M O B I L project (contract number 01 IV 701 T4) by the German Federal Min-.
			istry of Education, Science, Research and Technology.
			In addition, this work was supported by the National Science Foundation under Grant No.
			IIS9820687 through the 1999 Workshop on Language Engineering, Center for Language and Speech Processing, Johns Hopkins University.
	

