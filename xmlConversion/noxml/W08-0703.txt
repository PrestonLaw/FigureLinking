
	
		A stochastic approach to learning phonology.
		The model presented captures 715% more phonologically plausible underlying forms than a simple majority solution, because it prefers “pure” alternations.
		It could be useful in cases where an approximate solution is needed, or as a seed for more complex models.
		A similar process could be involved in some stages of child language acquisition; in particular, early learning of phonotactics.
	
	
			Sound changes in natural language, such as stem variation in inflected forms, can be described as phonological processes.
			These are governed by a constraint hierarchy as in Optimality Theory (OT), or by a set of ordered rules.
			Both rely on a single lexical representation of each morpheme (i.e., its underlying form), and context-sensitive transformations to surface forms.
			Phonological changes often affect segments near morpheme boundaries, but can also apply over an entire prosodic word, as in vowel harmony.
			It does not seem straightforward to incorporate context into a Bayesian model of phonology, although a clever solution may yet be found.
			A standard way of incorporating conditioning environments is to treat them as factors in a Gibbs model (Liang and Klein, 2007), but such models require an explicit calculation of the partition function.
			Unless the rule contexts possess some kind of locality, we don’t know how to compute this partition function efficiently.
			Some context could be captured by generating underlying phonemes from an n-gram model, or by annotating surface forms with neighborhood features.
			However, the effects of autosegmental phonology and other long-range dependencies (like vowel harmony) cannot be easily Bayesianized.
			1.1 Related Work.
			In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.
			A finite-state approximation of optimality theory (Karttunen, 1998) was later refined into a compact treatment of gradient constraints (Gerdemann and van Noord, 2000).
			Recent work on Bayesian models of morphological segmentation (Johnson et al., 2007) could be combined with phonological rule induction (Goldwater and Johnson, 2004) in a variety of ways, some of which will be explored in our discussion of future work.
			Also, the Hierarchical Bayes Compiler (Daume III, 2007) could be used to generate a model similar to the one presented here, but less con- strained1 which makes correspondingly more random, less accurate predictions.
			1.2 Dataset.
			As we describe the model and its implementation in this and subsequent sections, we will refer to a sam 1 Recent updates to HBC, inspired by discussions with the author, have addressed some of these limitations.
			12 Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 12–19, Columbus, Ohio, USA June 2008.
			Qc 2008 Association for Computational Linguistics ple dataset (in Figure 1), consisting of a paradigm2 of verb stems and person/number suffixes.
			The head of each row or column is an /underlying/ form, which in 3rd person singular is a phonologically null segment (represented as /ø/).
			In [surface] forms, the realization of each morpheme is affected by phonological processes.
			For example, in the combination of /tieta¨/ + /vat/, the result is [tieta¨+va¨t], where the 3rd person plural /a/ becomes [a¨] due to vowel harmony.
			1.3 Bayesian Approach.
			As a baseline model, we select the most frequently occurring allophone as the underlying form.
			Our goal is to outperform this baseline using a Bayesian model.
			In other words, what patterns in phonological processes can be inferred with such a statistical model?
			This simple framework begins learning with the assumption that the underlying forms are faithful to the surface (i.e., without considering markedness or phonotactics).
			We model the generation of surface forms from underlying ones on the segmental (character) level.
			Input is an inflectional paradigm, with tokens of the form stem+suffix.
			Morphology is limited to a single suffix (no agglutination), and is already identified.
			Each character of an underlying stem or suffix (ui ) generates surface characters (sij ) in an entire row or column of the input.
			To capture the phonology of a variety of languages with a single model, we need constraints from linguistically plausible priors (universal grammar).
			We prefer that underlying characters be preserved in surface forms, especially when there is no alternation.
			It is also reasonable that there be fewer underlying forms (phonemes) than surface forms (phones, phonetic inventory), to account for allo- phones.
			We expect to be able to capture a significant subset of phonological processes using a simple model (only faithfulness constraints).
			1.4 Pure Generators.
			Our model has an advantage over the baseline in its preference for “purity” in underlying forms.
			Each underlying segment should generate as few distinct 2 The paradigm format lends itself to analysis of word types, but if supplemented with surface counts, can also handle tokens.
			surface segments as possible: if it generates non- alternating (identical) segments, it will be less likely to generate an alternation in addition.
			This means that when two segments alternate, the underlying form should be the one that appears less frequently in other contexts, irrespective of the majority within the alternation.
			In the first stem of our Finnish verb conjugation (Figure 1), we see a [t,d] alternation (a case of consonant gradation), as well as unalternating [t].
			If we isolate three of the surface forms where /tieta¨/ is inflected (1st person singular, and 3rd person singular and plural), and consider only the dental segments in the stem of each, we have two underlying segments.
			Here, we use question marks to indicate unknown underlying segments.
			/??/ [dt] [tt] [tt] In this subset of the data, the reasonable candidate underlying forms are /t/ and /d/.
			These two compete to explain the observed data (surface forms).
			The nature of the prior probability distribution determines whether the majority is hypothesized for each underlying form, so /t/ produces both alternating and unalternating surface segments, or /d/ is hypothesized as the source of the alternation (and /t/ remains “pure”).
			In a Bayesian setting, we impose a sparse prior over underlying forms conditioned on the surface forms they generate.If u2 is hypothesized to be /t/, the posterior prob ability of u1 being /t/ goes down: P (u1 = /t/|u2 = /t/) < P (u1 = /t/) The probability of u1 being the competitor, /d/, correspondingly increases: P (u1 = /d/|u2 = /t/) > P (u1 = /d/) Even though the majority in this case would be /t/, the favored candidate for the alternating form was /d/.
			This happened because of how we defined the model’s prior, in combination with the evidence that /t/ (assigned to u2) generated the sequence of [t].
			So selection bias prefers /d/ as the source of an ambiguous segment, leaving /t/ to always generate itself.
			A similar effect can occur if there are both unalternating [t]’s and [d]’s on the surface, in addition to the [t,d] alternation.
			The candidate (/t/ or /d/) that is ❛ S Figure 1: Sample dataset (constructed by hand): Finnish verbs, with inflection for person and number.
			generating fewer unalternating segments is preferred to explain the alternation.
			For example, if there were 1000 cases of [t], 500 [d] and 500 [t,d], we would expect the following hypotheses: /t/ → [t], /d/ → [d] and /d/ → [t, d].
			This is because one of the two candidates must be responsible for both unalternating and alternating segments, but we prefer to have as much “‘purity” as possible, to minimize ambiguity.
			With this solution, we still have 1000 pure /t/ → [t], and only the 500 /d/ → [d] are now indistinct from /d/ → [t, d].
			If we had selected /t/ as the source of the alternation, there would be only 500 remaining “pure” (/d/) segments, and 1500 ambiguous /t/.
			Our Bayesian model should prefer the less prior over underlying form encourages sparse solutions, so βu < 1 for all u. The prior over surface form given underlying encourages identity mapping, /x/ → [x], so αxx > 1, and discourages different segments, /x/ → [y], so αxy < 1 for all x /= y. β α φ θ ambiguous (“purer”) solution, given an appropriate nc prior.
	
	
			We will use boldface to indicate vectors, and subscripts to identify an element from a vector or matrix.
			The variable N(u) is a vector of observed counts with the current underlying form hypothe ses.
			The notation we use for a vector u with one u s mnu nuFigure 2: Bayesian network: α and β are vectors of hy element i removed is u−i, so we can exclude the counts associated with a particular underlying form perparameters, and θi (for i ∈ {1, . . .
			, nc}) and φ are by indicating that in the parenthesized variable (i.e., N(u−4) is all the counts except those associated with the fourth underlying form).
			Ni (u) is the number of times character i is used as an underlying form, and Nij (u) is the number of times character i generated surface character j. The priors over surface s and underlying u segments in Figure 2 are captured by Dirichlet priors α and β, which generate the multinomial distributions θ and φ, respectively (see Figure 3).
			The distributions.
			u is a vector of underlying forms, generated from φ, and si (for i ∈ nu) is a set of observed surface forms generated from the hidden variable ui according to θi Phones and phonemes are drawn from a set of characters (e.g., IPA, unicode) C used to represent them.
			φi is the probability of a character (Ci for i ∈ nc) being an underlying form, irrespective of current alignments or its position in the paradigm.θij is the conditional probability of a surface char θc | α ∼ DIR(α), c = 1, . . .
			, nc φ | β ∼ DIR(β) ui | φi ∼ MU LTI(φi), i = 1, . . .
			, nu sij | ui , θui ∼ MU LTI(θ ui ), i = 1, . . .
			, nu, j = 1, . . .
			, mi Figure 3: Model parameters: nc is # different segments, nu is # underlying segments acter (skn = Cj for j ∈ nc, n ∈ mk ) given the underlying character it is generated from (uk = Cifor i ∈ nc, k ∈ nu), which is determined by its po sition in the paradigm.
			In our Finnish example (Figure 1), if k = 1, we are looking at the first underlying character, which is /t/ (from /tieta¨/), so assuming our character set is the Finnish alphabet, of which ‘t’ is the 20th char put in the paradigm.
			Rather than reestimate θ and φ at each iteration before sampling from u, we can marginalize these intermediate probability distributions in order to ease implementation and speed convergence.
			Our search procedure tries to sample from the posterior probability, according to Bayes’ rule.
			posterior ∝ likelihood ∗ prior P (u, s|β, α) ∝ P (u|β)P (s, u|α) Each of these probabilities is drawn from a Dirichlet distribution, which is defined in terms of the multivariate Beta function, C . The prior β added to underlying counts N(u) forms the posterior Dirichlet corresponding to P (u|β).
			In P (s|u, α), each αi vector is supplemented by the observed counts of (si).
			acter, u1 = C20 = t. It generates the first character of each inflected form (1st, 2nd, 3rd person, singu (underlying, surface) pairs N (β + N (u)) lar and plural) of that stem, so m1 = 6, and since there is no alternation s1n = t (for n ∈ {1, . . .
			, 6}).
			C P (u, s|β, α) = C (β)Given the phonologically plausible (gold) underly nc C (αc + i:ui =c N (si)) ing forms, the probability of /t/ is φ20 = 7/41.On the other hand, k = 33 identifies the 3rd per c=1 C (α) son singular /ø/, which inflects each of the seven stems, so m33 = 7.
			Since we need our alphabet to identify a null character, we’ll give it index zero (i.e., u33 = C0 = ø).
			For each of the (underlying, surface) alignments in this alternation (caused by vowel gemination), we can identify the probability in θ.
			For 3rd person singular [tieta¨+a¨], where s33,1 = C28 = a¨, the conditional probability θ0,28 = 1/7.
			The prior hyperparameters can be understood as follows.
			As βi gets smaller, an underlying form ukThe collapsed update procedure consists of re sampling each underlying form, u, incorporating the prior hyperparameters α, β and counts N over the rest of the dataset.
			The relevant counts for a candidate k being the underlying form ui are Nk (u−i) and Nksij (u−i) for j ∈ mi.
			P (ui = k|u−i, α, β) is proportional to the probability of generating ui = k, given the other u−i and all sij (for j ∈ mi), given s−i and u−i. Nc(u−i ) + βcis less likely to be Ci.
			As αij gets smaller, an un P (ui = c|u−i, α, β) ∝ n − 1 + β• derlying uk = Ci is less likely to generate a surface segment skn = Cj ∀n ∈ mk . In our experiments, C (α + i = i:u =c N (s! ) + N (si))we will vary αi=j (prior over identity map from un C (α + i =i:u =c N (s! )) derlying to surface) and αi =j . Our implementation of this model uses Gibbs sampling (c.f., (Bishop, 2006), pp 5428), an algorithm that produces samples from the posterior distribution.
			Each sample is an assignment of the hidden variables, u (i.e., a set of hypothesized underlying forms).
			Our sampler initializes u from a uniform distribution over segments in the training data, and resamples underlying forms in a fixed order, as in Suppose we were updating this sampler running on the Finnish verb inflections.
			If we had all segments as in Figure 1, but wanted to resample u31 (1st person singular /n/), we would consider the counts N excluding that form (i.e., under u−31).
			The prior for /n/, β14, is fixed, and there are no other occurrences, so N14(u−31) = 0.
			Another potential underlying form, like /t/, would have higher unconditioned posterior probability, because of the counts (7, in this case) added to its prior from β.
			Then, we have to multiply by the probability of each generated surface segment (all are [n], so 7 ∗ P ([n]|c, α) for a given hypothesis u31 = c).
			We select a given character c ∈ C for u31 from a distribution at random.
			Depending on the prior, /n/ will be the most likely choice, but other values are 3.2 Convergence.
			Testing convergence, we run again on the sample data (Figure 1), using αij = 0.1 when i /= j and 10 when i = j and β = 0.1, starting from different initializations, we get the same solution.
			6 x 10 still possible with smaller probability.
			The counts used for the next resampling, N (u−31), are affected by this choice, because the new identity of u31 has contributed to the posterior distribution.
			After unbounded iterations, Gibbs sampling is guaranteed to converge and produce samples from the true posterior (Geman and Geman, 1984).
	
	
			This model provides a language agnostic solution to a subset of phonological problems.
			We will first examine performance on the sample Finnish data (from Figure 1), and then look more closely at the issue of convergence.
			Finally, we present results from larger corpora 3 . 3.1 Finnish.
			Output from a trial run on Finnish verbs (from Figure 1) follows, with hyperparameters αij {100 ⇐⇒ i = j, 0.05 ⇐⇒ i /= j} and βi = {0.1}.
			In the paradigm (a sample after 1000 iterations), each [sur+face] form is followed by its hypothesized /under/ + /lying/ morphemes.
			[tieda¨+n] : /tieda¨/ + /n/ [tieda¨+t] : /tieda¨/ + /t/ [tieta¨+a¨] : /tieda¨/ + /a¨/ [tieda¨+mme] : /tieda¨/ + /mme/ [tieda¨+tte] : /tieda¨/ + /tte/ [tieta¨+va¨t] : /tieda¨/ + /va¨t/ [aiøo+n] : /aiøo/ + /n/ ...
			[pelka¨a¨+va¨t] : /pelka¨a¨/ + /vat/ With strong enough priors (faithfulness constraints), our sampler often selects the most common surface form aligned with an underlying segment.
			Although [vat] is more common than [va¨t], we choose the latter as the purer underlying form.
			So /a/ is always [a], but /a¨/ can be either [a¨] or [a].
			3 2.8 million word types from Morphochallenge2007 (Kurimo et al., 2007) 3.05 3.04 3.03 3.02 3.01 3 2.99 2.98 2.97 0 10 20 30 40 50 60 70 80 90 100 Iteration Figure 4: Posterior likelihood at each of the first 100 iterations, from 4 runs (with different random seeds) on 10% of the Morphochallenge dataset (αi/=j = 0.001, αi=j = 100, β = 0.1), indicating convergence within the first 15 iterations.
			To confirm that the sampler has converged, we output and plot trace statistics at each iteration, including marginal probability, log likelihood, and changes in underlying forms (i.e., variables resam- pled).
			If the sampler has converged, there should no longer be a trend (consistent slope) in any of these statistics (as in Figure 4).
			Examining the posterior probability of each selected underlying form reveals interesting patterns that also help explain the variation.
			In the above run, the ambiguous segments (with surface alternations) were drawn from the distributions (with improbable segments elided) in Figure 5.
			We expect this model to maximize the probability of either the “majority” solution or a solution demonstrating selection bias.
			We compare likelihood of the posterior sample with that of a “phono- logically plausible” solution (in which underlying forms are determined by referring to formal linguistic accounts of phonological derivation) and a “majority solution” (see Figure 6 for a log-log plot, where lower is more likely).
			The posterior sample has optimal likelihood with each parameter setting, as expected.
			The majority parse is selected with αi=j = 0.5 With lower values of αi=j , the “phonologically plausible” parse is u4=/d/ s4=[d,d,t,d,d,t] P (ui = c) ≈ d 0.99968 t 0.00014 u8=/k/ s8 =[ø,ø,k,ø,ø,k] (same behavior as u12) P (ui = c) ≈ ø 0.642 k 0.124 u33=/e/ s33=[a¨,o,e,u,ø,e,ø] P (ui = c) ≈ a ¨ , o , u 0.0 02 9 ø 0.2 15 a 0.0 00 3 e 0.2 97 Figure 5: Resampling probabilities for alternations, after 1000 iterations.
			posterior sample majority solution phonologically plausible 4.26 10 4.25 10 M ajo rit y Ba ye sia n ty p e s 5 0 . 8 4 6 9 . 5 3 tok en s 6 5 . 2 3 7 2 . 1 1 Figure 7: Accuracy of underlying segment hypotheses.
			notated morphemes.
			For example, the word ajavalle is listed in the corpus as follows: ajavalle aja:ajaa|V va:PCP1 lle:ALL The word is segmented into a verb stem, ‘aja’ (drive), a present participle marker ‘va’, and the allative suffix (“for”).
			Each surface realization of a given morpheme is identified by the same tag (e.g., PCP1).
			However, in this corpus, insertion and deletion are not explicitly marked (as they were in the paradigm, by ø).
			Rather than introduce another component to determine which segments in the form were dropped, we ignore these cases.
			The sampling algorithm proceeds as described in section 2.
			To run on tokens (as opposed to types), we incorporate another input file that contains counts from the original text (ajavalle appeared 8 times).
			The counts of each morpheme’s surface forms then reflect the number of times that form appeared in any word in the corpus.
			4.24 10 −2 −1 10 10 alpha Figure 6: Parse likelihood 3.
			3. 1 T y p e o r T o k e n 0 In Finnish verb conjugation, 3rd person (esp. sin gular) forms have high frequency and tend to be unmarked (i.e., closer to underlying).
			In types, unmarked is a minority (one third), but incorporating token frequency shifts that balance, benefitingmore likely than the majority.
			However, the sam pler does not converge to this solution, because in this [t,d] alternation, the “phonologically plausible” solution identifies /t/, but neither selection bias nor majority rules would lead to that with the given data.
			3.3 Morphologically segmented corpora.
			In our search for appropriate data for additional, larger-scale experiments, we found several viable alternatives.
			The correct morphological segmentations for Finnish data used in Morphochallenge2007 (Kurimo et al., 2007) provide a rich and varied set of words, and are readily analyzable by our sampler.
			Rather than associating each surface form with a position in the paradigm, we use the an the “majority learner.” Among noun inflections, unmarked has higher frequency in speech, but marked tokens may still dominate in text.
			We might expect that it is easier to learn from tokens than types, in part because more data is often helpful.
			Testing on half of the Morphochallenge 2007 Finnish data (1M word types, 5M morph types, 17.5M word tokens, 48M morph tokens), we ran both our Bayesian model and a majority solver on the morphological analyses, and compared against phonologically plausible (gold) underlying forms.
			Results are reported in Figure 7.
			The Bayesian estimate consistently outperformed the majority solution, and cases where the two differ could often be ascribed to the preference for “pure” analyses.
	
	
			We have described a model where surface forms are generated from underlying representations segment by segment.
			Taking this approach allowed us to investigate the properties of a Bayesian statistical learner, and how these can be useful in the context of sound systems, a basic component of language.
			Experiments with our implementation of a collapsed sampler have produced results largely confirming our hypotheses.
			Without context, we can often learn about 60 to 80 percent of the mapping from underlying phonemes to surface phones.
			Especially with lower values of αi=j , closer to 0, our model does prefer pure alternations.
			Gibbs sampling tends to select the majority underlying form, particularly with αi=j relatively high, closer to 1.
			So, a sparser prior leads us further from the baseline, and often closer to a phonologically plausible solution.
			4.1 Directions.
			In future research, we hope to integrate morphological analysis into this sort of a treatment of phonology.
			This is a natural approach for children learning their first language.
			They intuitively discover phonotactics, and how it affects the prosodic shape of each word, as they learn meaningful units and compose them together.
			It is clear that many layers of linguistic information interact in the early stages of child language acquisition (Demuth and Ellis, 2005 in press), so they should also interact in our models.
			As discussed above, the present model should be applicable to analysis of language- learners’ speech errors, and this connection should be explored in greater depth.
			It might be interesting to predispose the sampler to select underlying forms from open syllables.
			That is, set α to increase the probability of matching one of the surface segments if its context (feature annotations) includes a vocalic segment or a word boundary immediately following.
			The probability of phonological processes like assimilation could be similarly modeled, with the prior higher for choosing a segment that appears on the surface in a con- trastive context (where it shares few features with neighboring segments).
			If we define a MaxEnt distribution over Optimal- ity Theoretic constraints, we might use that to inform our selection of underlying forms.
			In (Goldwater and Johnson, 2003), the learning algorithm was given a set of candidate surface forms associated with an underlying form, and tried to optimize the constraint weights.
			In addition to the constraint weights, we must also optimize the underlying form, since our goal is to take as input only observable data.
			Sampling from this type of complex distribution is quite difficult, but some approaches (e.g., (Murray et al., 2006)) may help reduce the intractability.
	
