
	
		We investigate the problem of training probabilistic context-free grammars on the basis of a distribution defined over an infinite set of trees, by minimizing the cross-entropy.
		This problem can be seen as a generalization of the well-known maximum likelihood estimator on (finite)tree banks.
		We prove an unexpected theoretical property of grammars that are trained in this way, namely, we showthat the derivational entropy of the grammar takes the same value as the cross- entropy between the input distribution andthe grammar itself.
		We show that the result also holds for the widely applied maximum likelihood estimator on tree banks.
	
	
			Probabilistic context-free grammars are able to describe hierarchical, tree-shaped structures underlying sentences, and are widely used in statistical nat ural language processing; see for instance (Collins,2003) and references therein.
			Probabilistic contextfree grammars seem also more suitable than finite state devices for language modeling, and several language models based on these grammars havebeen recently proposed in the literature; see for in stance (Chelba and Jelinek, 1998), (Charniak, 2001) and (Roark, 2001).
			Empirical estimation of probabilistic context-free grammars is usually carried out on tree banks, thatis, finite samples of parse trees, through the max imization of the likelihood of the sample itself.
			It is well-known that this method also minimizes the cross-entropy between the probability distribution induced by the tree bank, also called the empiricaldistribution, and the tree probability distribution in duced by the estimated grammar.In this paper we generalize the maximum like lihood method, proposing an estimation techniquethat works on any unrestricted tree distribution de fined over an infinite set of trees.
			This generalizationis theoretically appealing, and allows us to prove unexpected properties of the already mentioned maxi mum likelihood estimator for tree banks, that were not previously known in the literature on statisticalnatural language parsing.
			More specifically, we investigate the following information theoretic quanti ties the cross-entropy between the unrestricted treedistribution given as input and the tree distri bution induced by the estimated probabilistic context-free grammar; and  the derivational entropy of the estimated probabilistic context-free grammar.
			These two quantities are usually unrelated.
			We show that these two quantities take the same value whenthe probabilistic context-free grammar is trained us ing the minimal cross-entropy criterion.
			We thentranslate back this property to the method of maximum likelihood estimation.
			Our general estima tion method also has practical applications in casesone uses a probabilistic context-free grammar to ap proximate strictly more powerful rewriting systems, 335as for instance probabilistic tree adjoining gram mars (Schabes, 1992).
			Not much is found in the literature about the estimation of probabilistic grammars from infinite distributions.
			This line of research was started in (Nederhof, 2005), investigating the problem of training an input probabilistic finite automaton from an infinite tree distribution specified by means of aninput probabilistic context-free grammar.
			The prob lem we consider in this paper can then be seen asa generalization of the above problem, where the in put model to be trained is a probabilistic context-free grammar and the input distribution is an unrestricted tree distribution.
			In (Chi, 1999) an estimator thatmaximizes the likelihood of a probability distribu tion defined over a finite set of trees is introduced,as a generalization of the maximum likelihood es timator.
			Again, the problems we consider here can be thought of as generalizations of such estimator to the case of distributions over infinite sets of trees or sentences.The remainder of this paper is structured as fol lows.
			Section 2 introduces the basic notation anddefinitions and Section 3 discusses our new estimation method.
			Section 4 presents our main re sult, which is transferred in Section 5 to the methodof maximum likelihood estimation.
			Section 6 dis cusses some simple examples, and Section 7 closes with some further discussion.
	
	
			Throughout this paper we use standard notation and definitions from the literature on formal languagesand probabilistic grammars, which we briefly sum marize below.
			We refer the reader to (Hopcroft and Ullman, 1979) and (Booth and Thompson, 1973) for a more precise presentation.A context-free grammar (CFG) is a tuple G =(N,S,R, S), where N is a finite set of nonterminalsymbols, S is a finite set of terminal symbols dis joint from N , S ? N is the start symbol and R is afinite set of rules.
			Each rule has the form A ? a,where A ? N and a ?
			(S ? N)*.
			We denote byL(G) and T (G) the set of all strings, resp., trees,generated by G. For t ? T (G), the yield of t isdenoted by y(t).
			For a nonterminal A and a string a, we write f(A,a) to denote the number of occurrences of Ain a. For a rule (A? a) ? R and a tree t ? T (G),f(A ? a, t) denotes the number of occurrences ofA?
			a in t. We let f(A, t) = ?a f(A? a, t).
			A probabilistic context-free grammar (PCFG) is a pair G = (G, pG), with G a CFG and pG a function from R to the real numbers in the interval [0, 1].A PCFG is proper if for every A ? N we have?
			a pG(A ? a) = 1.
			The probability of t ? T (G)is the product of the probabilities of all rules in t, counted with their multiplicity, that is, pG(t) =?
			A?a pG(A? a)f(A?a,t).
			(1) The probability of w ? L(G) is the sum of the probabilities of all the trees that generate w, that is, pG(w) =?
			y(t)=w pG(t).
			(2) A PCFG is consistent if?
			t?T (G) pG(t) = 1.In this paper we write log for logarithms in base 2and ln for logarithms in the natural base e. We alsoassume 0  log 0 = 0.
			We write Ep to denote theexpectation operator under distribution p. In case Gis proper and consistent, we can define the deriva tional entropy of G as the expectation of the information of parse trees in T (G), computed under distribution pG, that is, Hd(pG) = EpG log1 pG(t) = -?
			t?T (G) pG(t)  log pG(t).
			(3) Similarly, for each A ? N we also define the non-terminal entropy of A as HA(pG) = = EpG log1 pG(A? a) = -?
			a pG(A? a)  log pG(A? a).
			(4)
	
	
			Let T be an infinite set of (finite) trees with internal nodes labeled by symbols in N , root nodes la beled by S ? N and leaf nodes labeled by symbols 336in S. We assume that the set of rules that are ob served in the trees in T is drawn from some finite set R. Let pT be a probability distribution defined over T , that is, a function from T to set [0, 1] such that?t?T pT (t) = 1.The skeleton CFG underlying T is defined asG = (N,S,R, S).
			Note that we have T ? T (G)and, in the general case, there might be trees in T (G)that do not appear in T . We wish anyway to approx imate distribution pT the best we can, by turning G into some proper PCFG G = (G, pG) and setting parameters pG(A ? a) appropriately, for each(A? a) ? R.One possible criterion is to choose pG in such a way that the cross-entropy between pT and pG is minimized, where we now view pG as a probabilitydistribution defined over T (G).
			The cross-entropybetween pT and pG is defined as the expectation under distribution pT of the information, computed un der distribution pG, of the trees in T (G) H(pT || pG) = EpT log1 pG(t) = -?
			t?T pT (t)  log pG(t).
			(5) Since G should be proper, the minimization of (5) issubject to the constraints ?a pG(A ? a) = 1, for each A ? N .To solve the minimization problem above, we use Lagrange multipliers ?A for each A ? N and definethe form ? =?
			A?N ?A  (?
			a pG(A? a)- 1) + -?
			t?T pT (t)  log pG(t).
			(6) We now view ? as a function of all the ?A and thepG(A ? a), and consider all the partial derivativesof ?.
			For each A ? N we have ??
			??A=?
			a pG(A? a)- 1.
			For each (A? a) ? R we have ??
			?pG(A? a)= = ?A -?
			?pG(A? a) ? t?T pT (t)  log pG(t) = ?A -?
			t?T pT (t) ?
			?pG(A? a)log pG(t) = ?A -?
			t?T pT (t) ?
			?pG(A? a) log?
			(B?)?R pG(B ? )f(B?,t) = ?A -?
			t?T pT (t) ?
			?pG(A? a)?
			(B?)?R f(B ? , t)  log pG(B ? ) = ?A -?
			t?T pT (t) ?
			(B?)?R f(B ? , t)  ? ?pG(A? a)log pG(B ? ) = ?A -?
			t?T pT (t)  f(A? a, t)  1 ln(2) 1 pG(A? a) = ?A -1 ln(2) 1 pG(A? a) ?
			t?T pT (t)  f(A? a, t) = ?A -1 ln(2) 1 pG(A? a) EpT f(A? a, t).We now need to solve a system of |N |+ |R| equations obtained by setting to zero all of the above par tial derivatives.
			From each equation ???pG(A?a) = 0we obtain ?A  ln(2)  pG(A? a) = = EpT f(A? a, t).
			(7) We sum over all strings a such that (A? a) ? R ?A  ln(2) ?
			a pG(A? a) = =?
			a EpT f(A? a, t) =?
			a ? t?T pT (t)  f(A? a, t) =?
			t?T pT (t) ?
			a f(A? a, t) =?
			t?T pT (t)  f(A, t) = EpT f(A, t).
			(8) 337 From each equation ????A= 0 we obtain?a pG(A ? a) = 1 for each A ? N (our origi nal constraints).
			Combining with (8) we obtain ?A  ln(2) = EpT f(A, t).
			(9) Replacing (9) into (7) we obtain, for every rule (A? a) ? R, pG(A? a) =EpT f(A? a, t) EpT f(A, t).
			(10) The equations in (10) define the desired estimator for our PCFG, assigning to each rule A?
			a a probability specified as the ratio between the expected number of A ? a and the expected number of A,under the distribution pT . We remark here that theminimization of the cross-entropy above is equivalent to the minimization of the KullbackLeibler distance between pT and pG, viewed as tree distribu tions.
			Also, note that the likelihood of an infinite set of derivations would always be zero and therefore cannot be considered here.
			To be used in the next section, we now show that the PCFG G obtained as above is consistent.
			Theline of our argument below follows a proof providedin (Chi and Geman, 1998) for the maximum like lihood estimator based on finite tree distributions.
			Without loss of generality, we assume that in G thestart symbol S is never used in the right-hand side of a rule.
			For each A ? N , let qA be the probability that aderivation in G rooted in A fails to terminate.
			Wecan then write qA =?
			B?N qB ?
			a pG(A? a)f(B,a).(11) The inequality follows from the fact that the eventsconsidered in the right-hand side of (11) are not mu tually exclusive.
			Combining (10) and (11) we obtain qA  EpT f(A, t) = =?
			B?N qB ?
			a EpT f(A? a, t)f(B,a).
			Summing over all nonterminals we have?
			A?N qA  EpT f(A, t) = =?
			B?N qB ?
			A?N ? a EpT f(A? a, t)f(B,a) =?
			B?N qB  EpT fc(B, t), (12) where fc(B, t) indicates the number of times a nodelabeled by nonterminal B appears in the derivation tree t as a child of some other node.
			From our assumptions on the start symbol S, we have that S only appears at the root of the trees in T (G).
			Then it is easy to see that, for everyA 6= S, we have EpT fc(A, t) = EpT f(A, t), whileEpT fc(S, t) = 0 and EpT f(S, t) = 1.
			Using theserelations in (12) we obtain qS  EpT f(S, T ) = qS  EpT fc(S, T ), from which we conclude qS = 0, thus implying theconsistency of G.
	
	
			In this section we present the main result of the pa per.
			We show that, when G = (G, pG) is estimatedby minimizing the cross-entropy in (5), then suchcross-entropy takes the same value as the derivational entropy of G, defined in (3).In (Nederhof and Satta, 2004) relations are derived for the exact computation ofHd(pG).
			For lateruse, we report these relations below, under the as sumption that G is consistent (see Section 3).
			Wehave Hd(pG) =?
			A?N outG(A) HA(pG).
			(13) Quantities HA(pG), A ? N , have been definedin (4).
			For eachA ? N , quantity outG(A) is the sumof the probabilities of all trees generated by G, having root labeled by S and having a yield composed of terminal symbols with an unexpanded occurrence of nonterminal A. Again, we assume that symbol S does not appear in any of the right-hand sides of the rules in R. This means that S only appears at the root of the trees in T (G).
			Under this condition, quantities outG(A) can be exactly computedby solving the following system of linear equations (see also (Nederhof, 2005)) outG(S) = 1; (14) for each A 6= S outG(A) = =?
			B? outG(B)  f(A, )  pG(B ? ).(15) 338 We can now prove the equality Hd(pG) = H(pT || pG), (16) where G is the PCFG estimated by minimizing thecross-entropy in (5), as described in Section 3.
			We start from the definition of cross-entropy H(pT || pG) = = -?
			t?T pT (t)  log pG(t) = -?
			t?T pT (t)  log?
			A?a pG(A? a)f(A?a,t) = -?
			t?T pT (t)  ?
			A?a f(A? a, t)  log pG(A? a) = -?
			A?a log pG(A? a)  ?
			t?T pT (t)  f(A? a, t) = -?
			A?a log pG(A? a)  EpT f(A? a, t).
			(17) From our estimator in (10) we can write EpT f(A? a, t) = = pG(A? a)  EpT f(A, t).
			(18) Replacing (18) into (17) gives H(pT || pG) = = -?
			A?a log pG(A? a)  pG(A? a)  EpT f(A, t) = -?
			A?N EpT f(A, t)  ?
			a pG(A? a)  log pG(A? a) =?
			A?N EpT f(A, t) H(pG, A).
			(19) Comparing (19) with (13) we see that, in order to prove the equality in (16), we need to show relations EpT f(A, t) = outG(A), (20) for every A ? N . We have already observed in Section 3 that, under our assumption on the start symbol S, we have EpT f(S, t) = 1.
			(21) We now observe that, for any A ? N with A 6= Sand any t ? T (G), we have f(A, t) = =?
			B? f(B ? , t)  f(A, ).
			(22) For each A ? N with A 6= S we can then write EpT f(A, t) = =?
			t?T pT (t)  f(A, t) =?
			t?T pT (t) ?
			B? f(B ? , t)  f(A, ) =?
			B? ? t?T pT (t)  f(B ? , t)  f(A, ) =?
			B? EpT f(B ? , t)  f(A, ).
			(23) Once more we use relation (18), which replaced in (23) provides EpT f(A, t) = =?
			B? EpT f(B, t)  f(A, )  pG(B ? ).
			(24) Notice that the linear system in (14) and (15) and the linear system in (21) and (24) are the same.
			Thus we conclude that quantities EpT f(A, t) and outG(A)are the same for each A ? N . This completes ourproof of the equality in (16).
			Some examples will be discussed in Section 6.
			Besides its theoretical significance, the equality in (16) can also be exploited in the computation of the cross-entropy in practical applications.
			In fact, cross-entropy is used as a measure of tightness in comparing different models.
			In case of estimation from an infinite distribution pT , the definition of the cross-entropy H(pT || pG) contains an infinite summation, which is problematic for the computation of such quantity.
			In standard practice, this problem is overcome by generating a finite sample T (n) of largesize n, through the distribution pT , and then comput ing the approximation (Manning and Schutze, 1999) H(pT || pG) ~ -1 n ? t?T f(t, T (n))  log pG(t), 339 where f(t, T (n)) indicates the multiplicity, that is,the number of occurrences, of t in T (n).
			However, in practical applications n must be very large in order to have a small error.
			Based on the results in this section, we can instead compute the exact value of H(pT || pG) by computing the derivational entropyHd(pG), using relation (13) and solving the linearsystem in (14) and (15), which takes cubic time in the number of nonterminals of the grammar.
	
	
			In natural language processing applications, the estimation of a PCFG is usually carried out on the ba sis of a finite sample of trees, called tree bank.
			The so-called maximum likelihood estimation (MLE)method is exploited, which maximizes the likeli hood of the observed data.
			In this section we showthat the MLE method is a special case of the esti mation method presented in Section 3, and that the results of Section 4 also hold for the MLE method.
			Let T be a tree sample, and let T be the underlying set of trees.
			For t ? T , we let f(t, T ) be themultiplicity of t in T . We define f(A? a, T ) = =?
			t?T f(t, T )  f(A? a, t), (25) and let f(A, T ) =?
			a f(A ? a, T ).
			We can induce from T a probability distribution pT , definedover T , by letting for each t ? T pT (t) =f(t, T ) |T |.
			(26) Note that?
			t?T pT (t) = 1.
			Distribution pT is calledthe empirical distribution of T . Assume that the trees in T have internal nodes labeled by symbols in N , root nodes labeled by S and leaf nodes labeled by symbols in S. Let also R be the finite set of rules that are observed in T . We define the skeleton CFG underlying T asG = (N,S,R, S).
			In the MLE method we probabilistically extend the skeleton CFG G by means of a function pG that maximizes the likelihood of T ,defined as pG(T ) =?
			t?T pG(t)f(t,T ), (27) subject to the usual properness conditions on pG.
			Such maximization provides the estimator (see for instance (Chi and Geman, 1998)) pG(A? a) =f(A? a, T ) f(A, T ).
			(28) Let us consider the estimator in (10).
			If we replace distribution pT with our empirical distribution pT , we derive pG(A? a) = =EpT f(A? a, t) EpT f(A, t) = ?t?T f(t,T )|T |  f(A? a, t) ?t?T f(t,T )|T |  f(A, t) = ?t?T f(t, T )  f(A? a, t)?
			t?T f(t, T )  f(A, t) =f(A? a, T ) f(A, T ).
			(29) This is precisely the estimator in (28).
			From relation (29) we conclude that the MLE method can be seen as a special case of the generalestimator in Section 3, with the input distribution de fined over a finite set of trees.
			We can also derivethe well-known fact that, in the finite case, the maximization of the likelihood pG(T ) corresponds to theminimization of the cross-entropy H(pT || pG).Let now G = (G, pG) be a PCFG trained on T us ing the MLE method.
			Again from relation (29) and Section 3 we have that G is consistent.
			This resulthas been firstly shown in (Chaudhuri et al., 1983) and later, with a different proof technique, in (Chi and Geman, 1998).
			We can then transfer the resultsof Section 4 to the supervised MLE method, show ing the equality Hd(pG) = H(pT || pG).
			(30)This result was not previously known in the litera ture on statistical parsing of natural language.
			Some examples will be discussed in Section 6.
	
	
			In this section we discuss a simple example with theaim of clarifying the theoretical results in the previ ous sections.
			For a real number q with 0 < q < 1, 340 Figure 1: Derivational entropy of Gq and cross-entropies for three different corpora.
			consider the CFG G defined by the two rules S ?aS and S ? a, and let Gq = (G, pG,q) be the probabilistic extension of G with pG,q(S ? aS) = q andpG,q(S ? a) = 1 - q. This grammar is unambiguous and consistent, and each tree t generated by G has probability pG,q(t) = qi  (1 - q), where i = 0 is the number of occurrences of rule S ? aS in t.We use below the following well-known relations (0 < r < 1) +8?i=0 ri =1 1- r, (31) +8?i=1 i  ri1 =1 (1- r)2.
			(32) The derivational entropy of Gq can be directlycomputed from its definition as Hd(pG,q) = -+8?i=0 qi  (1- q)  log(qi  (1- q) ) = -(1- q)+8?i=0 qi log qi + -(1- q)  log(1- q) +8?i=0 qi = -(1- q)  log q +8?i=0 i  qi - log(1- q) = -q 1- q log q - log(1- q).
			(33) See Figure 1 for a plot of Hd(pG,q) as a functionof q. If a tree bank is given, composed of occurrencesof trees generated by G, the value of q can be es timated by applying the MLE or, equivalently, byminimizing the cross-entropy.
			We consider here sev eral tree banks, to exemplify the behaviour of thecross-entropy depending on the structure of the sam ple of trees.
			The first tree bank T contains a singletree t with a single occurrence of rule S ? aS anda single occurrence of rule S ? a. We then havepT (t) = 1 and pG,q(t) = q  (1 - q).
			The cross-entropy between distributions pT and pG,q is then H(pT , pG,q) = - log q  (1- q) = - log q - log(1- q).
			(34) The cross-entropy H(pT , pG,q), viewed as a function of q, is a convex-?
			function and is plotted inFigure 1 (line indicated by K d= 1, see below).
			We can obtain its minimum by finding a zero for the first derivative ddqH(pT , pG,q) = 1 q+ 1 1- q =2q1 q  (1- q)= 0, (35) which gives q = 0.5.
			Note from Figure 1 thatthe minimum of H(pT , pG,q) crosses the line corresponding to the derivational entropy, as should be expected from the result in Section 4.
			More in general, for integers d > 0 and K > 0,consider a tree sample Td,K consisting of d trees ti,1 = i = d. Each ti contains ki = 0 occurrencesof rule S ? aS and one occurrence of rule S ? a.Thus we have pTd,K (ti) = 1dand pG,q(ti) = q ki  (1- q).
			We let?d i=1 ki = K. The cross-entropy is H(pTd,K , pG,q) = = -d?i=0 1 d log qki - log(1- q) = -K dlog q - log(1- q).
			(36) In Figure 1 we plot H(pTd,K , pG,q) in the caseKd = 0.5 and in the case Kd = 1.5.
			Again, we have thatthese curves intersect with the curve corresponding to the derivational entropy Hd(pG,q) at the pointswere they take their minimum values.
			341
	
	
			We have shown in this paper that, when a PCFG isestimated from some tree distribution by minimiz ing the cross-entropy, then the cross-entropy takes the same value as the derivational entropy of the PCFG itself.
			As a special case, this result holds for the maximum likelihood estimator, widely applied in statistical natural language parsing.
			The resultalso holds for the relative weighted frequency esti mator introduced in (Chi, 1999) as a generalizationof the maximum likelihood estimator, and for the estimator introduced in (Nederhof, 2005) already dis cussed in the introduction.
			In a journal version of the present paper, which is under submission, we havealso extended the results of Section 4 to the unsupervised estimation of a PCFG from a distribution de fined over an infinite set of (unannotated) sentencesand, as a particular case, to the well-knonw inside outside algorithm (Manning and Schutze, 1999).
			In practical applications, the results of Section 4can be exploited in the computation of model tight ness.
			In fact, cross-entropy indicates how much theestimated model fits the observed data, and is com monly exploited in comparison of different models on the same data set.
			We can then use the givenrelation between cross-entropy and derivational en tropy to compute one of these two quantities from the other.
			For instance, in the case of the MLE method we can choose between the computation ofthe derivational entropy and the cross-entropy, de pending basically on the instance of the problem at hand.
			As already mentioned, the computation of thederivational entropy requires cubic time in the number of nonterminals of the grammar.
			If this num ber is large, direct computation of (5) on the corpus might be more efficient.
			On the other hand, if the corpus at hand is very large, one might opt for direct computation of (3).
	
	
			Helpful comments from Zhiyi Chi, Alberto lavelli,Mark-Jan Nederhof and Khalil Simaan are grate fully acknowledged.
	
