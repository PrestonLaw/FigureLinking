
	
	
			Words are the basic units to process for most N L P tasks.
			The problem of Chinese word segmentation (C W S) is to find these basic units for a given sentence, which is written as a continuous sequence of characters.
			It is the initial step for most Chinese processing applications.
			with contextual information outside the sentence.
			Human readers often use semantics, contextual information about the document and world knowledge to resolve segmentation ambiguities.
			There is no fixed standard for Chinese word segmentation.
			Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).
			Also, specific N L P tasks may require different segmentation criteria.
			For example, “ ” could be treated as a single word (Bank jing) for machine translation, while it is more natuChinese character sequences are ambiguous, of rally segmented into “ (Beijing) (bank)” ten requiring knowledge from a variety of sources for disambiguation.
			Out-of-vocabulary (O OV) words are a major source of ambiguity.
			For example, a difficult case occurs when an O OV word consists 840for tasks such as text-to-speech synthesis.
			There fore, supervised learning with specifically defined training data has become the dominant approach.
			Following Xue (2003), the standard approach to Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840–847, Prague, Czech Republic, June 2007.
			Qc 2007 Association for Computational Linguistics supervised learning for C W S is to treat it as a tagging task.
			Tags are assigned to each character in the sentence, indicating whether the character is a single- character word or the start, middle or end of a multi- character word.
			The features are usually confined to a five-character window with the current character in the middle.
			In this way, dynamic programming algorithms such as the Viterbi algorithm can be used for decoding.
			Several discriminatively trained models have recently been applied to the C W S problem.
			Examples include Xue (2003), Peng et al.
			(2004) and Shi and Wang (2007); these use maximum entropy (M E) and conditional random field (C R F) models (Ratnaparkhi, 1998; Lafferty et al., 2001).
			An advantage of these models is their flexibility in allowing knowledge from various sources to be encoded as features.
			Contextual information plays an important role in word segmentation decisions; especially useful is information about surrounding words.
			Consider the beam and the importance of word-based features.
			We compare the accuracy of our final system to the state-of-the-art C W S systems in the literature using the first and second S I G H A N bakeoff data.
			Our system is competitive with the best systems, obtaining the highest reported F-scores on a number of the bakeoff corpora.
			These results demonstrate the importance of word-based features for C W S. Furthermore, our approach provides an example of the potential of search-based discriminative training methods for N L P tasks.
	
	
			We formulate the C W S problem as finding a mapping from an input sentence x ∈ X to an output sentence y ∈ Y , where X is the set of possible raw sentences and Y is the set of possible segmented sentences.
			Given an input sentence x, the correct output segmentation F (x) satisfies: sentence “ ”, which can be from “ F (x) = arg max Score(y) (among w oreign) (compan -!!
			y∈GEN(x) or “ hich) (f h i n a ) ies)”, (for omp anies ) where GEN( x) deno tes the set of possi ble segmen ( ess)”.
			Note e five-character wind tations for an input sentence, consistent with nota busin that th ow x surrounding “ ” is the same in both cases, making tion from Collins (2002).
			the tagging d ion for that character difficult given the local window.
			However, the correct decision can be made by comparison of the two three-word windows containing this character.
			In order to explore the potential of word-based models, we adapt the perceptron discriminative learning algorithm to the C W S problem.
			Collins (2002) proposed the perceptron as an alternative to the C R F method for H M M-style taggers.
			However, our model does not map the segmentation problem to a tag sequence learning problem, but defines features on segmented sentences directly.
			Hence we use a beam-search decoder during training and testing; our idea is similar to that of Collins and Roark (2004) who used a beam-search decoder as part of a perceptron parsing model.
			Our work can also be seen as part of the recent move towards search-based learning methods which do not rely on dynamic programming and are thus able to exploit larger parts of the context for making decisions (Daume III, 2006).
			We study several factors that influence the performance of the perceptron word segmentor, including the averaged perceptron method, the size of the The score for a segmented sentence is computed by first mapping it into a set of features.
			A feature is an indicator of the occurrence of a certain pattern in a segmented sentence.
			For example, it can be the occurrence of “ ” as a single word, or the occurrence of “ ” separated from “ ” in two adjacent words.
			By defining features, a segmented sentence is mapped into a global feature vector, in which each dimension represents the count of a particular feature in the sentence.
			The term “global” feature vector is used by Collins (2002) to distinguish between feature count vectors for whole sequences and the “local” feature vectors in M E tagging models, which are Boolean valued vectors containing the indicator features for one element in the sequence.
			Denote the global feature vector for segmented sentence y with Φ(y) ∈ Rd, where d is the total number of features in the model; then Score(y) is computed by the dot product of vector Φ(y) and a parameter vector α ∈ Rd, where αi is the weight for the ith feature: Score(y) = Φ(y) · α Inputs: training examples (xi, yi) Initialization: set α = 0 Algorithm: for t = 1..T , i = 1..N calculate zi = arg maxy∈GEN(xi ) Φ(y) · α if zi 1= yi α = α + Φ(yi) − Φ(zi) Outputs: α Figure 1: the perceptron learning algorithm, adapted from Collins (2002) The perceptron training algorithm is used to determine the weight values α.
			The training algorithm initializes the parameter vector as all zeros, and updates the vector by decoding the training examples.
			Each training sentence is turned into the raw input form, and then decoded with the current parameter vector.
			The output segmented sentence is compared with the original training example.
			If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.
			The algorithm can perform multiple passes over the same training sentences.
			Figure 1 gives the algorithm, where N is the number of training sentences and T is the number of passes over the data.
			Note that the algorithm from Collins (2002) was designed for discriminatively training an H M M-style tagger.
			Features are extracted from an input sequence x and its corresponding tag sequence y: Score(x, y) = Φ(x, y) · α Our algorithm is not based on an H M M. For a given input sequence x, even the length of different candidates y (the number of words) is not fixed.
			Because the output sequence y (the segmented sentence) contains all the information from the input sequence x (the raw sentence), the global feature vector Φ(x, y) is replaced with Φ(y), which is extracted from the candidate segmented sentences directly.
			Despite the above differences, since the theorems 2.1 The averaged perceptron.
			The averaged perceptron algorithm (Collins, 2002) was proposed as a way of reducing overfitting on the training data.
			It was motivated by the voted- perceptron algorithm (Freund and Schapire, 1999) and has been shown to give improved accuracy over the non-averaged perceptron on a number of tasks.
			Let N be the number of training sentences, T the number of training iterations, and αn,t the parameter vector immediately after the nth sentence in the tth iteration.
			The averaged parameter vector γ ∈ Rd is defined as: 1 αn,t N T n=1..N,t=1..T To compute the averaged parameters γ, the training algorithm in Figure 1 can be modified by keep ing a total parameter vector σn,t = I: αn,t, which is updated using α after each training example.
			After the final iteration, γ is computed as σn,t/N T . In the averaged perceptron algorithm, γ is used instead of α as the final parameter vector.
			With a large number of features, calculating the total parameter vector σn,t after each training example is expensive.
			Since the number of changed dimensions in the parameter vector α after each training example is a small proportion of the total vector, we use a lazy update optimization for the training process.1 Define an update vector τ to record the number of the training sentence n and iteration t when each dimension of the averaged parameter vector was last updated.
			Then after each training sentence is processed, only update the dimensions of the total parameter vector corresponding to the features in the sentence.
			(Except for the last example in the last iteration, when each dimension of τ is updated, no matter whether the decoder output is correct or not).
			Denote the sth dimension in each vector before processing the nth example in the tth iteration as n−1,t n−1,t n−1,t of convergence and their proof (Collins, 2002) are only dependent on the feature vectors, and not on αs , σs and τs = (nτ,s, tτ,s).
			Suppose that the decoder output zn,t is different from the the source of the feature definitions, the perceptron training example yn.
			Now αn,t n,t n,t algorithm is applicable to the training of our C W S model.
			s , σs and τs can 1 Daume III (2006) describes a similar algorithm..
			be updated in the following way: Input: raw sentence sent – a list of characters Initialization: set agendas src = [[]], tgt = [] σn,t n−1,t n−1,t Varia bles: candi date sente nce item – a list of word s s = σs + αs × (tN +n −tτ,sN − nτ,s) αn,t n−1,t Algo rith m: s = αs + Φ(yn) − Φ(zn,t) for index = 0..sent.length−1: σn,t n,t s = σs + Φ(yn) − Φ(zn,t) s = (n, t) We found that this lazy update method was significantly faster than the naive method.
	
	
			The decoder reads characters from the input sentence one at a time, and generates candidate segmentations incrementally.
			At each stage, the next incoming character is combined with an existing candidate in two different ways to generate new candidates: it is either appended to the last word in the candidate, or taken as the start of a new word.
			This method guarantees exhaustive generation of possible segmentations for any input sentence.
			Two agendas are used: the source agenda and the target agenda.
			Initially the source agenda contains an empty sentence and the target agenda is empty.
			At each processing stage, the decoder reads in a character from the input sentence, combines it with each candidate in the source agenda and puts the generated candidates onto the target agenda.
			After each character is processed, the items in the target agenda are copied to the source agenda, and then the target agenda is cleaned, so that the newly generated candidates can be combined with the next incoming character to generate new candidates.
			After the last character is processed, the decoder returns the candidate with the best score in the source agenda.
			Figure 2 gives the decoding algorithm.For a sentence with length l, there are 2l−1 differ ent possible segmentations.
			To guarantee reasonable running speed, the size of the target agenda is limited, keeping only the B best candidates.
	
	
			The feature templates are shown in Table 1.
			Features 1 and 2 contain only word information, 3 to 5 contain character and length information, 6 and 7 contain only character information, 8 to 12 contain word and character information, while 13 and 14 contain var char = sent[index] foreach item in src: // append as a new word to the candidate var item1 = item item1.append(char.toWord()) tgt.insert(item1) // append the character to the last word if item.length > 1: var item2 = item item2[item2.length−1].append(char) tgt.insert(item2) src = tgt tgt = [] Outputs: src.best item Figure 2: The decoding algorithm word and length information.
			Any segmented sentence is mapped to a global feature vector according to these templates.
			There are 356, 337 features with nonzero values after 6 training iterations using the development data.
			For this particular feature set, the longest range features are word bigrams.
			Therefore, among partial candidates ending with the same bigram, the best one will also be in the best final candidate.
			The decoder can be optimized accordingly: when an incoming character is combined with candidate items as a new word, only the best candidate is kept among those having the same last word.
	
	
			Among the character-tagging C W S models, Li et al.
			(2005) uses an uneven margin alteration of the traditional perceptron classifier (Li et al., 2002).
			Each character is classified independently, using information in the neighboring five-character window.
			Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score.
			It can be seen as an alternative to the M E and C R F models (Xue, 2003; Peng et al., 2004), which 1 word w 2 word bigram w1w2 3 single-character word w 4 a word starting with character c and having length l 5 a word ending with character c and having length l 6 space-separated characters c1 and c2 7 character bigram c1c2 in any word 8 the first and last characters c1 and c2 of any word 9 word w immediately before character c 10 character c immediately before word w 11 the starting characters c1 and c2 of two consecutive words 12 the ending characters c1 and c2 of two consecutive words 13 a word of length l and the previous word w 14 a word of length l and the next word w Table 1: feature templates do not involve word information.
			Wang et al.
			(2006) incorporates an N-gram language model in M E tagging, making use of word information to improve the character tagging model.
			The key difference between our model and the above models is the word- based nature of our system.
			One existing method that is based on sub-word information, Zhang et al.
			(2006), combines a C R F and a rule-based model.
			Unlike the character-tagging models, the C R F submodel assigns tags to sub- words, which include single-character words and the most frequent multiple-character words from the training corpus.
			Thus it can be seen as a step towards a word-based model.
			However, sub-words do not necessarily contain full word information.
			Moreover, sub-word extraction is performed separately from feature extraction.
			Another difference from our model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al.
			(1996).
	
	
			Two sets of experiments were conducted.
			The first, used for development, was based on the part of Chinese Treebank 4 that is not in Chinese Treebank 3 (since C T B3 was used as part of the first bake- off).
			This corpus contains 240K characters (150K words and 4798 sentences).
			80% of the sentences (3813) were randomly chosen for training and the rest (985 sentences) were used as development testing data.
			The accuracies and learning curves for the non-averaged and averaged perceptron were compared.
			The influence of particular features and the agenda size were also studied.
			The second set of experiments used training and testing sets from the first and second international Chinese word segmentation bakeoffs (Sproat and Emerson, 2003; Emerson, 2005).
			The accuracies are compared to other models in the literature.
			F-measure is used as the accuracy measure.
			Define precision p as the percentage of words in the decoder output that are segmented correctly, and recall r as the percentage of gold standard output words that are correctly segmented by the decoder.
			The (balanced) F-measure is 2pr/(p + r).
			C W S systems are evaluated by two types of tests.
			The closed tests require that the system is trained only with a designated training corpus.
			Any extra knowledge is not allowed, including common surnames, Chinese and Arabic numbers, European letters, lexicons, part-of-speech, semantics and so on.
			The open tests do not impose such restrictions.
			Open tests measure a model’s capability to utilize extra information and domain knowledge, which can lead to improved performance, but since this extra information is not standardized, direct comparison between open test results is less informative.
			In this paper, we focus only on the closed test.
			However, the perceptron model allows a wide range of features, and so future work will consider how to integrate open resources into our system.
			6.1 Learning curve.
			In this experiment, the agenda size was set to 16, for both training and testing.
			Table 2 shows the precision, recall and F-measure for the development set after 1 to 10 training iterations, as well as the number of mistakes made in each iteration.
			The corresponding learning curves for both the non-averaged and averaged perceptron are given in Figure 3.
			The table shows that the number of mistakes made in each iteration decreases, reflecting the convergence of the learning algorithm.
			The averaged per Ite rat io n 1 2 3 4 5 6 7 8 9 10 P (n on av g) R ( n o n a v g ) F ( n o n a v g ) 89 .0 91.6 92.0 92.3 92.5 92.5 92.5 92.7 92.6 92.6 88 .3 91.4 92.2 92.6 92.7 92.8 93.0 93.0 93.1 93.2 88 .6 91.5 92.1 92.5 92.6 92.6 92.7 92.8 92.8 92.9 P (a vg ) R ( a v g ) F ( a v g ) 91 .7 92.8 93.1 92.2 93.1 93.2 93.2 93.2 93.2 93.2 91 .6 92.9 93.3 93.4 93.4 93.5 93.5 93.5 93.6 93.6 91 .6 92.9 93.2 93.3 93.3 93.4 93.3 93.3 93.4 93.4 # W ro ng se nt en ce s 34 01 1652 945 621 463 288 217 176 151 139 Table 2: accuracy using non-averaged and averaged perceptron.
			P - precision (%), R - recall (%), F - F-measure.
			B 2 4 8 16 32 64 12 8 25 6 51 2 10 24 Tr 66 0 61 0 68 3 83 0 11 11 16 45 25 45 49 22 91 04 15 59 8 Se g 18 .6 5 18 .1 8 28 .8 5 26 .5 2 36 .5 8 56 .4 5 95 .4 5 17 3.
			38 32 5.
			99 55 9.
			87 F 86 .9 0 92 .9 5 93 .3 3 93 .3 8 93 .2 5 93 .2 9 93 .1 9 93 .0 7 93 .2 4 93 .3 4 Table 3: the influence of agenda size.
			B - agenda size, Tr - training time (seconds), Seg - testing time (seconds), F - F-measure.
			0.94 0.93 0.92 0.91 0.9 0.89 0.88 0.87 0.86 n o n a v e r a g e d a v e r a g e d 1 2 3 4 5 6 7 8 9 10 n u m b e r o f t r a i n i n g i t e r a t i o n s also affects the training time, and resultin g model, since the perceptr on training algorith m uses the decoder output to adjust the model paramet ers.
			Table 3 shows the accuraci es with ten differen t agenda sizes, each used for both training and testing.
			Accu racy does not increase beyond B = 16.
			Moreov er, the accurac y is quite competi tive even with B as low as 4.
			This reflects the fact that the best segment ation is often within the current top few candidates in the agenda.
			2 Since.
			the training and testing time generall y increase s as N increase s, the agenda size is fixed to 16 for the remaini ng experim ents.
			6 . 3 T h e i n f l u e n c e o f p a r t i c u l a r f e a t u r e s Figure 3: learning curves of the averaged and non- averaged perceptron algorithms ceptron algorithm improves the segmentation accuracy at each iteration, compared with the non- averaged perceptron.
			The learning curve was used to fix the number of training iterations at 6 for the remaining experiments.
			6.2 The influence of agenda size.
			Reducing the agenda size increases the decoding speed, but it could cause loss of accuracy by eliminating potentially good candidates.
			The agenda size Our C W S model is highly dependent upon word information.
			Most of the features in Table 1 are related to words.
			Table 4 shows the accuracy with various features from the model removed.
			Among the features, vocabulary words (feature 1) and length prediction by characters (features 3 to 5) showed strong influence on the accuracy, while word bigrams (feature 2) and special characters in them (features 11 and 12) showed comparatively weak influence.
			2 The optimization in Section 4, which has a pruning effect, was applied to this experiment.
			Similar observations were made in separate experiments without such optimization.
			Fe at ur es F Fe at ur es F A l l w / o 2 w / o 6 w / o 8 w /o 1 1, 1 2 93 .3 8 93 .3 6 93 .1 3 93 .1 4 93 .3 8 w/ o 1 w / o 3 , 4 , 5 w / o 7 w / o 9 , 1 0 w / o 1 3 , 1 4 92 .8 8 92 .7 2 93 .1 3 93 .3 1 93 .2 3 Table 4: the influence of features.
			(F: F-measure.
			Feature numbers are from Table 1) 6.4 Closed test on the S I G H A N bakeoffs.
			Four training and testing corpora were used in the first bakeoff (Sproat and Emerson, 2003), including the Academia Sinica Corpus (AS), the Penn Chinese Treebank Corpus (CTB), the Hong Kong City University Corpus (CU) and the Peking University Corpus (PU).
			However, because the testing data from the Penn Chinese Treebank Corpus is currently unavailable, we excluded this corpus.
			The corpora are encoded in GB (PU, CTB) and BIG5 (AS, CU).
			In order to test them consistently in our system, they are all converted to UTF8 without loss of information.
			The results are shown in Table 5.
			We follow the format from Peng et al.
			(2004).
			Each row represents a C W S model.
			The first eight rows represent models from Sproat and Emerson (2003) that participated in at least one closed test from the table, row “Peng” represents the C R F model from Peng et al.
			(2004), and the last row represents our model.
			The first three columns represent tests with the AS, CU and PU corpora, respectively.
			The best score in each column is shown in bold.
			The last two columns represent the average accuracy of each model over the tests it participated in (SAV), and our average over the same tests (OAV), respectively.
			For each row the best average is shown in bold.
			We achieved the best accuracy in two of the three corpora, and better overall accuracy than the majority of the other models.
			The average score of S10 is 0.7% higher than our model, but S10 only participated in the HK test.
			Four training and testing corpora were used in the second bakeoff (Emerson, 2005), including the Academia Sinica corpus (AS), the Hong Kong City University Corpus (CU), the Peking University Corpus (PK) and the Microsoft Research Corpus (MR).
			Table 5: the accuracies over the first S I G H A N bake- off data.
			A S CU PK MR SA V O A V S1 4 S1 5b S2 7 94 .7 94.3 95.0 96.4 95 .2 94.1 94.1 95.8 94 .5 94.0 95.0 96.0 95 .1 94 .8 94 .9 95 .4 95 .4 95 .4 Zh -a Zh -b 94 .7 94.6 94.5 96.4 95 .1 95.1 95.1 97.1 95 .1 95 .6 95 .4 95 .4 94 .6 95.1 94.5 97.2 Table 6: the accuracies over the second S I G H A N bakeoff data.
			Different encodings were provided, and the UTF8 data for all four corpora were used in this experiment.
			Following the format of Table 5, the results for this bakeoff are shown in Table 6.
			We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.
			(2006) for comparison.
			Row “Zh-a” and “Zh-b” represent the pure sub-word C R F model and the confidence-based combination of the C R F and rule-based models, respectively.
			Again, our model achieved better overall accuracy than the majority of the other models.
			One system to achieve comparable accuracy with our system is Zh-b, which improves upon the sub-word C R F model (Zh-a) by combining it with an independent dictionary-based submodel and improving the accuracy of known words.
			In comparison, our system is based on a single perceptron model.
			In summary, closed tests for both the first and the second bakeoff showed competitive results for our system compared with the best results in the literature.
			Our word-based system achieved the best F- measures over the AS (96.5%) and CU (94.6%) corpora in the first bakeoff, and the CU (95.1%) and MR (97.2%) corpora in the second bakeoff.
	
	
			We proposed a word-based C W S model using the discriminative perceptron learning algorithm.
			This model is an alternative to the existing character- based tagging models, and allows word information to be used as features.
			One attractive feature of the perceptron training algorithm is its simplicity, consisting of only a decoder and a trivial update process.
			We use a beam-search decoder, which places our work in the context of recent proposals for search- based discriminative learning algorithms.
			Closed tests using the first and second S I G H A N C W S bake- off data demonstrated our system to be competitive with the best in the literature.
			Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve accuracy.
			Therefore, given the flexibility of the feature-based perceptron model, an obvious next step is the study of open features in the seg- mentor.
			Also, we wish to explore the possibility of incorporating P O S tagging and parsing features into the discriminative model, leading to joint decoding.
			The advantage is twofold: higher level syntactic information can be used in word segmentation, while joint decoding helps to prevent bottom- up error propagation among the different processing steps.
	
	
			This work is supported by the ORS and Clarendon Fund.
			We thank the anonymous reviewers for their insightful comments.
	

