
	
		The problem of finding lexical alignments for givensentence pairs is computationally expensive.
		Furthermore, it is much difficult to find lexical alignments between Korean and English since they have considerably different syntactic structures and the coverage of word-for-word correspondences is low.This paper presents a method for extracting structural features which can reduce mapping space by allowing only probable alignments.
		We describe how the features improve the performance of the lexical alignment model.
		The structural features providethe information for the correspondences of parts-of-speech (POS) sequences which are useful in translation.
		Based on maximum entropy (ME) concept, the structural features are incrementally selected, which are later embedded in the lexical alignment model.It turns out that the features help get better lexical alignments of Korean and English by offering linguistic knowledge.
	
	
			Aligned bitexts are useful for the derivation ofbilingual lexical resources which are used for ma chine translation and cross languages informationretrieval.
			Thus, a lot of approaches have been sug gested to find sets of corresponding word tokens (Brown et al., 1993; Berger et al., 1996; Melamed,1997), phrase (Shin et al., 1996), noun phrase (Ku piec, 1993), and collocation (Smadja et al., 1996) in a bitext.
			Some works have used lexical association measures for finding word correspondences (Gale and Church,1991; Fung and Church, 1994).
			However, the associ ation measures can be misled in cases where a word in a source language frequently co-occurs with more than one word in a target language or in cases of indirect association&apos; (Melamed, 1997).
			In other works, iterative parameter re-estimation &apos;Suppose that Uk and vk are indeed mutual translation and Uk and Ukki often co-occur in text.
			Then vk and 24+1 will also co-occur more than expected by chance , which is represented as indirect association.
			techniques based on IBM model 15 2 have been employed (Brown et al., 1993).
			They were usually incorporated in the EM algorithm (Brown et al., 1993; Kupiec, 1993; Tillmann and Ney, 2000; Och et al., 2000).
			However, we are often faced with some difficulties as follows, when the IBM model-based approaches are directly applied to the alignment, especially on bitext involving a less closely related language pair.
			1.
			It needs excessive iteration time for parame-.
			ter estimation and high decoding complexity.Thus most systems assumed one-to-one correspondence to reduce computational complexity.
			However, word sequences are not trans lated literally word for word.
			For example, incases of collocations, compound nouns, and ambiguous words with different meaning depen dent on the context, they require phrase-level correspondences.
	
	
			edge to structure the underlying models.
			Thedistortion probability and the fertility probabil ity for finding word correspondences is a weakdescription for word order change between languages and 1:n mapping modeling.
			As the re sult, lots of ungrammatical sentences and too many parameters to be estimated are allowed.
			In addition, many words are aligned to the empty word due to the overfitting problem (Och et al., 2000).
	
	
			quires a very large volume of bilingual aligned text.
			In this paper, we use structural correspondences to 2 7/1 P(f, ale) = Hn(Oilei)Ht(fjlea,)d(jlajâ€ž/) n is the fertility probability that an English word generates 7/ French words, t is the word alignment probability that theEnglish word e generates the French word f, and d is the dis tortion probability that an English word in a certain position will generate a French word in a certain position.
			English Korean can/MD eu1/ENTR1 sa/NNDE ss/AJ da/ENTE World/NNP Cup/NNP weoideakeop/NNIN1 to/TO take/VBP off/RP iryugha/VB neart/ENTR1 MD:modal auxiliary, NNP:proper noun, VBP:verb, RP:particle NNDE:dependent noun, AJ:adjective ENTE: final ending, NNIN1:proper noun, VB:verb, ENTR1:adnominal endingTable 1: Lexical differences between English and Ko reanovercome the above problem in bilingual text align ment.
			2 The problems of lexical alignment.
			in Korean and English bitext Although one-to-one correspondence assumption has been shown to give highly accurate results in closely related language pair (Melamed, 1997), it does notfit in structurally different language pair such as Ko rean and English.
			According to Shin et al.
			(1996), the coverage of one-to-one correspondence in Koreanand English bitext is approximately 34% and many to-many mappings exceed 40% in lexical alignments.
			Besides, Korean and English show much difference in terms of unit of mutual translation as shown in Table 1.
			Thus, we use structural or linguistic informationto find accurate lexical alignments of bitext.
			In gen eral, structural information can be represented bythe form of bilingual correspondence of phrase structure (Watanabe et al., 2000) or dependency struc ture (Yamamoto, 2000).
			To find these structural correspondences betweena language pair, unification-based grammar (Mat sumoto, 1993) or bilingual grammar (Wu, 1997) can be used.
			That is, if we try to find structural match of bitext, syntactic analysis of each text should be donefirst.
			However, syntactic analysis of each text(or bi text) is also a hard and computationally complicated problem.
			In this paper, we propose POS sequence feature.We consider correspondences between POS tag se quences of bitext as the structural information whichis important in determining structures and reduc ing unnecessary parameters of a statistical alignment model.
			We induce the correspondences(structural features) by a feature selection method based on the ME framework.
			Finally, the structural features are embedded in a lexical alignment model of English and Korean bitext.
			Our model offers the following advantages: 1.
			It can overcome the limitation of word-for-word.
			correspondences.
			2.
			The model can take advantage of the explicit.
			introduction of some knowledge about the language.
			Therefore, it can reduce a lot of parameters in statistical machine translation by elim inating syntactically unlikely alignments.
			Thelexical alignment model iterates only over a sub set of probable alignments.
			3.
			It is possible to estimate the probability of lex-.
			ical alignment in a relatively small size of cor pora.
	
	
			struction of a bilingual grammar.
			3 Overview of the model.
			In general, there exist constaints among POS se quences mapping when aligning bitext.
			For instance, in many cases a noun in Korean is translated to a noun or a noun preceded by an article in English.
			This POS constraint would be useful information in alignment of a closely related pair as well as a less closely related language pair.
			Some approaches design an alignment algorithm that maximizes the number of matching POS in aligned segments (Papageorgiou et al., 1994).
			We also assume that the mapping information of POS sequences of a language pair is useful in a model of statistical machine translation.
			If there are similarities between corresponding POS sequences in bitext, the structural feature would be easily computed or identified.
			However,a POS sequence in English often correspond to a to tally different POS sequence in Korean as shown in the following example: can/MD eui/ENTR1 sa/NNDE1 &apos;iss/AJMA da/ENTEIt is caused by the discrepancy between two lan guages.
			Korean is an agglutinative language.
			A sentence in Korean consists of a series of syntactic units called eojeol.
			An eojeol delimited by whitespace is often composed of a content word and function words.
			Tense markers, clausal connectives, particles and so forth are contained in an eojeol.
			Thus, one or more words in English often correspond to an eojeol, i.e. a couple of morphemes.
			For instance, a phrase, &apos;tothe school&apos;, in English corresponds to an eojeol &amp;quot;d4 2___(haggyoro, school-to)&apos; in Korean.Thus, we need a method for mapping POS se quences of bitext.
			In this paper, the correspondencesof POS tag sequences are obtained by automatic fea ture selection based on the ME framework.
			Here, a feature is defined as a correspondence of POS tag sequences in bitext.
			The outline of finding correspondences of POS tag sequences is as follows: First, our model starts with initial features obtained by a supervision step.
			The initial features are extracted from a small portion of bitext, which of weights are trained by the IIS algorithm (Berger et al., 1996; Pietra et al., 1997).
			These are called initial active features.
			In the next step, a feature pool is constructed from training data.
			At this time, only features giving alarge gain to the model are selected.
			The final out put of feature selection is the set of active features and the correspondences of POS tag sequences that are represented with conditional probabilities.
			The resulted features are used for the parameters of bilingual text alignment.
			In the process, we look at the words to ensure a correct alignment.
			That is, the POS sequences are in fact encoding particular words.
			The underlying process is as follows: Input: a set L of POS-labeled sentence pairs.
			1.
			Make a set .F of correspondences of tag se-.
			quences, (te, tk) from a small portion of L by hand.
			2.
			Set .F into a set of initial active features, A..
			3.
			Compute the weights of the initial active fea-.
			tures A using HS algorithm.
			4.
			Create a feature pool P which is a set of possi-.
			ble combinations of tag sequences from sentence pairs.
	
	
			with A.
	
	
			in P.
	
	
			add A.
	
	
			p(tk Ite) where (te, tk) E A. 4 Feature Selection.
			As mentioned above, features which represent mappings of POS sequences in two languages are auto matically learned from bitext.
			The learning consistsof two steps: (1) supervised step and (2) unsuper vised feature selection.In the supervised step, we manually aligned En glish and Korean texts.
			From the manually aligned text, we consturct a feature pool which has initial POS sequence correspondence.
			In the unsupervised feature selection, the POS sequence correspondeces are added to the feature pool.
			4.1 Supervision Step.
			In the supervision step, a small portion of bi text is tagged using Brill&apos;s tagger (Brill, 1995) and `MORANY&apos; (Yoon et.
			al., 1999), each of which is for English and Korean tagging respectively.
			We manually aligned each sentence pair in the bitext and collected their correspondences of tag sequences.
			For simplicity, we adjusted some part of Brill&apos;s tag set.First of all, we classified sentential patterns of En glish and Korean.
			Then, we aligned English and Korean sentence pairs on the basis of the patterns.
			Also, we made tag sequence construction rules with respect to each language by analysis of the caseswhere the words of one unit are separated and ad jacent.
			The rules are used when making a feature pool.
			After collecting the correspondences of POS tag sequences, we used them as a set of initial active features, A. 4.2 Construction of a Feature Pool.
			In this chapter, we describe how a feature pool isconstructed.
			Our task is to construct a probabil ity model p that produces a corresponding Korean tag sequence &amp;quot;Tic for a given English tag sequenceJ.
			As features to represent the model, we use co occurrence information of POS tag sequences.Let Ts be all possible correspondences of tag se quences for a specific sentence pair, S. We then define a feature function (or a feature) as follows: 1 x = te &amp; y = tk Sz fte,t,(x,y) = pair(te,tk) E TS 0 otherwiseA feature fte,t,, which indicates co-occurrence be tween tags appearing in Ts, expresses information for predicting that an English POS tag sequence te maps into a Korean POS tag sequence tk â€¢ In order to make a feature pool, given a sentence pair, we first construct all possible combinations ofEnglish POS tag sequences and Korean POS tag sequences.
			Among them, only features fte,t, that sat isfy the following two conditions are added into the feature pool P. â€¢ count(fte,t,)&gt; 15 â€¢ there exist tk such that (te, tk.)
			E A and the similarity value (simply counting of same tag) between tk and tkx is greater than 0.6 4.3 Feature selection.
			Since the set of P is too vast, a feature selection pro cess is needed to select useful features.
			For this, eachfeature is evaluated according to how much they con tribute to the likelihood of training data, which is called gain.
			Before explaining feature gain and a process of feature selection, we will give a brief introduction ofME.
			In ME, a feature gives information to a proba bility model and it has a weight.Thus, the model is constrained by features we de fined.
			In general weights of the features are trained by the improved iterative scaling (IIS) algorithmthat minimizes the KullbackLeibler divergence be tween the model and the empirical distribution of the training data.
			In fact, our model reduces to a simple type of probability model that can be derived simply from a ratio counts since the features do not overlap.
			Let pA be the optimal model constrained by a set of initial active features A, then it can be represented in (1).
			pA(ylx) = LAyix)eE fi (x,Y) (1) zA(x) = Ep(yix)eE The weights (A) of active features are computed by the IIS before feature selection.
			Let ,41, be AU L, and pAL be the optimal model in the space of probability distribution after addingfeature L. The model p.,611 contains another param eter a, in addition to the parameters given by active features, which is a weight for the feature L. In order to make it tractable to compute feature selection, we assume that the addition of a feature L affects only the single parameter a. The only parameter which distinguishes models of (2) is a. 1 (2) pAf, = zct(x)PA(Yix)eaL(x&apos;Y) Z(x) =EPA(Yix)ectMx&apos;Y) The improvement (gain) of a model after adding asingle feature L can be estimated by measuring dif ference of maximum log-likelihood between L(pAL) and L(pA).
			We denote the gain for feature L by A(,41), which is represented in (3).
			A (AL) maxcEGAf, (a) (3) GAfi (a) = L(PAf) L(PA) log PAâ€˜f PA The following algorithm is used to compute the gain of the model with respect to L. We adopted Berger&apos;s method for computing gains (Berger et al., 1996).
			For the details, the reader is referred to (Berger et al., 1996; Pietra et al., 1997).
			1.
			Let r = {.
			2.
			Set a() = 0.
			3. Repeat the following until GAL (an) has con-.
			verged: Compute anÂ±i from an using anÂ±i = an + 1 log (1 G GA:tf: (0)) Compute GAL (an+i) using GAL (a) = â€”E.73(x)logz(x)+c(fi) GiAfi(a) = .13(M â€” Ex /3(x)m(x) , CA.fi (a) = â€” E. i)(x)p.Afi â€”m(x))2 Ix) where a = an+i, Afi = Au M(x) Pc:kfi(filx) P.Afi(filX) = PAfi (Y1x)fi(x,Y) 4.
			Set â€” AL(AL) GAL (an).
			This algorithm is iteratively computed using Netwon&apos;s method.
			With the gain value, we can recognize importance of a feature, i.e. how much the feature accords with the model.
			As a result, we can se lect useful features with high gain values and obtain their conditional probabilities.
			Put another way, we can get the correspondence probabilities of POS tag sequences.
			5 lexical alignment In this section, we describe how the correspondenceprobabilities between bilingual POS sequences is embedded in lexical alignment.
			In KoreanEnglish machine translation, a translation system finds the cor responding sentence e given a Korean sentence k. The fundamental equation of machine translationcan be represented in (4), where the random variables K and E are a Korean sentence and a English sentence making up a translation, and the ran dom variable A is an alignment between them.
			Theequation is related with a language model probabil ity, P(e), a translation model probability P(kle).
			Inthis paper, we are interested in estimating the trans lation model probability P(kle).
			P(elk) = P(e)P(kle) p(k) = arg nTx P(e)P(kle) (4) In this work, we modified the IBM 1 model using the structural information for estimating translationprobabilities.
			The translation probability of a spe cific sentence pair, k and e is m P(kle) = e t(kpjlepi)P(Ckp repi ) (5) j=1 j=1 In (5), an English sentence e has / possible phrases, epiep2...e and a Korean sentence k has m possible phrases, kpikp2...kpm and the phrases of e have corresponding sequences of POS categories,Cep1Cep2...C, , and the phrases of k have the se quences of PPOS categories Ckp1Ckp2...Ckpm,.
			E is 1 if ii(f) PAW â€”1 otherwise English Korean Train Sentences 30,000 Words/Eojeol 282,967 201,771 Vocabulary 47,647 80,360 Morph Types 52,197 57,800 Table 2: Training corpus size English words Korean morphemes Ratio 1 1 31.2 1 2 22.4 1 3 13.6 2 1 6.9 etc etc 25.9 Table 3: The result of alignment unit a small, fixed normalizing number (Brown et al., 1993).Note that the probability P(Ckp, rep) is pre computed by the feature selection process and onlylexical alignment parameter t(kpj lep,) can be esti mated by the EM algorithm using sentence pairs of bitext, (k(s), e(s)), s = 1,..., S. According to the equation (4), we can eliminate the combination that P(Ckp, &apos;Cep) is zero.
			Thus, the model iterates only over a subset of probable alignments.
			To estimate probability t(kplep), the expected number of times(fractional counts) that the phrase ep connects to kp in the translation (kle) is used.
			The count denoted by e(kplep; k, e) can be expressedin (7) using (6) and translation probabilities are rees timated by (8).
			P(ale, = P(k, ale) P(k, ale) (6) P(kle) E P(k, ale) 711 1 e(kplep; k, e) = j=1 i=1 t(kplep)P(ckplcep) = t(kplepOP(ckplcepi)+...+t(kplepi)P(ckplcepi) t(kplep) = sc(kplep; k, E E e(kp iep; k(s), e(s)) kp s=1 6 Experiments.
			We present results tested on EnglishKorean bitext that is extracted from the web site of &apos;Korea Times&apos; and a magazine for English learning (Table 2).
			We manually aligned 700 POS-tagged sentencepairs to obtain initial parameters for correspon dences of tag sequences.
			As shown in Table 3, the coverage of word-for-word correspondences in EnglishKorean bitext was only 31.2%.As a result, 1,483 correspondences of tag se quences were collected from the manually aligned bitext (Table 4).
			The correspondences were used as initial activefeatures and the weights of the initial active fea tures were computed by ITS algorithm.
			Table 5 set description disjoint features A active features 1,483 P filtered feature pool 8,147 N selected new features 702 Table 4: Features(correspondences of Tag Sequence) shows weights A of the initial active features such that f+ ,BEPFJJ,tk E A. Through the process of feature pool construction, 8,147 features(tag sequence correspondences) were selected from the 23,000 sentence pairs.
			Since weused the filtering method and tag sequence construc tion rules, the size of the feature pool was not large.In the feature selection step, we chose useful fea tures with the gain threshold of 0.008.
			As a result of feature selection, we obtained the probability model that given a specific English POS tag sequence, a corresponding Korean POS tag sequence happens to occur.Table 6 shows some examples of conditional prob abilities.
			The table shows that the determiner ofEnglish is generally translated into NULL or adnom inal word in Korean.
			The conditional probabilities regarding the correspondences of POS tag sequenceswere used as known parameters of the lexical align ment model.
			Effect of SmoothingAs mentioned before, in previous IBM-based mod els, many words are aligned to the empty word and rare words are mis-aligned.
			Thus, we tested if the structural features are effective in smoothing.
			For this, the accuracy of lexical correspondences wasevaluated both on low frequency words and high fre t e p(tkite) tk &apos;4 &apos;4 &apos;4 &apos;4 &apos;4 &apos;4 &apos;4 NNIN2 0.524131 &apos;4 &apos;4 &apos;4 &apos;4 &apos;4 &apos;4 , +++++++ H H H H H H (1) ANDEÂ±NNIN2 0.15161 ANNUÂ±NNDE2 0.091036 NNIN2+PPCA1 0.063515 NNIN2+NNIN2 0.058322 NNIN2+PPAU 0.05768 ADCO 0.049622 etc Table 6: Examples of conditional probability fte,tk Ai p(tk Ite) e k fBEPÂ±JJ,VBMAÂ±ENC03Â±AXÂ±ENTE 10.1369 0.4247 are-Fprepared junbidoiFeo-FissFda fBEPÂ±JJ,V BMA 8.8520 0.1180 is-Fcareful ju&apos;yiha fBEPÂ±JJ,AJMA 8.6787 0.0996 am-Fhealthy geongangha fBEPÂ±JJ,AJMAÂ±ENTE 8.2628 0.0655 is-Fnew syaelobFda fBEPÂ±JJ,VBMAÂ±ENTE 7.2379 0.0236 am-Fsure hwagsinhaFbnida fBEPÂ±JJ,NNIN2+CO 7.1372 0.0210 am-Frich bujaFi fBEPÂ±JJ,NNIN2+COÂ±VBMA 6.9909 0.0183 is-Fselfish igijeogFi-Fdoi fBEPÂ±JJ,NNIN2+PPCA1+VBMAÂ±ENTE 6.8402 0.0157 is-Fpatriotic &apos;aegugjaFga-FdoiFda fBEPÂ±JJ,NNIN2+COÂ±ENTE 6.8308 0.0156 is-Freasonable habligeogFi-Fda fBEPÂ±JJ,NNIN2+PPCA2+AXÂ±ENTE 6.4256 0.0105 is-Frephrehensible binanbadFeul-FmanhaFda fBEPÂ±JJ,NNIN2+PPCA1+VBMA 6.4250 0.0105 am-Fhelpful doumFi-FdoiFda BE:be verb (present tense), DT:determiner, JJ:adjective(ordinal), NN:common noun, RB :adverb AJMA:adjective, VBMA:verb, AX:auxilary verb ENC03:auxilary ending, ENTE:final ending ANCO:configurative adnominal, ANDE: demonstrative adnominal , ANNU:numeral adnominal NNIN2:common noun, NNDE2:unit-dependent noun, CO:copular PPCA1:nominative postposition, PPCA2:accusative postposition, PPAU:auxilary postposition Table 5: Examples of active features frequency English Korean Accuracy Words Eojeols (Total) 4 100(2149) 100(3317) 70.3% 50,-400 100(712) 100(486) 78.9% the number of parameters IBM 1 14,776 Our Model 1,344 Table 8: Problem Space Table 7: Evaluation Vocabulary quency words.Table 7 shows the accuracy of some results ob tained by lexical alignment.
			In the training bitext, English 2149 words and Korean 3317 eojeols with low frequency 4 and English 712 words and Korean 486 eojeols with high frequency (50300) were found.For an evaluation, 100 words and 100 eojeols were selected out of them.
			The alignments of the words (eo jeols) were evaluated.
			As a result, it turns out that the structural features are effective in rare words.
			Effect of reducing a parameter spaceAnother advantage of the use of structural fea tures is to reduce parameter space of the lexical alignment model.
			To show the impacts on the size ofthe parameter space reduced by our model, we com pared the results from our model with those from the IBM 1-model presented by Brown et al.
			(1993) on 100 sentence pairs.
			The number of parametersobtained means the counts of possible lexical map pings.
			As shown in Table 8, the number of prameters were drastically reduced in our model.
			Considering the complexity of another models (IBM 25) it is obvious that they have more parameters.
			Effect of improving the results of lexical alignmentsFor evaluating the efficacy of the structural fea tures, we compared the results with IBM model 1.
			Accuracy IBM 1 59.8 Our Model 73.7 Table 9: Accuracy of n:1 and 1:1 alignmentsAs described before, only n(English):1 and 1:1 mapping are possible in IBM model since one-to-one cor respondence is assumed.
			Thus, we selected only n:1 and 1:1 alignments out of the alignment results.
			For comparison, we investigated the alignments of the English 100 words with high frequency, which were explained above.
			Table 9 shows the accuracy of lexical alignments.
			It is shown that the structural features have an effect on the alignment even though the amount of data investigated is small.
			However, the overall accuracy of the alignment is somewhat low, which is mainly due to the small size of training samples.
			Table 10 shows some results ofmutual translation.
			We see a considerable improvement when allowing for structural features (correspondences of POS tag sequences) in lexical align ments.
			Error AnalysisExcept the errors by the incorrect parameter esti mation, most errors of correspondences of POS tagsequences are caused by POS tagging errors.
			In ad dition, the correspondences of adverbs turn out to be Korean English Probability jeongbu government 0.7312 jeongbu the government 0.2012 jeongbu republic 0.0328 jeongbu authorities 0.0171 jeongbu officials 0.0119 jeongbu Chinese 0.0041 Table 10: Examples of alignment sometimes erroneous, which is due to the fact that the position of adverb can be moved quite free.
			7 Conclusion.
			Because of the considerable difference between English and Korean, computation cost of lexical align ment is very high.
			One solution of the problem is to provide mapping information of syntactic structure between the two languages.
			In this paper, we defined the structural feature asthe correspondence of POS tag sequences and pre sented a method for extraction of structural features for KoreanEnglish bilingual alignment.
			Firstly, the initial active features were collected from a small size of manually aligned bitext, which are trained by IIS which is a training algorithm for ME. Secondly, extracted from training data, the features giving a large gain were added to the set of active features.
			Furthermore, the features extracted were tested for lexical alignment of bitext.
			The experiment showed that the features are helpful for reducing the mapping space in alignment.
			We expect that the alignment can be more accurate and efficient by combining the structural features with translation lexicon in the future.
	
