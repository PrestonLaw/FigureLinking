T1 Caption 4135 4261 Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an.
A1 Type T1 Table
A2 Num T1 1
T2 Caption 4265 4346 The distinctions in the ATB are linguistically justified, but complicate parsing.
A3 Type T2 Table
A4 Num T2 1
T3 Caption 4350 4416 Table 8a shows that the best model recovers SBAR at only 71.0% F1.
A5 Type T3 Table
A6 Num T3 1
T4 Reference 4350 4416 Table 8a shows that the best model recovers SBAR at only 71.0% F1.
A7 RefType T4 Direct
A8 Type T4 Table
A9 Num T4 8
T5 Reference 5293 5418 This is especially true in the case of quotations—which are common in the ATB—where (1) will follow a verb like (2) (Figure 1
A10 RefType T5 Direct
A11 Type T5 Table
A12 Num T5 1
T6 Caption 6757 6899 Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1)
A13 Type T6 Figure
A14 Num T6 1
T7 Reference 6767 6900 The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1).
A15 RefType T7 Direct
A16 Type T7 Table
A17 Num T7 1
T8 Reference 7459 7550 Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase.
A18 RefType T8 Direct
A19 Type T8 Figure
A20 Num T8 4
T9 Reference 4775 4799 Table 1 shows four words
A21 RefType T9 Direct
A22 Type T9 Table
A23 Num T9 1
T10 Reference 4827 4886 whose unvocalized surface forms 0 an are indistinguishable.
A24 RefType T10 Direct
A25 Type T10 Table
A26 Num T10 1
T11 Caption 8299 8396 Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2–23) and the ATB (p1–3
A27 Type T11 Table
A28 Num T11 2
T12 Caption 8879 8937 Table 4: Gross statistics for several different treebanks.
A29 Type T12 Table
A30 Num T12 4
T13 Caption 9145 9275 Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction.
A31 Type T13 Table
A32 Num T13 3
T14 Reference 8112 8196 As a result, Arabic sentences are usually long relative to English, especially after
A33 RefType T14 Direct
A34 Type T14 Table
A35 Num T14 2
T15 Reference 9279 9302 segmentation (Table 2).
A36 RefType T15 Direct
A37 Type T15 Table
A38 Num T15 2
T16 Reference 9410 9535 But it conflates the coordinating and discourse separator functions of wa (<..4.b � �) into one analysis: conjunction(Table 3
A39 RefType T16 Direct
A40 Type T16 Table
A41 Num T16 3
T17 Reference 9695 9824 We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).
A42 RefType T17 Direct
A43 Type T17 Table
A44 Num T17 8
T18 Reference 10083 10182 We compared the ATB5 to tree- banks for Chinese (CTB6), German (Negra), and English (WSJ) (Table 4)
A45 RefType T18 Direct
A46 Type T18 Table
A47 Num T18 4
T19 Caption 12079 12146 Table 5: Evaluation of 100 randomly sampled variation nuclei types.
A48 Type T19 Table
A49 Num T19 5
T20 Reference 12985 13049 Table 5 shows type- and token-level error rates for each corpus.
A50 RefType T20 Direct
A51 Type T20 Table
A52 Num T20 5
T21 Caption 14036 14291 Figure 2: An ATB sample from the human evaluation.
			The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a).
			But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation i
A53 Type T21 Figure
A54 Num T21 2
T22 Reference 14575 14790 A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).
A55 RefType T22 Direct
A56 Type T22 Table
A57 Num T22 7
T23 Caption 16588 16686 Table 6: Incremental dev set results for the manually annotated grammar (sentences of length ≤ 70)
A58 Type T23 Table
A59 Num T23 6
T24 Reference 19596 19760 To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC).
A60 RefType T24 Direct
A61 Type T24 Table
A62 Num T24 3
T25 Reference 19172 19230 In Table 7 we give results for several evaluation metrics.
A63 RefType T25 Direct
A64 Type T25 Table
A65 Num T25 7
T26 Caption 21634 21983 Table 7: Test set results.
			Maamouri et al.
			(2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length ≤ 40.
			The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them.
			We are unaware of prior results for the Sta
A66 Type T26 Table
A67 Num T26 7
T27 Caption 22072 22210 Figure 3: Dev set learning curves for sentence lengths ≤ 70.
			All three curves remain steep at the maximum training set size of 18818 tr
A68 Type T27 Figure
A69 Num T27 3
T28 Reference 25271 25356 Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify.
A70 RefType T28 Direct
A71 Type T28 Table
A72 Num T28 8
T29 Reference 25874 25983 We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8.
A73 RefType T29 Direct
A74 Type T29 Table
A75 Num T29 8
T30 Reference 25446 25625 However, the learning curves in Figure 3 show that the Berkeley parser does not exceed our manual grammar by as wide a margin as has been shown for other languages (Petrov, 2009).
A76 RefType T30 Direct
A77 Type T30 Figure
A78 Num T30 3
T31 Reference 25799 25870 In Figure 4 we show an example of variation between the parsing models.
A79 RefType T31 Direct
A80 Type T31 Figure
A81 Num T31 4
T32 Caption 29023 29717 Table 8: Per category performance of the Berkeley parser on sentence lengths ≤ 70 (dev set, gold segmentation).
			(a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse.
			We showed in §2 that lexical ambiguity explains the underperformance of these categories.
			(b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ).
			Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing.
			(c) Coordination ambiguity is shown in dependency scores by e.g., ∗SSS R) and ∗NP NP NP R).
			∗NP NP PP R) and ∗NP NP ADJP R) ar
A82 Type T32 Table
A83 Num T32 8
T33 Reference 30434 30577 Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.
A84 RefType T33 Direct
A85 Type T33 Table
A86 Num T33 9
T34 Caption 31239 31292 Table 9: Dev set results for sentences of length ≤ 70
A87 Type T34 Table
A88 Num T34 9
T35 Reference 31476 31598 In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6.
A89 RefType T35 Direct
A90 Type T35 Table
A91 Num T35 6
