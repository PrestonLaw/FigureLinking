T1 Caption 1933 2071 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags:
A1 Type T1 Equation
A2 Num T1 1
T2 Reference 2104 2155 This sentence should be tagged as shown in table 1.
A3 RefType T2 Direct
A4 Type T2 Table
A5 Num T2 1
T3 Caption 2638 2724 Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.
A6 Type T3 Table
A7 Num T3 1
T4 Reference 6602 6722 Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns.
A8 RefType T4 Direct
A9 Type T4 Figure
A10 Num T4 1
T5 Caption 8004 8076 Figure 1: Probability estimation tree for the nomi native case of nouns.
A11 Type T5 Figure
A12 Num T5 1
T6 Caption 8080 8152 The test 1:ART.Nom checks if the preceding word is a nominative article.
A13 Type T6 Figure
A14 Num T6 1
T7 Caption 20978 21037 Table 2: Tagging accuracies on development data in percent.
A15 Type T7 Table
A16 Num T7 2
T8 Caption 21041 21124 Results for 2 and for 10 preceding POS tags as context are reported for our tagger.
A17 Type T8 Table
A18 Num T8 2
T9 Reference 21145 21219 Table 3 shows the results of an evaluation based on the plain STTS tagset.
A19 RefType T9 Direct
A20 Type T9 Table
A21 Num T9 3
T10 Reference 21528 21627 Table 2 summarizes the results obtained with different taggers and tagsets on the development data.
A22 RefType T10 Direct
A23 Type T10 Table
A24 Num T10 2
T11 Reference 21631 21849 The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3% without and 69.4% with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon.
A25 RefType T11 Indirect
A26 Type T11 Table
A27 Num T11 2
T12 Reference 21853 21914 The TnT tagger achieves 86.3% accuracy on the default tagset.
A28 RefType T12 Indirect
A29 Type T12 Table
A30 Num T12 2
T13 Reference 21980 22081 The tagset refinement increases the accuracy by about 0.6%, and the external lexicon by another 3.5%.
A31 RefType T13 Indirect
A32 Type T13 Table
A33 Num T13 2
T14 Caption 22814 22972 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.
A34 Type T14 Table
A35 Num T14 3
T15 Caption 23270 23342 Figure 2: Tagging accuracy on development data depending on context size
A36 Type T15 Figure
A37 Num T15 2
T16 Reference 23343 23424 Figure 2 shows that the tagging accuracy tends to increase with the context size.
A38 RefType T16 Direct
A39 Type T16 Figure
A40 Num T16 2
T17 Reference 23850 23897 Table 4 shows the performance on the test data.
A41 RefType T17 Direct
A42 Type T17 Table
A43 Num T17 4
T18 Caption 24517 24558 Table 4: Tagging accuracies on test data.
A44 Type T18 Table
A45 Num T18 4
T19 Reference 24562 24653 By far the most frequent tagging error was the confusion of nominative and accusative case.
A46 RefType T19 Indirect
A47 Type T19 Table
A48 Num T19 4
T20 Reference 24837 24937 this error is not counted, the tagging accuracy on the development data rises from 92.17% to 94.27%.
A49 RefType T20 Indirect
A50 Type T20 Table
A51 Num T20 4
T21 Caption 26610 26675 Figure 3: Accuracy on development data depend ing on context size
A52 Type T21 Figure
A53 Num T21 3
T22 Reference 26877 26980 The corresponding figures for the test data are.
			89.53% for our tagger and 88.88% for the TnT tag- g
A54 RefType T22 Indirect
A55 Type T22 Figure
A56 Num T22 3
